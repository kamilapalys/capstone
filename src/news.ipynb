{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from string import punctuation\n",
    "import os\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(filename):\n",
    " '''Opens the input text file and\n",
    " returns a list of all of its words.'''\n",
    " file = open(filename, 'r')\n",
    " text = file.read()\n",
    " file.close()\n",
    " text = text.replace('\\n', ' ')\n",
    " text = text.replace('  ', ' ')\n",
    " return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kamilapalys/Desktop/school/data450/capstone\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kamilapalys/Desktop/school/data450/capstone'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Donald Trump faces more than 30 counts related to business fraud in an indictment from a Manhattan grand jury, according to two sources familiar with the case – the first time in American history that a current or former president has faced criminal charges. Trump is expected to appear in court on Tuesday. The indictment has been filed under seal and will be announced in the coming days. The charges are not publicly known at this time. Manhattan District Attorney Alvin Bragg’s office has been investigating the former president in connection with his alleged role in a hush money payment scheme and cover-up involving adult film star Stormy Daniels that dates to the 2016 presidential election. Grand jury proceedings are secret, but a source familiar with the case told CNN that a witness gave about 30 minutes of testimony before it voted to indict Trump. The decision is sure to send shockwaves across the country, pushing the American political system – which has never seen one of its ex-leaders confronted with criminal charges, let alone while running again for president – into uncharted waters. Trump released a statement in response to the indictment claiming it was “Political Persecution and Election Interference at the highest level in history.” “I believe this Witch-Hunt will backfire massively on Joe Biden,” the former president said. “The American people realize exactly what the Radical Left Democrats are doing here. Everyone can see it. So our Movement, and our Party – united and strong – will first defeat Alvin Bragg, and then we will defeat Joe Biden, and we are going to throw every last one of these Crooked Democrats out of office so we can MAKE AMERICA GREAT AGAIN!” Trump was caught off guard by the grand jury’s decision to indict him, according to a person who spoke directly with him. While the former president was bracing for an indictment last week, he began to believe news reports that a potential indictment was weeks – or more – away. “Is this a shock today? Hell yes,” the person said, speaking on a condition of anonymity as Trump’s team calculated its response. Bragg’s office said it is in touch with Trump’s lawyers. “This evening we contacted Mr. Trump’s attorney to coordinate his surrender to the Manhattan D.A.’s Office for arraignment on a Supreme Court indictment, which remains under seal,” the district attorney’s office said in a statement Thursday. “Guidance will be provided when the arraignment date is selected.” Joe Tacopina, the defense lawyer for Trump, said the former president was initially asked to turn himself in to authorities in New York on Friday, the day after a Manhattan grand jury voted to indict him. But Tacopina said he was surprised by the timing of the indictment and that more time was needed, as the Secret Service that protects the former president needs to coordinate his surrender in New York. The legal action against Trump jolts the 2024 presidential campaign into a new phase, as the former president has vowed to keep running in the face of criminal charges. Trump has frequently called the various investigations surrounding him a “witch hunt,” attempting to sway public opinion on them by casting himself as a victim of what he’s claimed are political probes led by Democratic prosecutors. As the indictment reportedly neared, Trump urged his supporters to protest his arrest, echoing his calls to action following the 2020 election as he tried to overturn his loss to President Joe Biden. Trump has long avoided legal consequences in his personal, professional and political lives. He has settled a number of private civil lawsuits through the years and paid his way out of disputes concerning the Trump Organization, his namesake company. As president, he was twice impeached by the Democratic-led House, but avoided conviction by the Senate. In December, the Trump Organization was convicted on multiple charges of tax fraud, though Trump himself was not charged in that case. Trump’s Republican allies – as well as his 2024 GOP rivals – have condemned the Manhattan district attorney’s office over the looming indictment. “I think the unprecedented indictment of a former president of the United States on a campaign finance issue is an outrage,” former Vice President Mike Pence told CNN’s Wolf Blitzer in an interview Thursday night. “It appears to millions of Americans to be nothing more than a political prosecution that’s driven by a prosecutor who literally ran for office on a pledge to indict the former president.” GOP rallies to Trump’s defense House Speaker Kevin McCarthy has vowed to launch an investigation into the matter, and congressional Republicans quickly rallied to Trump’s defense, attacking Bragg on Twitter and accusing the district attorney of a political witch hunt. “Outrageous,” tweeted House Judiciary Chairman Jim Jordan of Ohio, one of the Republican committee chairmen who has demanded Bragg testify before Congress about the Trump investigation. Sen. Ted Cruz, a Texas Republican, called the indictment “completely unprecedented” and said it is “a catastrophic escalation in the weaponization of the justice system.” But at least one moderate Republican told CNN he trusted the legal system. “I believe in the rule of law. I think we have checks and balances and I trust the system,” said Rep. Don Bacon of Nebraska. “We have a judge. We have jurors. There is appeals. So I think in the end, justice will be done. If he’s guilty it will show up. But if not, I think that will be shown too,” Bacon told CNN. Former Vice President Mike Pence is interviewed by CNN's Wolf Bitzer on Thursday, March 30. 'An outrage': Pence reacts to Trump indictment Investigation began under Cy Vance Bragg’s office had signaled as recently as early March that they were close to bringing charges against Trump after they invited the ex-president to testify before the grand jury probing the hush money scheme. Potential defendants in New York are required by law to be notified and invited to appear before a grand jury weighing charges. But Trump ultimately declined to appear before the panel. The long-running investigation first began under Bragg’s predecessor, Cy Vance, when Trump was in office. It relates to a $130,000 payment made by Trump’s then-personal attorney Michael Cohen to Daniels in late October 2016, days before the 2016 presidential election, to silence her from going public about an alleged affair with Trump a decade earlier. Trump has denied the affair. At issue in the investigation is the payment made to Daniels and the Trump Organization’s reimbursement to Cohen. According to court filings in Cohen’s own federal prosecution, Trump Organization executives authorized payments to him totaling $420,000 to cover his original $130,000 payment and tax liabilities and reward him with a bonus. The Trump Organization noted the reimbursements as a legal expense in its internal books. Trump has denied knowledge of the payment.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of using the function\n",
    "\n",
    "filepath = 'data/text/cnn_trump.txt'\n",
    "test_txt = text_to_tokens(filepath)\n",
    "test_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Takes in a list of tokens and cleans them by converting\n",
    "    to lowercase, removing punctuation, removing stopwords, \n",
    "    and stemming. Returns the new list of tokens.'''\n",
    "    ps = PorterStemmer()\n",
    "    # create list of stopwords \n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # make the text lowercase\n",
    "    text = text.lower()\n",
    "    # convert to ascii characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    new_text = ''\n",
    "    for chr in text:\n",
    "        # only keep characters in the string that are not punctuation symbols\n",
    "        if chr in string.punctuation:\n",
    "            text = text.replace(chr, ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    # stem the tokens within the text\n",
    "    tokens = text.split(\" \")\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed_word = ps.stem(token)\n",
    "        # only include new token in the cleaned list if not a stopword\n",
    "        if stemmed_word not in stopwords_list:\n",
    "            stemmed.append(stemmed_word)\n",
    "    cleaned_text = \" \".join(stemmed)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donald trump face 30 count relat busi fraud indict manhattan grand juri accord two sourc familiar case first time american histori current former presid ha face crimin charg trump expect appear court tuesday indict ha file seal announc come day charg publicli known thi time manhattan district attorney alvin bragg offic ha investig former presid connect hi alleg role hush money payment scheme cover involv adult film star stormi daniel date 2016 presidenti elect grand juri proceed secret sourc familiar case told cnn wit gave 30 minut testimoni befor vote indict trump decis sure send shockwav across countri push american polit system ha never seen one ex leader confront crimin charg let alon run presid unchart water trump releas statement respons indict claim wa polit persecut elect interfer highest level histori believ thi witch hunt backfir massiv joe biden former presid said american peopl realiz exactli radic left democrat everyon see movement parti unit strong first defeat alvin bragg defeat joe biden go throw everi last one crook democrat offic make america great trump wa caught guard grand juri decis indict accord person spoke directli former presid wa brace indict last week began believ news report potenti indict wa week away thi shock today hell ye person said speak condit anonym trump team calcul respons bragg offic said touch trump lawyer thi even contact mr trump attorney coordin hi surrend manhattan offic arraign suprem court indict remain seal district attorney offic said statement thursday guidanc provid arraign date select joe tacopina defens lawyer trump said former presid wa initi ask turn author new york friday day manhattan grand juri vote indict tacopina said wa surpris time indict time wa need secret servic protect former presid need coordin hi surrend new york legal action trump jolt 2024 presidenti campaign new phase former presid ha vow keep run face crimin charg trump ha frequent call variou investig surround witch hunt attempt sway public opinion cast victim claim polit probe led democrat prosecutor indict reportedli near trump urg hi support protest hi arrest echo hi call action follow 2020 elect tri overturn hi loss presid joe biden trump ha long avoid legal consequ hi person profession polit live ha settl number privat civil lawsuit year paid hi way disput concern trump organ hi namesak compani presid wa twice impeach democrat led hous avoid convict senat decemb trump organ wa convict multipl charg tax fraud though trump wa charg case trump republican alli well hi 2024 gop rival condemn manhattan district attorney offic loom indict think unpreced indict former presid unit state campaign financ issu outrag former vice presid mike penc told cnn wolf blitzer interview thursday night appear million american noth polit prosecut driven prosecutor liter ran offic pledg indict former presid gop ralli trump defens hous speaker kevin mccarthi ha vow launch investig matter congression republican quickli ralli trump defens attack bragg twitter accus district attorney polit witch hunt outrag tweet hous judiciari chairman jim jordan ohio one republican committe chairmen ha demand bragg testifi befor congress trump investig sen ted cruz texa republican call indict complet unpreced said catastroph escal weapon justic system least one moder republican told cnn trust legal system believ rule law think check balanc trust system said rep bacon nebraska judg juror appeal think end justic done guilti show think shown bacon told cnn former vice presid mike penc interview cnn wolf bitzer thursday march 30  outrag  penc react trump indict investig began cy vanc bragg offic signal recent earli march close bring charg trump invit ex presid testifi befor grand juri probe hush money scheme potenti defend new york requir law notifi invit appear befor grand juri weigh charg trump ultim declin appear befor panel long run investig first began bragg predecessor cy vanc trump wa offic relat 130 000 payment made trump person attorney michael cohen daniel late octob 2016 day befor 2016 presidenti elect silenc go public alleg affair trump decad earlier trump ha deni affair issu investig payment made daniel trump organ reimburs cohen accord court file cohen feder prosecut trump organ execut author payment total 420 000 cover hi origin 130 000 payment tax liabil reward bonu trump organ note reimburs legal expens intern book trump ha deni knowledg payment '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = clean_text(test_txt)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/text/nyp_affirmative.txt',\n",
       " 'data/text/bbc_tanks.txt',\n",
       " 'data/text/cnn_hamas.txt',\n",
       " 'data/text/wsj_trump.txt',\n",
       " 'data/text/wp_affirmative.txt',\n",
       " 'data/text/bbc_balloon.txt',\n",
       " 'data/text/wsj_affirmative.txt',\n",
       " 'data/text/nyt_trump.txt',\n",
       " 'data/text/wp_balloon.txt',\n",
       " 'data/text/cnn_pentagon.txt',\n",
       " 'data/text/cnn_tanks.txt',\n",
       " 'data/text/bbc_hamas.txt',\n",
       " 'data/text/nbc_tanks.txt',\n",
       " 'data/text/nyp_hamas.txt',\n",
       " 'data/text/.DS_Store',\n",
       " 'data/text/cnn_balloon.txt',\n",
       " 'data/text/fox_pentagon.txt',\n",
       " 'data/text/wp_biden.txt',\n",
       " 'data/text/nyp_tanks.txt',\n",
       " 'data/text/nbc_hamas.txt',\n",
       " 'data/text/abc_balloon.txt',\n",
       " 'data/text/nyt_pentagon.txt',\n",
       " 'data/text/fox_tanks.txt',\n",
       " 'data/text/wp_trump.txt',\n",
       " 'data/text/fox_hamas.txt',\n",
       " 'data/text/nyp_balloon.txt',\n",
       " 'data/text/abc_hamas.txt',\n",
       " 'data/text/abc_santos.txt',\n",
       " 'data/text/nyp_santos.txt',\n",
       " 'data/text/fox_affirmative.txt',\n",
       " 'data/text/wsj_biden.txt',\n",
       " 'data/text/nyt_biden.txt',\n",
       " 'data/text/abc_tanks.txt',\n",
       " 'data/text/nbc_affirmative.txt',\n",
       " 'data/text/nyt_hamas.txt',\n",
       " 'data/text/wsj_tanks.txt',\n",
       " 'data/text/wp_pentagon.txt',\n",
       " 'data/text/nyp_pentagon.txt',\n",
       " 'data/text/bbc_trump.txt',\n",
       " 'data/text/nyt_affirmative.txt',\n",
       " 'data/text/cnn_trump.txt',\n",
       " 'data/text/abc_biden.txt',\n",
       " 'data/text/cnn_affirmative.txt',\n",
       " 'data/text/bbc_affirmative.txt',\n",
       " 'data/text/wsj_hamas.txt',\n",
       " 'data/text/wsj_pentagon.txt',\n",
       " 'data/text/nyt_tanks.txt',\n",
       " 'data/text/nbc_balloon.txt',\n",
       " 'data/text/fox_biden.txt',\n",
       " 'data/text/wsj_balloon.txt',\n",
       " 'data/text/nbc_pentagon.txt',\n",
       " 'data/text/bbc_santos.txt',\n",
       " 'data/text/wsj_santos.txt',\n",
       " 'data/text/nbc_trump.txt',\n",
       " 'data/text/nyp_trump.txt',\n",
       " 'data/text/cnn_santos.txt',\n",
       " 'data/text/abc_pentagon.txt',\n",
       " 'data/text/fox_santos.txt',\n",
       " 'data/text/wp_hamas.txt',\n",
       " 'data/text/fox_trump.txt',\n",
       " 'data/text/nbc_biden.txt',\n",
       " 'data/text/nyp_biden.txt',\n",
       " 'data/text/nyt_balloon.txt',\n",
       " 'data/text/wp_tanks.txt',\n",
       " 'data/text/wp_santos.txt',\n",
       " 'data/text/bbc_biden.txt',\n",
       " 'data/text/cnn_biden.txt',\n",
       " 'data/text/abc_trump.txt',\n",
       " 'data/text/nyt_santos.txt',\n",
       " 'data/text/bbc_pentagon.txt',\n",
       " 'data/text/abc_affirmative.txt',\n",
       " 'data/text/nbc_santos.txt',\n",
       " 'data/text/fox_balloon.txt']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how all the text files will be looped through\n",
    "\n",
    "directory = 'data/text/'\n",
    "file_list = []\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    file_list.append(f\"{filepath}\")\n",
    "file_list\n",
    "    # use preprocessing functions\n",
    "    # somehow append each list of tokens (each article) to a dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'start engine'\n",
    "doc2 = 'start your engine'\n",
    "doc3 = 'whenever youre ready to start'\n",
    "docs = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.preprocessing.text.Tokenizer object at 0x178a732b0>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>engine</th>\n",
       "      <th>your</th>\n",
       "      <th>whenever</th>\n",
       "      <th>youre</th>\n",
       "      <th>ready</th>\n",
       "      <th>to</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  engine  your  whenever  youre  ready   to source\n",
       "0    1.0     1.0   0.0       0.0    0.0    0.0  0.0    cnn\n",
       "1    1.0     1.0   1.0       0.0    0.0    0.0  0.0    cnn\n",
       "2    1.0     0.0   0.0       1.0    1.0    1.0  1.0    abc"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes all docs and makes them into a dataframe\n",
    "# make sure that docs is just a list of all the tokens from all articles\n",
    "# first make one list of tokens for each article and then add all those lists up\n",
    "# mode is one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(docs)\n",
    "print(t)\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
    "#print(encoded_docs)\n",
    "words = [x for x in t.word_index.keys()]\n",
    "bow = pd.DataFrame(data = encoded_docs[:, 1:], columns=words)\n",
    "bow['source'] = ['cnn']*2 + ['abc']\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.preprocessing.text.Tokenizer object at 0x178c5de20>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump</th>\n",
       "      <th>indict</th>\n",
       "      <th>presid</th>\n",
       "      <th>ha</th>\n",
       "      <th>hi</th>\n",
       "      <th>former</th>\n",
       "      <th>wa</th>\n",
       "      <th>offic</th>\n",
       "      <th>charg</th>\n",
       "      <th>said</th>\n",
       "      <th>...</th>\n",
       "      <th>420</th>\n",
       "      <th>origin</th>\n",
       "      <th>liabil</th>\n",
       "      <th>reward</th>\n",
       "      <th>bonu</th>\n",
       "      <th>note</th>\n",
       "      <th>expens</th>\n",
       "      <th>intern</th>\n",
       "      <th>book</th>\n",
       "      <th>knowledg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 361 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trump  indict  presid   ha   hi  former   wa  offic  charg  said  ...  420  \\\n",
       "0    1.0     1.0     1.0  1.0  1.0     1.0  1.0    1.0    1.0   1.0  ...  1.0   \n",
       "\n",
       "   origin  liabil  reward  bonu  note  expens  intern  book  knowledg  \n",
       "0     1.0     1.0     1.0   1.0   1.0     1.0     1.0   1.0       1.0  \n",
       "\n",
       "[1 rows x 361 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "\n",
    "docs = [cleaned_text]\n",
    "t.fit_on_texts(docs)\n",
    "print(t)\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
    "#print(encoded_docs)\n",
    "words = [x for x in t.word_index.keys()]\n",
    "bow = pd.DataFrame(data = encoded_docs[:, 1:], columns=words)\n",
    "bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m      6\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 7\u001b[0m bow \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(bow)\n\u001b[1;32m     10\u001b[0m count_array \u001b[38;5;241m=\u001b[39m bow\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cmps530/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cmps530/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cmps530/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cmps530/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cmps530/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = {'text': docs, 'source': ['cnn', 'abc', 'nbc']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(df['text'])\n",
    "print(bow)\n",
    "\n",
    "count_array = bow.toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "print(count_array)\n",
    "print(features)\n",
    "#df = pd.DataFrame(data=count_array, columns=features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
