---
title: "How Differently Do News Sources Report on the Same Events?"
subtitle: "Spring 2024"
author: "Kamila Palys"
bibliography: references.bib
number-sections: false
format:
  html:
    theme: default
    rendering: embed-resources
    code-fold: true
    code-tools: true
    toc: true
  pdf: default
jupyter: python3
---


![Source: Unsplash](newspaper.jpg){fig-alt="A man reading a newspaper."}


# Introduction

Everybody at some point has been at the center of a debate or has heard one about which news sources are trustworthy or not. These opinions are usually associated with the identification of a political party. A lot of the times, these opinions are largely subjective, but can they be confirmed or changed with a more technical analysis on news sources? What can we tell about each news source just based on the words that they use? This project takes a closer look at the language that various news sources use and examines if these news sources really report on the same events in a different manner. This will be done mainly through sentiment analysis and finally, creating classifiers to predict the source of a given article. 

# Data Collection

 Nine total news sites are studied for a mix of political affiliations, including CNN, The New York Times, The Washington Post, Fox News, New York Post, NBC News, The Wall Street Journal, BBC, and ABC News. Eight political events and topics from 2023 were chosen as the subjects for the articles to be studied for a higher potential for bias. For each topic, one article from each source was collected, making for a total of 72 articles. To ensure as little room for extraneous variables as possible, the articles for a particular topic across sources were chosen in a way so that they were published on the same day or as closely together in time as possible. This would mean that each source should have had the same information available when writing the article. The text from each article was collected through copying and pasting into its own text file. In addition, a string denoting each article's topic and source name was added to the end of each article's text file to be able to identify it. 

# Data Preprocessing

The text had to go through a lot of preparation before it could be ready for analyzing. Functions were created to make this process faster for each article. One function accesses a text file and returns a string of all the text in there. Another function cleans the text, which includes making all text lowercase, removing punctuation, removing stopwords (words like "and," "the," etc.), and stemming words, if desired. Stemming words keeps only the root form of a word, removing suffixes so that words like "jumps" and "jumping" are both reduced to "jump." This is desired for some parts of the analysis in this project, but not others. 

```{python}
# don't show future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# initialize dictionary to be able to trace back a stemmed word to its original form
stemmed_dict = {}

def file_to_string(filename):
  '''Opens the input text file and
  returns a string of all its text.'''
  file = open(filename, 'r')
  text = file.read()
  file.close()
  text = text.replace('\n', ' ')
  text = text.replace('  ', ' ')
  return text

def clean_text(text, stem):
  '''Takes in a string of text and cleans it by converting
  to lowercase, removing punctuation, and removing stopwords. 
  Also takes in a binary value to indicate if stemming should
  be performed. Returns the new string.'''
  if stem not in [0, 1]:
      raise ValueError("Stem must be a binary value (0 or 1)")
  ps = PorterStemmer()
  stemmed_dict = {}
  # create list of stopwords 
  stopwords_list = stopwords.words('english')
  # make the text lowercase
  text = text.lower()
  text = text.replace('â€”', ' ')
  text = text.replace('u.s.', 'us')
  # convert to ascii characters
  text = text.encode("ascii", "ignore").decode()
  for chr in text:
      # only keep characters in the string that are not punctuation symbols
      if (chr in string.punctuation or chr in string.digits):
          text = text.replace(chr, ' ')
  text = text.replace('  ', ' ')
  # stem the tokens within the text
  tokens = text.split()
  new_tokens = []
  for token in tokens[:-2]: # last two tokens identify source and topic, we do not want to stem them
      # only include new token in the cleaned list if not a stopword
      if token not in stopwords_list:
          if stem == 1:
              stemmed_word = ps.stem(token)
              new_tokens.append(stemmed_word)
              # to be able to map each token to the resulting stemmed word
              if token not in stemmed_dict:
                  stemmed_dict[token] = stemmed_word
          else:
              new_tokens.append(token)
  # add back in last two tokens
  new_tokens.append(tokens[-2])
  new_tokens.append(tokens[-1])
  cleaned_text = " ".join(new_tokens)
  cleaned_text = cleaned_text.replace('  ', ' ')
  return cleaned_text
```

# Dataframes

All of the article text collected is then transformed into multiple dataframe versions. One way of representing textual data within a dataframe is with binary values. In this case, each column is a token, or a word, that appears in any of the documents (articles) collected. Each row represents one document. The values are either a 0 or 1, denoting whether or not the token appears within the article. Another representation as a dataframe has the values of the dataframe as the frequencies of each token within an article. Finally, another dataframe is created with the Term Frequency-Inverse Document Frequency (TF-IDF) scores of the tokens within an article as the values. Some of these dataframes may work better than others when it comes to the predictive modeling portion of the project and revealing patterns within the text of a news source. 

```{python, include=False}
import os
from os import listdir

# we want to be in the capstone folder, not in the doc folder
os.chdir("..")
```

```{python}
#| output: false
pwd
```


```{python}
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import string
from string import punctuation
from tensorflow.keras.preprocessing.text import Tokenizer

# looping through all text files to apply preprocessing functions
article_docs = []
dir = os.listdir('data/text/')
dir.sort()
for filename in dir:
    filepath = os.path.join('data/text/', filename)
    if filename.split(".")[-1] == "txt":
        article_string = file_to_string(filepath)
        new_string = clean_text(article_string, 1)
        article_docs.append(new_string)

# convert the list of article strings into a binary-value dataframe
t = Tokenizer()
t.fit_on_texts(article_docs)
encoded_docs = t.texts_to_matrix(article_docs, mode='binary')
words = [x for x in t.word_index.keys()]
binary_df = pd.DataFrame(data = encoded_docs[:, 1:], columns=words)
# List of conditions
source_conditions = [
      binary_df['abcarticle'] == 1
    , binary_df['bbcarticle'] == 1
    , binary_df['cnnarticle'] == 1
    , binary_df['foxarticle'] == 1
    , binary_df['nbcarticle'] == 1
    , binary_df['nyparticle'] == 1
    , binary_df['nytarticle'] == 1
    , binary_df['wparticle'] == 1
    , binary_df['wsjarticle'] == 1
]

# List of values to return
source_choices  = [
      "ABC News"
    , "BBC"
    , "CNN"
    , "Fox News"
    , "NBC News"
    , "New York Post"
    , "The New York Times"
    , "The Washington Post"
    , "The Wall Street Journal"
]

# List of conditions
topic_conditions = [
      binary_df['affirmativearticle'] == 1
    , binary_df['balloonarticle'] == 1
    , binary_df['bidenarticle'] == 1
    , binary_df['hamasarticle'] == 1
    , binary_df['pentagonarticle'] == 1
    , binary_df['santosarticle'] == 1
    , binary_df['tanksarticle'] == 1
    , binary_df['trumparticle'] == 1
]
# List of values to return
topic_choices  = [
      "Supreme Court Ruling on Affirmative Action"
    , "Chinese Surveillance Balloon"
    , "Biden's Low Approval Rates in Polls"
    , "The Deadliest Attack by Hamas"
    , "Pentagon Documents Leak"
    , "George Santos' Expulsion from Congress"
    , "U.S. and Germany Send Tanks to Ukraine"
    , "Trump's Indictment"
]
# create a new source column 
binary_df["article_source"] = np.select(source_conditions, source_choices, "ERROR")

# create a new topic column
binary_df["article_topic"] = np.select(topic_conditions, topic_choices, "ERROR")

# drop the token columns that identify the topic/source, not part of actual article text
binary_df.drop(columns=['abcarticle', 'bbcarticle', 'cnnarticle', 'foxarticle', 'nbcarticle',
                        'nyparticle', 'nytarticle', 'wparticle', 'wsjarticle', 'affirmativearticle',
                        'balloonarticle', 'bidenarticle', 'hamasarticle', 'pentagonarticle',
                        'santosarticle', 'tanksarticle', 'trumparticle'], inplace=True)

binary_df.head()

encoded_docs_freq = t.texts_to_matrix(article_docs, mode='count')
freq_df = pd.DataFrame(data = encoded_docs_freq[:, 1:], columns=words)
# List of conditions
source_conditions = [
      freq_df['abcarticle'] == 1
    , freq_df['bbcarticle'] == 1
    , freq_df['cnnarticle'] == 1
    , freq_df['foxarticle'] == 1
    , freq_df['nbcarticle'] == 1
    , freq_df['nyparticle'] == 1
    , freq_df['nytarticle'] == 1
    , freq_df['wparticle'] == 1
    , freq_df['wsjarticle'] == 1
]

# List of values to return
source_choices  = [
      "ABC News"
    , "BBC"
    , "CNN"
    , "Fox News"
    , "NBC News"
    , "New York Post"
    , "The New York Times"
    , "The Washington Post"
    , "The Wall Street Journal"
]

# List of conditions
topic_conditions = [
      freq_df['affirmativearticle'] == 1
    , freq_df['balloonarticle'] == 1
    , freq_df['bidenarticle'] == 1
    , freq_df['hamasarticle'] == 1
    , freq_df['pentagonarticle'] == 1
    , freq_df['santosarticle'] == 1
    , freq_df['tanksarticle'] == 1
    , freq_df['trumparticle'] == 1
]
# List of values to return
topic_choices  = [
      "Supreme Court Ruling on Affirmative Action"
    , "Chinese Surveillance Balloon"
    , "Biden's Low Approval Rates in Polls"
    , "The Deadliest Attack by Hamas"
    , "Pentagon Documents Leak"
    , "George Santos' Expulsion from Congress"
    , "U.S. and Germany Send Tanks to Ukraine"
    , "Trump's Indictment"
]
# create a new source column 
freq_df["article_source"] = np.select(source_conditions, source_choices, "ERROR")

# create a new topic column
freq_df["article_topic"] = np.select(topic_conditions, topic_choices, "ERROR")

# drop the token columns that identify the topic/source, not part of actual article text
freq_df.drop(columns=['abcarticle', 'bbcarticle', 'cnnarticle', 'foxarticle', 'nbcarticle',
                        'nyparticle', 'nytarticle', 'wparticle', 'wsjarticle', 'affirmativearticle',
                        'balloonarticle', 'bidenarticle', 'hamasarticle', 'pentagonarticle',
                        'santosarticle', 'tanksarticle', 'trumparticle'], inplace=True)

freq_df.head()

# create dataframe with tf-idf values

encoded_docs_tfidf = t.texts_to_matrix(article_docs, mode='tfidf')
tfidf_df = pd.DataFrame(data = encoded_docs_tfidf[:, 1:], columns=words)
# List of conditions
source_conditions = [
      tfidf_df['abcarticle'] != 0
    , tfidf_df['bbcarticle'] != 0
    , tfidf_df['cnnarticle'] != 0
    , tfidf_df['foxarticle'] != 0
    , tfidf_df['nbcarticle'] != 0
    , tfidf_df['nyparticle'] != 0
    , tfidf_df['nytarticle'] != 0
    , tfidf_df['wparticle'] != 0
    , tfidf_df['wsjarticle'] != 0
]

# List of values to return
source_choices  = [
      "ABC News"
    , "BBC"
    , "CNN"
    , "Fox News"
    , "NBC News"
    , "New York Post"
    , "The New York Times"
    , "The Washington Post"
    , "The Wall Street Journal"
]

# List of conditions
topic_conditions = [
      tfidf_df['affirmativearticle'] != 0
    , tfidf_df['balloonarticle'] != 0
    , tfidf_df['bidenarticle'] != 0
    , tfidf_df['hamasarticle'] != 0
    , tfidf_df['pentagonarticle'] != 0
    , tfidf_df['santosarticle'] != 0
    , tfidf_df['tanksarticle'] != 0
    , tfidf_df['trumparticle'] != 0
]
# List of values to return
topic_choices  = [
      "Supreme Court Ruling on Affirmative Action"
    , "Chinese Surveillance Balloon"
    , "Biden's Low Approval Rates in Polls"
    , "The Deadliest Attack by Hamas"
    , "Pentagon Documents Leak"
    , "George Santos' Expulsion from Congress"
    , "U.S. and Germany Send Tanks to Ukraine"
    , "Trump's Indictment"
]
# create a new source column 
tfidf_df["article_source"] = np.select(source_conditions, source_choices, "ERROR")

# create a new topic column
tfidf_df["article_topic"] = np.select(topic_conditions, topic_choices, "ERROR")

# drop the token columns that identify the topic/source, not part of actual article text
tfidf_df.drop(columns=['abcarticle', 'bbcarticle', 'cnnarticle', 'foxarticle', 'nbcarticle',
                        'nyparticle', 'nytarticle', 'wparticle', 'wsjarticle', 'affirmativearticle',
                        'balloonarticle', 'bidenarticle', 'hamasarticle', 'pentagonarticle',
                        'santosarticle', 'tanksarticle', 'trumparticle'], inplace=True)

tfidf_df.head()
```




# Exploratory Analysis

We may be interested in clustering news sources together to see which ones are most similar to each other. This can be done in a dendrogram using the TF-IDF scores of their tokens. For this purpose, a new dataframe will have to be created with each row representing not just one article, but all articles from a single source in order to have one entry per source. 

```{python}
# create a list of strings where each string is all articles from one source (for dendogram)
source_docs = []

j = 0

for i in range(9):
    # looping through each article of each source, and only taking up until and not including second to last
    # token, bc last two tokens represent the name of the source and topic of article
    # for last article, we index up until and not including only the last token bc we don't want to include
    # the topic of the article, but we do want to include the source name, which is the second to last token
    source = " ".join(article_docs[j].split()[:-2]) + " " + " ".join(article_docs[j+1].split()[:-2]) + " "\
        + " ".join(article_docs[j+2].split()[:-2]) + " " + " ".join(article_docs[j+3].split()[:-2]) + " "\
        + " ".join(article_docs[j+4].split()[:-2]) + " " + " ".join(article_docs[j+5].split()[:-2]) + " "\
        + " ".join(article_docs[j+6].split()[:-2]) + " " + " ".join(article_docs[j+7].split()[:-1])
    source_docs.append(source)
    j += 8

source_docs

# create a dataframe of token tf-idf's with each row representing all articles of one source

# convert the list of article strings into a tf-idf-value dataframe
t = Tokenizer()
t.fit_on_texts(source_docs)
encoded_source_docs = t.texts_to_matrix(source_docs, mode='tfidf')
words = [x for x in t.word_index.keys()]
tfidf_source_df = pd.DataFrame(data = encoded_source_docs[:, 1:], columns=words)
# List of conditions
source_conditions = [
      tfidf_source_df['abcarticle'] != 0
    , tfidf_source_df['bbcarticle'] != 0
    , tfidf_source_df['cnnarticle'] != 0
    , tfidf_source_df['foxarticle'] != 0
    , tfidf_source_df['nbcarticle'] != 0
    , tfidf_source_df['nyparticle'] != 0
    , tfidf_source_df['nytarticle'] != 0
    , tfidf_source_df['wparticle'] != 0
    , tfidf_source_df['wsjarticle'] != 0
]

# List of values to return
source_choices  = [
      "ABC News"
    , "BBC"
    , "CNN"
    , "Fox News"
    , "NBC News"
    , "New York Post"
    , "The New York Times"
    , "The Washington Post"
    , "The Wall Street Journal"
]

# create a new source column 
tfidf_source_df["article_source"] = np.select(source_conditions, source_choices, "ERROR")
tfidf_source_df.set_index('article_source', inplace=True) # removes article_source column and makes it the index
tfidf_source_df.drop(['abcarticle', 'bbcarticle', 'cnnarticle', 'foxarticle',
                      'nbcarticle', 'nyparticle', 'nytarticle', 'wparticle',
                      'wsjarticle'], axis=1, inplace=True)
tfidf_source_df
```


```{python}
#| label: fig-dendrogram
#| fig-cap: "A dendrogram clustering sources together using TF-IDF scores of their tokens."

# create dendrogram with complete linkage from tfidf scores df, whose rows each represent one source

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

Z = linkage(tfidf_source_df, 'complete')
fig = plt.figure(figsize=(14, 4))
fig.suptitle("Complete Linkage", fontsize=14)
plt.xlabel('Source', fontsize=6)
plt.yticks(fontsize = 8) 
dn = dendrogram(Z, labels=tfidf_source_df.index)
plt.xticks(fontsize = 8)
plt.show()
```

According to the dendrogram, ABC News and NBC News, as well as BBC and Fox News are the two pairs of sources that are most similar to one another. However, it's important to keep in mind that these results are based on only the small sample of articles used for each source and may not be generalizable to the sources as wholes. It is also important to keep in mind that the similarity of these sources is solely being based on the vocabulary usage of each source and not any other factors.  

Another thing that could be interesting to look at are the wordclouds for each source. On a wordcloud, the bigger the word, the more frequently it appears within the text. Looking at these could also give a sense of the difference in vocabulary between each source. This is why topic coverage was kept the same across all sources, so that comparisons like these may be made to scope out differences in wording, given that the sources are each writing about the same topics.

```{python}
# create a function to be able to make a series of wordclouds, one for each source
from wordcloud import WordCloud

def create_wordcloud(text, title):
    '''Given a string of all text and a string
    for the title, creates a wordcloud.'''
    plt.figure(figsize = (6,3))
    wc = WordCloud(colormap = "YlOrRd", background_color='white', width=1600, height=800, max_font_size = 400).generate(text)
    plt.title(title, fontsize=12)
    plt.axis("off")
    title_snake = title.lower().replace(" ", "_")
    wc.to_file(f'wordcloud_{title_snake}.png')
    #plt.savefig(f'wordcloud_{title_snake}.png', dpi = 1000)
    plt.tight_layout(pad=0)
    plt.imshow(wc)

# loop through the list of strings, each one representing all articles' text from a given source, and create the wordcloud for it. 
for i in range(len(source_docs)):
    create_wordcloud(source_docs[i], tfidf_source_df.index[i])
```

The wordclouds for each source look mostly similar, with the most common word throughout them all being "said." This is unsurprising. The only exception here is Fox News, whose most frequently appearing word throughout all the articles is "Trump." Trump is a frequently appearing word for all sources because one of the topics chosen for the articles is explicitly about Trump. However, for Fox News, it is even more commonly used than the word "said." This is interesting, but this alone should not provoke any conclusions about this source being biased towards Trump, for example. It is important to remember that all this means is that "Trump" is the word that appeared the most frequently througout Fox News' articles, which could potentially be because Fox News had the longest article about Trump. Longer articles will certainly have a higher count of words overall. This can be examined further.

```{python}
# create a list of unstemmed article document texts
# looping through all text files to apply preprocessing functions
from plotnine import *

article_docs_unstemmed = []
dir = os.listdir('data/text/')
dir.sort()
for filename in dir:
    filepath = os.path.join('data/text/', filename)
    if filename.split(".")[-1] == "txt":
        article_string = file_to_string(filepath)
        new_string = clean_text(article_string, 0)
        article_docs_unstemmed.append(new_string)

# investigating why Fox News has Trump as most frequent word, to see if it's related to length of article 

trump_article_lengths = []
trump_word_counts = []

i = 1
for article in article_docs_unstemmed:
    if i%8 == 0:
        trump_count = 0
        for word in article.split():
            if word == "trump":
                trump_count += 1
        trump_word_counts.append(trump_count)
        article_length = len(article.split())
        trump_article_lengths.append(article_length)
    i += 1

# let's do the same for the word "biden" in the biden articles and see if there are outliers 

biden_article_lengths = []
biden_word_counts = []

i = 1
for article in article_docs_unstemmed:
    if article.split()[-1] == "bidenarticle":
        biden_count = 0
        for word in article.split():
            if word == "biden":
                biden_count += 1
        biden_word_counts.append(biden_count)
        article_length = len(article.split())
        biden_article_lengths.append(article_length)
    i += 1

# create a df with sources, their trump article lengths, and the frequency of the word "trump"

trump_words = pd.DataFrame(columns=['source', 'article_length', 'trump_word_count'])
trump_words['source'] = tfidf_source_df.index
trump_words['article_length'] = trump_article_lengths
trump_words['trump_word_count'] = trump_word_counts
trump_words

biden_words = pd.DataFrame(columns=['source', 'article_length', 'biden_word_count'])
biden_words['source'] = tfidf_source_df.index
biden_words['article_length'] = biden_article_lengths
biden_words['biden_word_count'] = biden_word_counts

# create scatterplot of length of trump article vs frequency of the word "Trump"

display(
ggplot(data=trump_words,
       mapping=aes(x='article_length', y='trump_word_count', color='source'))
       + geom_point(show_legend=True)
       + geom_smooth(method = "lm", se=False, color="darkgrey")
       + labs(title="Trump Article Length vs. Mentions of Trump",
              x='# of Words in Article',
              y='# of Times "Trump" Mentioned')
)

# create scatterplot of length of biden article vs frequency of the word "Biden"

(
ggplot(data=biden_words,
       mapping=aes(x='article_length', y='biden_word_count', color='source'))
       + geom_point(show_legend=True)
       + geom_smooth(method = "lm", se=False, color="darkgrey")
       + labs(title="Biden Article Length vs. Mentions of Biden",
              x='# of Words in Article',
              y='# of Times "Biden" Mentioned')
)
```

Above we used scatterplots to see how the length of the Trump articles are related to the frequency of the word "Trump" within them, by source. We do, in fact, see that the point for Fox News is an outlier. Given its article length, it does mention Trump a lot more frequently than the Trump articles from other sources. Out of curiosity, we look at the same visualization for the word "Biden" in the Biden articles. Interestingly, Fox News again appears to be an outlier. Given its article length, it also mentions Biden a lot more frequently than the remaining sources. Now, it is not as easy to make assumptions about the partiality of Fox News just based on these word frequencies. They could be more telling of the writing styles of each source, or it could also merely be a result of using such a small sample size of articles.

# Sentiment Analysis

Next, the connotation of each article and source overall is studied. The TextBlob package in Python provides polarity and subjectivity scores for any text string. A polarity score, on a scale of [-1, 1], tells if the text has a positive or negative connotation to it. Subjectivity scores, on a scale of [0, 1] tells how opinionated or subjective a text sounds as opposed to being factual. The higher the score, the more subjective the text is. A dataframe is created that displays the polarity and subjectivity scores of each article, with various other columns being created that help place each article's score in the context of the entire corpus of documents and that will be used in following visualizations.

```{python}
# calculate polarity and subjectivity scores, create dataframe
from textblob import TextBlob

scores_df = pd.DataFrame({'source':tfidf_df['article_source'], 'topic':tfidf_df['article_topic']})

scores_df['source'] = np.where(scores_df['source'] == "The New York Times", "New York Times", scores_df['source'])
scores_df['source'] = np.where(scores_df['source'] == "The Washington Post", "Washington Post", scores_df['source'])
scores_df['source'] = np.where(scores_df['source'] == "The Wall Street Journal", "Wall Street Journal", scores_df['source'])

polarity_scores = []
subjectivity_scores = []

for article in article_docs_unstemmed:
    polarity_scores.append(round(TextBlob(" ".join(article.split()[:-2])).sentiment.polarity, 2))
    subjectivity_scores.append(round(TextBlob(" ".join(article.split()[:-2])).sentiment.subjectivity, 2))

scores_df['polarity_score'] = polarity_scores
scores_df['subjectivity_score'] = subjectivity_scores

average_topic_polarity_scores = []
average_source_polarity_scores = []

for topic in scores_df['topic'].value_counts().index:
    mean_score = round(scores_df[scores_df['topic'] == topic]['polarity_score'].mean(), 2)
    average_topic_polarity_scores.append(mean_score)

for source in scores_df['source'].value_counts().index:
    mean_score = round(scores_df[scores_df['source'] == source]['polarity_score'].mean(), 2)
    for i in range(8):
        average_source_polarity_scores.append(mean_score)

scores_df['average_polarity_for_topic'] = average_topic_polarity_scores * 9
scores_df['average_polarity_for_source'] = average_source_polarity_scores

average_topic_subjectivity_scores = []
average_source_subjectivity_scores = []

for topic in scores_df['topic'].value_counts().index:
    mean_score = round(scores_df[scores_df['topic'] == topic]['subjectivity_score'].mean(), 2)
    average_topic_subjectivity_scores.append(mean_score)

for source in scores_df['source'].value_counts().index:
    mean_score = round(scores_df[scores_df['source'] == source]['subjectivity_score'].mean(), 2)
    for i in range(8):
        average_source_subjectivity_scores.append(mean_score)

scores_df['average_subjectivity_for_topic'] = average_topic_subjectivity_scores * 9
scores_df['average_subjectivity_for_source'] = average_source_subjectivity_scores

scores_df['polarity_diff_from_topic_mean'] = scores_df['polarity_score'] - scores_df['average_polarity_for_topic']
scores_df['subjectivity_diff_from_topic_mean'] = scores_df['subjectivity_score'] - scores_df['average_subjectivity_for_topic']
scores_df['polarity_diff_from_source_mean'] = scores_df['polarity_score'] - scores_df['average_polarity_for_source']
scores_df['subjectivity_diff_from_source_mean'] = scores_df['subjectivity_score'] - scores_df['average_subjectivity_for_source']

sources_polarity_dev = []

for source in scores_df['source'].value_counts().index:
    total_dev = scores_df[scores_df['source'] == source]['polarity_diff_from_topic_mean'].sum()
    for i in range(8):
        sources_polarity_dev.append(total_dev/8)

sources_subjectivity_dev = []

for source in scores_df['source'].value_counts().index:
    total_dev = scores_df[scores_df['source'] == source]['subjectivity_diff_from_topic_mean'].sum()
    for i in range(8):
        sources_subjectivity_dev.append(total_dev/8)

scores_df['average_source_polarity_deviation_from_topic_mean'] = sources_polarity_dev
scores_df['average_source_subjectivity_deviation_from_topic_mean'] = sources_subjectivity_dev

scores_df['polarity_sign'] = np.where(scores_df['polarity_score'] > 0, 'pos', 'neg')
scores_df['polarity_magnitude'] = abs(scores_df['polarity_score'])

scores_df.head(5)
```

Since the documentation of the calculation of these scores is limited, one may wonder if they are somehow related. That is, are polarity scores related to subjectivity scores in some way? A scatterplot is created to see if a higher polarity score magnitude (absolute value) may mean a higher subjectivity score as well.

```{python}
# to better understand subjectivity scores: are polarity scores and subjectivity scores correlated?
# this may suggest that polarity score is used in the calculation of a subjectivity score

(
ggplot(scores_df, aes(x="polarity_magnitude", y="subjectivity_score", color='topic'))
+ geom_point()
+ labs(x="Absolute Value of Polarity Score",
       y='Subjectivity Score',
       title='Are Polarity Score Magnitudes Correlated with Subjectivity Scores?')
)
```

There does not appear to be a linear or any other sort of relationship between the polarity and subjectivity scores of the articles used in this project. However, the graph does show almost a cluster of points of the same color, telling that the articles about the Supreme Court ruling against affirmative action tend to have stronger polarity scores. The same graph may be visualized colored by source instead, to see if any clusters of sources are observed.


```{python}
(
ggplot(scores_df, aes(x="polarity_magnitude", y="subjectivity_score", color='source'))
+ geom_point()
+ labs(x="Absolute Value of Polarity Score",
       y='Subjectivity Score',
       title='Are Polarity Score Magnitudes Correlated with Subjectivity Scores?')
)
```

Something that can be observed on the scatterplot above is that the green dots, both for CNN and Fox News, tend to be on the lower half of the plot, meaning their articles tend to have lower subjectivity scores. In addition, the dark blue dots, representing The New York Times, tend to be on the upper half, signifying higher subjectivity scores for its articles. 

These scores will now be compared across sources in a different, more comprehensive way. It is intuitive that some of the more grim topics, like the deadly Hamas attack, should have a negative polarity score. Other more neutral topics may have positive polarity scores. Taking an average of the polarity scores would not be a good measure since the negative and positive scores could cancel each other out and not give a clear picture of the connotation of each source compared to others. Instead, the average polarity score is computed for each *topic*. Then, the average polarity score for the article's topic is subtracted from the article's polarity score to get its deviation from the mean. For each source, the average of these deviations is computed, which will tell if a source tends to write in a more negative or positive connotation, even given the nature of the topic it is writing about.

```{python}
# create a df for each source and its polarity score total deviations from the topic means
from pandas.api.types import CategoricalDtype

polarity_devs = pd.DataFrame({'source':scores_df['source'].value_counts().index, 
                              'polarity_dev':scores_df[scores_df['topic'] == 'Chinese Surveillance Balloon']\
                                ['average_source_polarity_deviation_from_topic_mean']})
polarity_devs['sign'] = np.where(polarity_devs['polarity_dev'] > 0, 'pos', 'neg')

polarity_devs = polarity_devs.reindex(polarity_devs['polarity_dev'].abs().sort_values(ascending=False).index)
polarity_devs.reset_index(drop=True, inplace=True)

source_order = CategoricalDtype(
    ["New York Times", "Wall Street Journal", "ABC News", "NBC News", 
     "CNN", "New York Post", "Washington Post", "Fox News", "BBC"], 
    ordered=False
)
polarity_devs['source'] = polarity_devs['source'].astype(source_order)

polarity_devs

# create bar chart representing how much, on average, a source's article
# deviates away from that topic's mean polarity score
# intended to show if a source tends to be more negative/positive than average, 
# even accounting for the nature of the topic

colors = {"pos":'#ffbf00', "neg":'#b7141c'}  

(
ggplot(polarity_devs, aes(x="source", y="polarity_dev", fill="sign"))
+ geom_col(stat="identity")
+ labs(x="Source", y='Average Article Polarity Score Deviation', title='Average Source Polarity Score Deviations from Mean')
+ theme(figure_size = (10, 4), axis_text_x=element_text(size=7, face='bold'), 
        title = element_text(size=13),legend_position = "none")
+ scale_fill_manual(values = colors)
)
```

The visualization above tells us that The New York Times articles tend to be the most negative on average. They have the largest deviations from the topic's average polarity scores, which may indicate stronger wording. Each individual may infer what they wish with these results, but definitive conclusions should not be drawn from them, which will be explained a bit later. ABC News tends to write their articles more positive than average for the topics' mean polarity scores. BBC's articles are the closest to average polarity scores for each topic. The polarity scores may be viewed individually for the set of articles of any topic. Below, we see the polarity scores for the articles about the ruling against affirmative action. The New York Times was the only source with a negative polarity score for this topic, which could already be suggestive of its political views. 

```{python}
(
ggplot(scores_df[scores_df['topic']=="Supreme Court Ruling on Affirmative Action"],
       aes(x="reorder(source, -polarity_score)", y="polarity_score", fill="polarity_sign"))
+ geom_col(stat="identity")
+ labs(x='Source', y='Polarity Score', title='Polarity Scores for Ban on Affirmative Action Articles')
+ theme(figure_size = (10, 4), axis_text_x=element_text(size=8.5, face='bold'), 
        title = element_text(size=13), legend_position = "none")
+ scale_fill_manual(values = colors)
)
```

Now let's look at a comparison of each source's average subjectivity scores. These do not need to be broken up by topic, since the scale for the score is [0,1].

```{python}
# create a df with average subjectivity scores per source

source_avg_subj = pd.DataFrame({'source':scores_df['source'].value_counts().index, 
                              'avg_subj':scores_df[scores_df['topic'] == 'Chinese Surveillance Balloon']\
                                ['average_subjectivity_for_source']})
source_avg_subj.sort_values(by='avg_subj', ascending=True, inplace=True)
source_avg_subj.reset_index(drop=True, inplace=True)

# order in ascending order of subjectivity
source_order = CategoricalDtype(
    ["Fox News", "Wall Street Journal", "CNN", 
     "Washington Post", "BBC", "New York Post", "ABC News", "NBC News", "New York Times"], 
    ordered=False
)
source_avg_subj['source'] = source_avg_subj['source'].astype(source_order)

source_avg_subj

# bar chart showing avg subjectivity per source

(
ggplot(source_avg_subj, aes(x="source", y="avg_subj"))
+ geom_col(stat="identity", fill="#ffa140")
+ labs(x='Source', y='Average Subjectivity Score', title='Average Subjectivity Score by Source')
+ theme(figure_size = (10, 4), axis_text_x=element_text(size=8.5, face='bold'), 
        title = element_text(size=13), legend_position = "none")
)
```

The bar chart above shows that Fox News has the lowest subjectivity scores on average, while the New York Times has the highest subjectivity score on average. Surprised? Do these results align with your opinions or what you commonly hear about these sources? Regardless, it is important to remember what these scores mean exactly and what we are able to reasonably infer from these results. The package that computes these scores has limited documentation, so the exact formulas are not known for the computation of a polarity or subjectivity score of a text. Each individual word has a scoring of its own and is marked as positive or negative, and the overall polarity score is some sort of aggregation of these scores. This does limit us in the interpretation of the scores. Moreover, this is only a small sample of articles collected from each source and their insights should not represent a source as a whole. It is entirely possible that a larger sample of articles from a wider variety of topics could yield different results.

To further analyze the polarity scores, we may want to know which ones are outliers. So, for each topic, the interquartile range formula is used to find any articles with an outlier score. 

```{python}
# create a list of tuples that contain articles that have an outlier polarity score,
# compared to other articles about the same topic

articles =[]

for topic in scores_df['topic'].value_counts().index:
    topic_df = scores_df[scores_df['topic'] == topic]
    Q1 = topic_df['polarity_score'].describe()[4]
    Q3 = topic_df['polarity_score'].describe()[6]
    IQR = Q3-Q1
    lower = Q1-(1.5*IQR)
    upper = Q3+(1.5*IQR)
    # the line below would print the lower and upper bounds for non-outliers for each topic
    #print(topic, "lower bound:", round(lower, 2), "upper bound:", round(upper,2))
    for row in topic_df.itertuples():
        if row.polarity_score < lower or row.polarity_score > upper:
            articles.append((row.source, row.topic, row.polarity_score))

print("The source, topic, and polarity score of articles with an outlier polarity score:")
for article in articles:
  print(article)
```

From looking at outlier polarity scores, the articles about the Supreme Court ruling against affirmative action have several outlier scores, both positive and negative. This could be telling of the major difference in opinions regarding this topic. As for sources, The Wall Street Journal had the most amount of articles with an outlier polarity score.

# Predictive Modeling

Now, for the main part of this project, a few different classifiers will be used to attempt to predict the source of a given article. If this can be done with high accuracy, it will show that the wording alone of the studied sources is distinct enough to be an identifying factor. First, Naive Bayes classifiers will be trained and tested on the articles included. Six topics will be used for training, and two for testing. As many classifiers will be created as needed (28) in order to use every possible split of topics for the training and testing sets. In addition, this process will be repeated for each of the three dataframes created (using binary values, frequencies, and TF-IDF scores of tokens) for a total of 84 Naive Bayes classifiers. This means that the models will be trained on the tokens (words) that appear in each model and their values (one of the three possible mentioned).

```{python}
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics  
from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score

best_accuracies_by_cf = {}

# function to apply Naive Bayes model using different holdout article topics each time

topic_list = topic_choices  = [
      "Supreme Court Ruling on Affirmative Action"
    , "Chinese Surveillance Balloon"
    , "Biden's Low Approval Rates in Polls"
    , "The Deadliest Attack by Hamas"
    , "Pentagon Documents Leak"
    , "George Santos' Expulsion from Congress"
    , "U.S. and Germany Send Tanks to Ukraine"
    , "Trump's Indictment"
]

def naive_bayes_cf(df, value_type):
    '''Takes in a dataframe of tokens and the desired values 
    to train the model on (i.e. tfidf scores, frequencies)
    and a string representing what the values are.
    Trains, tests, and outputs performance metrics of a series
    of Multinomial Naive Bayes classifiers, one for each
    possible holdout set of two article topics.'''

    accuracy_list = []
    f1_score_list = []

    for i in range(len(topic_list)):
        for j in range(i+1, len(topic_list)):
            # uncomment lines below to see the holdout topics for the model
            #print("Holdout article topics: ")
            #print(topic_list[i])
            #print(topic_list[j])
            X_train_df = df[(df['article_topic'] != topic_list[i]) & 
                    (df['article_topic'] != topic_list[j])].iloc[:, :-2]
            X_test_df = df[(df['article_topic'] == topic_list[i]) | 
                    (df['article_topic'] == topic_list[j])].iloc[:, :-2]
            y_train_df = df[(df['article_topic'] != topic_list[i]) & 
                    (df['article_topic'] != topic_list[j])]['article_source']
            y_test_df = df[(df['article_topic'] == topic_list[i]) | 
                    (df['article_topic'] == topic_list[j])]['article_source']
            X_train = X_train_df.to_numpy()
            X_test = X_test_df.to_numpy()
            y_train = y_train_df.to_numpy()
            y_test = y_test_df.to_numpy()

            nb_model = MultinomialNB()
            nb_model.fit(X_train, y_train)

            predictions_array = nb_model.predict(X_test)

            prediction_list = []

            for pred in predictions_array:
                prediction_list.append(pred)

            actuals_list = []

            for source in y_test:
                actuals_list.append(source)

            preds_actuals_df = pd.DataFrame({'Actual':actuals_list, 'Predicted':prediction_list})

            #display(preds_actuals_df)

            accuracy = round(accuracy_score(predictions_array, y_test), 2)
            f1 = round(f1_score(predictions_array, y_test, average="weighted"), 2)

            accuracy_list.append(accuracy)
            try:
                if accuracy > best_accuracies_by_cf['Naive Bayes']:
                    best_accuracies_by_cf['Naive Bayes'] = accuracy
            except:
                best_accuracies_by_cf['Naive Bayes'] = accuracy
            f1_score_list.append(f1)

            # uncomment lines below to display results of every model
            #print("Accuracy of Multinomial Naive Bayes Model:", accuracy)
            #print("F1 score of Multinomial Naive Bayes Model:", f1)
            #print("\n\n")

    print(f"Minimum Accuracy Score of any Multinomial Naive Bayes Model using {value_type}:",
        min(accuracy_list))
    print(f"Maximum Accuracy Score of any Multinomial Naive Bayes Model using {value_type}:",
        max(accuracy_list))
    print(f"Maximum F1 Score of any Multinomial Naive Bayes Model using {value_type}:",
        max(f1_score_list))

dfs = [(tfidf_df, "TFIDF Score"), (binary_df, "Binary Values"), (freq_df, "Frequencies")]

for df in dfs:
    naive_bayes_cf(df[0], df[1])
```

For the series of Naive Bayes models created, the maximum accuracy score reached for any model is 44%. This was for a model trained on TF-IDF scores. This accuracy does not seem very high. Even the best of these models' predictions are wrong more often than they are right. However, since there are nine possible predictions that could be made (nine different sources), a random guess would expect to result in a 11% accuracy rate. This happens to also be the minimum accuracy score of any of these models. Therefore, these classifiers are just as good or better than guessing which source an article comes from, simply given its text. 

The same will be repeated with Random Forest models. In addition to using various holdout topic sets and dataframes to train and test these models on, a couple parameters will also be tested to see which ones result in the best performing model. 

```{python}
from sklearn.ensemble import RandomForestClassifier

best_rf_params = {}
df_list = []
best_model_pred_vs_actuals = pd.DataFrame()
average_accuracies = []

def random_forest_cf(df, value_type, n, crit):
    '''Takes in a dataframe of tokens and the desired values 
    to train the model on (i.e. tfidf scores, frequencies)
    and a string representing what the values are. 
    Also takes in specifications for n_estimators and criterion
    parameters of the Random Forest classifier. Trains, tests,
    and outputs performance metrics of a series of RF classifiers, 
    one for each possible holdout set of two article topics.'''

    accuracy_list = []
    f1_score_list = []
    best_accuracies_by_cf['Random Forest'] = 0

    for i in range(len(topic_list)):
        for j in range(i+1, len(topic_list)):
            #print("Holdout article topics: ")
            #print(topic_list[i])
            #print(topic_list[j])
            X_train_df = df[(df['article_topic'] != topic_list[i]) & 
                    (df['article_topic'] != topic_list[j])].iloc[:, :-2]
            X_test_df = df[(df['article_topic'] == topic_list[i]) | 
                    (df['article_topic'] == topic_list[j])].iloc[:, :-2]
            y_train_df = df[(df['article_topic'] != topic_list[i]) & 
                    (df['article_topic'] != topic_list[j])]['article_source']
            y_test_df = df[(df['article_topic'] == topic_list[i]) | 
                    (df['article_topic'] == topic_list[j])]['article_source']
            X_train = X_train_df.to_numpy()
            X_test = X_test_df.to_numpy()
            y_train = y_train_df.to_numpy()
            y_test = y_test_df.to_numpy()

            rf_model = RandomForestClassifier(n_estimators = n, criterion=crit, random_state=42)
            rf_model.fit(X_train, y_train)

            predictions_array = rf_model.predict(X_test)

            prediction_list = []

            for pred in predictions_array:
                prediction_list.append(pred)

            actuals_list = []

            for source in y_test:
                actuals_list.append(source)

            preds_actuals_df = pd.DataFrame({'Actual':actuals_list, 'Predicted':prediction_list})

            #display(preds_actuals_df)

            accuracy = round(accuracy_score(predictions_array, y_test), 2)
            f1 = round(f1_score(predictions_array, y_test, average="weighted"), 2)

            accuracy_list.append(accuracy)
            try:
                if accuracy > best_accuracies_by_cf['Random Forest']:
                    best_accuracies_by_cf['Random Forest'] = accuracy
                    best_rf_params['n_estimators'] = n
                    best_rf_params['criterion'] = crit 
                    best_rf_params['value'] = value_type
                    best_rf_params['holdout_topics'] = (topic_list[i], topic_list[j])
                    best_model_pred_vs_actuals = preds_actuals_df.copy()
            except:
                best_accuracies_by_cf['Random Forest'] = accuracy
                best_rf_params['n_estimators'] = n
                best_rf_params['criterion'] = crit 
            f1_score_list.append(f1)

    cum_acc = 0

    for acc in accuracy_list:
        cum_acc += acc

    average_accuracies.append((cum_acc/len(accuracy_list), value_type, n, crit))

            #print("Accuracy of Random Forest Model:", accuracy)
            #print("F1 score of Random Forest Model:", f1)
            #print("\n\n")

    print(f"Minimum Accuracy Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",
          min(accuracy_list))
    print(f"Maximum Accuracy Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",
          max(accuracy_list))
    #print(f"Maximum F1 Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",
          #max(f1_score_list))
    print("\n")

crits = ['gini', 'entropy']
n_ests = [10, 100, 200]

for df in dfs:
    for crit in crits:
        for n in n_ests:
            random_forest_cf(df[0], df[1], n, crit)

```

The best random forest classifier upon one iteration of this code had an accuracy score of 78%. This was using the dataframe with binary values for tokens, 200 n_estimators, and gini criterion. Using these same parameters and the same dataframe, the worst of these models (with a different holdout set) had an accuracy score of 22%. Of course, there is an inherently random element for this type of classifier, so results may differ each time. However, many of the random forest models trained and tested thusfar have been able to reach a similar accuracy score. This can be considered a very good accuracy score for our purposes! Using the right parameters and training/testing sets, we could correctly predict the source of an article three out of every four times. As seen, however, even using the best parameters in the random forest models, the accuracy can wildly differ (22-78%) depending on which topics were used for training and testing. It is reasonable to assume that this could be remedied with a much larger sample of articles in the future, which could provide a rich set for both training and testing data. 

::: {.callout-caution collapse="true"}
## Articles used in project
@abcaffirmative
@abcballoon
@abcbiden
@abchamas
@abcpentagon
@abcsantos
@abctanks
@abctrump
@bbcaffirmative
@bbcballoon
@bbcbiden
@bbchamas
@bbcpentagon
@bbcsantos
@bbctanks
@bbctrump
@cnnaffirmative
@cnnballoon
@cnnbiden
@cnnhamas
@cnnpentagon
@cnnsantos
@cnntanks
@cnntrump
@foxaffirmative
@foxballoon
@foxbiden
@foxhamas
@foxpentagon
@foxsantos
@foxtanks
@foxtrump
@nbcaffirmative
@nbcballoon
@nbcbiden
@nbchamas
@nbcpentagon
@nbcsantos
@nbctanks
@nbctrump
@nytaffirmative
@nytballoon
@nytbiden
@nythamas
@nytpentagon
@nytsantos
@nyttanks
@nyttrump
@nypaffirmative
@nypballoon
@nypbiden
@nyphamas
@nyppentagon
@nypsantos
@nyptanks
@nyptrump
@wsjaffirmative
@wsjballoon
@wsjbiden
@wsjhamas
@wsjpentagon
@wsjsantos
@wsjtanks
@wsjtrump
@wpaffirmative
@wpballoon
@wpbiden
@wphamas
@wppentagon
@wpsantos
@wptanks
@wptrump
:::