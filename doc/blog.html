<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kamila Palys">

<title>How Differently Do News Sources Report on the Same Events?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="blog_files/libs/clipboard/clipboard.min.js"></script>
<script src="blog_files/libs/quarto-html/quarto.js"></script>
<script src="blog_files/libs/quarto-html/popper.min.js"></script>
<script src="blog_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="blog_files/libs/quarto-html/anchor.min.js"></script>
<link href="blog_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="blog_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="blog_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="blog_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="blog_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-language-of-news-sources" id="toc-the-language-of-news-sources" class="nav-link active" data-scroll-target="#the-language-of-news-sources">The Language of News Sources</a></li>
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection">Data Collection</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#dataframes" id="toc-dataframes" class="nav-link" data-scroll-target="#dataframes">Dataframes</a></li>
  <li><a href="#exploratory-analysis" id="toc-exploratory-analysis" class="nav-link" data-scroll-target="#exploratory-analysis">Exploratory Analysis</a>
  <ul class="collapse">
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering">Clustering</a></li>
  <li><a href="#wordclouds" id="toc-wordclouds" class="nav-link" data-scroll-target="#wordclouds">Wordclouds</a>
  <ul class="collapse">
  <li><a href="#does-the-length-of-an-article-have-to-do-with-how-frequently-a-certain-word-appears" id="toc-does-the-length-of-an-article-have-to-do-with-how-frequently-a-certain-word-appears" class="nav-link" data-scroll-target="#does-the-length-of-an-article-have-to-do-with-how-frequently-a-certain-word-appears">Does the length of an article have to do with how frequently a certain word appears?</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sentiment-analysis" id="toc-sentiment-analysis" class="nav-link" data-scroll-target="#sentiment-analysis">Sentiment Analysis</a></li>
  <li><a href="#predictive-modeling" id="toc-predictive-modeling" class="nav-link" data-scroll-target="#predictive-modeling">Predictive Modeling</a>
  <ul class="collapse">
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes">Naive Bayes</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">Random Forest</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future Work</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">How Differently Do News Sources Report on the Same Events?</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">Spring 2024</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kamila Palys </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="newspaper.jpg" class="img-fluid figure-img" alt="A man reading a newspaper."></p>
<figcaption class="figure-caption">Source: Unsplash</figcaption>
</figure>
</div>
<section id="the-language-of-news-sources" class="level1">
<h1>The Language of News Sources</h1>
<p>By now, you’ve surely heard many opinions and likely have one yourself about which news sources are trustworthy and not. These opinions are usually divided by political party identification. People’s views on these matters are also largely subjective, but can they be confirmed or changed with a more technical analysis on news sources? What can we tell about each news source just based on their choice of words? This project takes a closer look at the language that various news sources use and examines if these sources really report on the same events in a different manner. This will be done through sentiment analysis and creating classifiers to see if we can determine the source of an article, given the article text.</p>
</section>
<section id="data-collection" class="level1">
<h1>Data Collection</h1>
<p>Nine total news sites are studied for a mix of political affiliations, including CNN, The New York Times, The Washington Post, Fox News, New York Post, NBC News, The Wall Street Journal, BBC, and ABC News. Eight political events and topics from 2023 were chosen as the subjects for the articles to be studied for a higher potential for bias. For each topic, one article from each source was collected, making for a total of 72 articles. To ensure as little room for extraneous variables as possible, the articles for a particular topic across sources were chosen in a way so that they were published on the same day or as closely together in time as possible. This would mean that each source should have had the same information available to them when writing the article. The text from each article was collected through copying and pasting into its own text file. In addition, a string denoting each article’s topic and source name was added to the end of each article’s text file to be able to identify it.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Topics chosen for article selection
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Indictment of Trump</p></li>
<li><p>Supreme Court’s ruling against affirmative action</p></li>
<li><p>Biden’s low approval rates in polls</p></li>
<li><p>Deadliest attack by Hamas to date</p></li>
<li><p>Chinese surveillance balloon</p></li>
<li><p>Pentagon document leaks</p></li>
<li><p>Sending tanks to Ukraine</p></li>
<li><p>Expulsion of George Santos</p></li>
</ul>
</div>
</div>
</section>
<section id="data-preprocessing" class="level1">
<h1>Data Preprocessing</h1>
<p>The text had to go through a lot of preparation before it could be ready for analyzing. Functions were created to make this process faster for each article. One function accesses an article’s text file and returns a string of all the text in there. Another function cleans the text, which includes making all text lowercase, removing punctuation, removing stopwords (words like “and,” “the,” etc.), and stemming words, if desired. Stemming words keeps only the root form of a word, removing suffixes so that words like “jumps” and “jumping” are both reduced to “jump.” This is desired for some parts of the analysis in this project, but not others. The idea to create the two preprocessing functions used and what they may include was taken from <a href="https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/">Machine Learning Mastery</a> (see <span class="citation" data-cites="preprocessing">Brownlee (<a href="#ref-preprocessing" role="doc-biblioref">2020</a>)</span>).</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># don't show future warnings</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#warnings.simplefilter(action='ignore', category=FutureWarning)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize dictionary to be able to trace back a stemmed word to its original form</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>stemmed_dict <span class="op">=</span> {}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> file_to_string(filename):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">'''Opens the input text file and</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">  returns a string of all its text.'''</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="bu">file</span>.close()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>, <span class="st">' '</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'  '</span>, <span class="st">' '</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> text</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text, stem):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">'''Takes in a string of text and cleans it by converting</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">  to lowercase, removing punctuation, and removing stopwords. </span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">  Also takes in a binary value to indicate if stemming should</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">  be performed. Returns the new string.'''</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> stem <span class="kw">not</span> <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>      <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Stem must be a binary value (0 or 1)"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  ps <span class="op">=</span> PorterStemmer()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  stemmed_dict <span class="op">=</span> {}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create list of stopwords </span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  stopwords_list <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># make the text lowercase</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.lower()</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'—'</span>, <span class="st">' '</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the three below would be stemmed to "u", rather than "us" for example</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'u.s.'</span>, <span class="st">'us'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'u.k.'</span>, <span class="st">'uk'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'u.n.'</span>, <span class="st">'un'</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># convert to ascii characters</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.encode(<span class="st">"ascii"</span>, <span class="st">"ignore"</span>).decode()</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="bu">chr</span> <span class="kw">in</span> text:</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>      <span class="co"># only keep characters in the string that are not punctuation symbols</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (<span class="bu">chr</span> <span class="kw">in</span> string.punctuation <span class="kw">or</span> <span class="bu">chr</span> <span class="kw">in</span> string.digits):</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>          text <span class="op">=</span> text.replace(<span class="bu">chr</span>, <span class="st">' '</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'  '</span>, <span class="st">' '</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># stem the tokens within the text</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  tokens <span class="op">=</span> text.split()</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>  new_tokens <span class="op">=</span> []</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> token <span class="kw">in</span> tokens[:<span class="op">-</span><span class="dv">2</span>]: <span class="co"># last two tokens identify source and topic, we do not want to stem them</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>      <span class="co"># only include new token in the cleaned list if not a stopword</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> stopwords_list:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span> stem <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>              stemmed_word <span class="op">=</span> ps.stem(token)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>              new_tokens.append(stemmed_word)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>              <span class="co"># to be able to map each token to the resulting stemmed word</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>              <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> stemmed_dict:</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>                  stemmed_dict[token] <span class="op">=</span> stemmed_word</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>          <span class="cf">else</span>:</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>              new_tokens.append(token)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>  <span class="co"># add back in last two tokens</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>  new_tokens.append(tokens[<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>  new_tokens.append(tokens[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>  cleaned_text <span class="op">=</span> <span class="st">" "</span>.join(new_tokens)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>  cleaned_text <span class="op">=</span> cleaned_text.replace(<span class="st">'  '</span>, <span class="st">' '</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> cleaned_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="dataframes" class="level1">
<h1>Dataframes</h1>
<p>All of the article text collected is then transformed into multiple dataframe versions. One way of representing textual data within a dataframe is with binary values. In this case, each column is a token, or a word, that appears in any of the documents (articles) collected. Each row represents one document. The values are either a 0 or 1, denoting whether or not the token appears within the article. Another dataframe representation has the values instead be the frequencies of each token within an article. Finally, a third dataframe is created with the Term Frequency-Inverse Document Frequency (TF-IDF) scores of the tokens within an article as the values. A TF-IDF score signifies the importance and uniqueness of a word within an article, as it compares to the remaining documents. Some of these dataframes may work better than others when it comes to the predictive modeling portion of the project and revealing patterns within the text of a news source. An example dataframe is shown below, with the TF-IDF scores of each stemmed token shown for the first five articles (rows).</p>
<p>For the clustering part of the analysis, a new dataframe is also created with each row representing not just one article, but all articles from a single source. This is because we will be clustering entire sources and not individual articles.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> PorterStemmer</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> string <span class="im">import</span> punctuation</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># looping through all text files to apply preprocessing functions</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>article_docs <span class="op">=</span> []</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> os.listdir(<span class="st">'data/text/'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span>.sort()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> filename <span class="kw">in</span> <span class="bu">dir</span>:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    filepath <span class="op">=</span> os.path.join(<span class="st">'data/text/'</span>, filename)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> filename.split(<span class="st">"."</span>)[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">"txt"</span>:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        article_string <span class="op">=</span> file_to_string(filepath)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        new_string <span class="op">=</span> clean_text(article_string, <span class="dv">1</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        article_docs.append(new_string)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the list of article strings into a binary-value dataframe</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> Tokenizer()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>t.fit_on_texts(article_docs)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>encoded_docs <span class="op">=</span> t.texts_to_matrix(article_docs, mode<span class="op">=</span><span class="st">'binary'</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> t.word_index.keys()]</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>binary_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_docs[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>      binary_df[<span class="st">'abcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'bbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'cnnarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'foxarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'nbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'nyparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'nytarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'wparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'wsjarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>topic_conditions <span class="op">=</span> [</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>      binary_df[<span class="st">'affirmativearticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'balloonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'bidenarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'hamasarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'pentagonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'santosarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'tanksarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'trumparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>topic_choices  <span class="op">=</span> [</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>binary_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new topic column</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>binary_df[<span class="st">"article_topic"</span>] <span class="op">=</span> np.select(topic_conditions, topic_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the token columns that identify the topic/source, not part of actual article text</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>binary_df.drop(columns<span class="op">=</span>[<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>, <span class="st">'nbcarticle'</span>,</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>, <span class="st">'wsjarticle'</span>, <span class="st">'affirmativearticle'</span>,</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'balloonarticle'</span>, <span class="st">'bidenarticle'</span>, <span class="st">'hamasarticle'</span>, <span class="st">'pentagonarticle'</span>,</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'santosarticle'</span>, <span class="st">'tanksarticle'</span>, <span class="st">'trumparticle'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>binary_df.head()</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>encoded_docs_freq <span class="op">=</span> t.texts_to_matrix(article_docs, mode<span class="op">=</span><span class="st">'count'</span>)</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>freq_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_docs_freq[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>      freq_df[<span class="st">'abcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'bbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'cnnarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'foxarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'nbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'nyparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'nytarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'wparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'wsjarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>topic_conditions <span class="op">=</span> [</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>      freq_df[<span class="st">'affirmativearticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'balloonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'bidenarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'hamasarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'pentagonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'santosarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'tanksarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'trumparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>topic_choices  <span class="op">=</span> [</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>freq_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new topic column</span></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>freq_df[<span class="st">"article_topic"</span>] <span class="op">=</span> np.select(topic_conditions, topic_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the token columns that identify the topic/source, not part of actual article text</span></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>freq_df.drop(columns<span class="op">=</span>[<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>, <span class="st">'nbcarticle'</span>,</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>, <span class="st">'wsjarticle'</span>, <span class="st">'affirmativearticle'</span>,</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'balloonarticle'</span>, <span class="st">'bidenarticle'</span>, <span class="st">'hamasarticle'</span>, <span class="st">'pentagonarticle'</span>,</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'santosarticle'</span>, <span class="st">'tanksarticle'</span>, <span class="st">'trumparticle'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>freq_df.head()</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataframe with tf-idf values</span></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>encoded_docs_tfidf <span class="op">=</span> t.texts_to_matrix(article_docs, mode<span class="op">=</span><span class="st">'tfidf'</span>)</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>tfidf_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_docs_tfidf[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>      tfidf_df[<span class="st">'abcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'bbcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'cnnarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'foxarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'nbcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'nyparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'nytarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'wparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'wsjarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>topic_conditions <span class="op">=</span> [</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>      tfidf_df[<span class="st">'affirmativearticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'balloonarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'bidenarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'hamasarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'pentagonarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'santosarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'tanksarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'trumparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>topic_choices  <span class="op">=</span> [</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>tfidf_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new topic column</span></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>tfidf_df[<span class="st">"article_topic"</span>] <span class="op">=</span> np.select(topic_conditions, topic_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the token columns that identify the topic/source, not part of actual article text</span></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>tfidf_df.drop(columns<span class="op">=</span>[<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>, <span class="st">'nbcarticle'</span>,</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>, <span class="st">'wsjarticle'</span>, <span class="st">'affirmativearticle'</span>,</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'balloonarticle'</span>, <span class="st">'bidenarticle'</span>, <span class="st">'hamasarticle'</span>, <span class="st">'pentagonarticle'</span>,</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'santosarticle'</span>, <span class="st">'tanksarticle'</span>, <span class="st">'trumparticle'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of strings where each string is all articles from one source (for dendogram)</span></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>source_docs <span class="op">=</span> []</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>j <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>):</span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>    <span class="co"># looping through each article of each source, and only taking up until and not including second to last</span></span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token, bc last two tokens represent the name of the source and topic of article</span></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for last article, we index up until and not including only the last token bc we don't want to include</span></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the topic of the article, but we do want to include the source name, which is the second to last token</span></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> <span class="st">" "</span>.join(article_docs[j].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">1</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span>\</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">2</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">3</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span>\</span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">4</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">5</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span>\</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">6</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">7</span>].split()[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>    source_docs.append(source)</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a>    j <span class="op">+=</span> <span class="dv">8</span></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a>source_docs</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe of token tf-idf's with each row representing all articles of one source</span></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the list of article strings into a tf-idf-value dataframe</span></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> Tokenizer()</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a>t.fit_on_texts(source_docs)</span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>encoded_source_docs <span class="op">=</span> t.texts_to_matrix(source_docs, mode<span class="op">=</span><span class="st">'tfidf'</span>)</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> t.word_index.keys()]</span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>tfidf_source_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_source_docs[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>      tfidf_source_df[<span class="st">'abcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'bbcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'cnnarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'foxarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'nbcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'nyparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'nytarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'wparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df[<span class="st">'wsjarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>tfidf_source_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a><span class="co"># remove article_source column and make it the index</span></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a>tfidf_source_df.set_index(<span class="st">'article_source'</span>, inplace<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the columns for the tokens that were used to identify the articles</span></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>tfidf_source_df.drop([<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>,</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a>                      <span class="st">'nbcarticle'</span>, <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>,</span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a>                      <span class="st">'wsjarticle'</span>], axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a><span class="co"># tfidf_source_df</span></span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>tfidf_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">said</th>
<th data-quarto-table-cell-role="th">us</th>
<th data-quarto-table-cell-role="th">trump</th>
<th data-quarto-table-cell-role="th">biden</th>
<th data-quarto-table-cell-role="th">offici</th>
<th data-quarto-table-cell-role="th">mr</th>
<th data-quarto-table-cell-role="th">israel</th>
<th data-quarto-table-cell-role="th">presid</th>
<th data-quarto-table-cell-role="th">tank</th>
<th data-quarto-table-cell-role="th">hous</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">messr</th>
<th data-quarto-table-cell-role="th">overlap</th>
<th data-quarto-table-cell-role="th">vs</th>
<th data-quarto-table-cell-role="th">convert</th>
<th data-quarto-table-cell-role="th">marku</th>
<th data-quarto-table-cell-role="th">schreiber</th>
<th data-quarto-table-cell-role="th">riski</th>
<th data-quarto-table-cell-role="th">extort</th>
<th data-quarto-table-cell-role="th">article_source</th>
<th data-quarto-table-cell-role="th">article_topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1.827036</td>
<td>1.839130</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>ABC News</td>
<td>Supreme Court Ruling on Affirmative Action</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3.081563</td>
<td>2.857814</td>
<td>0.000000</td>
<td>2.267700</td>
<td>4.139471</td>
<td>0.0</td>
<td>0.000000</td>
<td>1.881491</td>
<td>0.000000</td>
<td>1.871958</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>ABC News</td>
<td>Chinese Surveillance Balloon</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2.238584</td>
<td>0.000000</td>
<td>5.086179</td>
<td>3.733245</td>
<td>0.000000</td>
<td>0.0</td>
<td>0.000000</td>
<td>2.428008</td>
<td>0.000000</td>
<td>2.128570</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>ABC News</td>
<td>Biden's Low Approval Rates in Polls</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2.683881</td>
<td>2.545320</td>
<td>0.000000</td>
<td>1.823774</td>
<td>2.667558</td>
<td>0.0</td>
<td>6.446654</td>
<td>1.334974</td>
<td>0.000000</td>
<td>0.891998</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>ABC News</td>
<td>The Deadliest Attack by Hamas</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2.238584</td>
<td>3.079532</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.493348</td>
<td>0.0</td>
<td>2.519533</td>
<td>1.334974</td>
<td>3.689062</td>
<td>0.000000</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>ABC News</td>
<td>Pentagon Documents Leak</td>
</tr>
</tbody>
</table>

<p>5 rows × 5290 columns</p>
</div>
</div>
</div>
</section>
<section id="exploratory-analysis" class="level1">
<h1>Exploratory Analysis</h1>
<section id="clustering" class="level2">
<h2 class="anchored" data-anchor-id="clustering">Clustering</h2>
<p>To begin exploring this data, we may be interested in clustering news sources together to see which ones are most similar to each other. This can be done in a dendrogram using the TF-IDF scores of their tokens, for example.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create dendrogram with complete linkage from tfidf scores df,</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># whose rows each represent one source</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> linkage(tfidf_source_df, <span class="st">'complete'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">4</span>))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">"Complete Linkage"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Source'</span>, fontsize<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.yticks(fontsize <span class="op">=</span> <span class="dv">8</span>) </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>dn <span class="op">=</span> dendrogram(Z, labels<span class="op">=</span>tfidf_source_df.index)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.xticks(fontsize <span class="op">=</span> <span class="dv">8</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-5-output-1.png" width="1084" height="385"></p>
</div>
</div>
<p>According to the dendrogram, ABC News and NBC News as well as BBC and Fox News are the two pairs of sources that are most similar to one another. Again, the articles were compared by looking at how similar their TF-IDF scores were for the same tokens. We also see that there aren’t two distinct groups of sources clustered together, while we may have expected sources to be clustered together based on their political affiliation. However, it’s important to keep in mind that these results are based on only the small sample of articles used for each source and may not be generalizable to the sources as wholes. It is also important to keep in mind that the similarity of these sources is solely being based on the vocabulary usage of each source and not any other factors.</p>
</section>
<section id="wordclouds" class="level2">
<h2 class="anchored" data-anchor-id="wordclouds">Wordclouds</h2>
<p>Another thing that could be interesting to look at are the wordclouds for each source. On a wordcloud, the bigger the word is, the more frequently it appeared within the text. Looking at these could give a sense of the difference in vocabulary between each source. This is why topic coverage was kept the same across all sources, so that comparisons like these may be made to scope out differences in wording, given that the sources are each writing about the same topics.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a function to be able to make a series of wordclouds, one for each source</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_wordcloud(text, title):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Given a string of all text and a string</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    for the title, creates a wordcloud.'''</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize <span class="op">=</span> (<span class="dv">6</span>,<span class="dv">3</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    wc <span class="op">=</span> WordCloud(colormap <span class="op">=</span> <span class="st">"YlOrRd"</span>, background_color<span class="op">=</span><span class="st">'white'</span>, width<span class="op">=</span><span class="dv">1600</span>, height<span class="op">=</span><span class="dv">800</span>, max_font_size <span class="op">=</span> <span class="dv">400</span>).generate(text)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    plt.title(title, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    title_snake <span class="op">=</span> title.lower().replace(<span class="st">" "</span>, <span class="st">"_"</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    wc.to_file(<span class="ss">f'wordcloud_</span><span class="sc">{</span>title_snake<span class="sc">}</span><span class="ss">.png'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout(pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    plt.imshow(wc)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the list of strings, each one representing all articles'</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># text from a given source, and create the wordcloud for it. </span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(source_docs)):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    create_wordcloud(source_docs[i], tfidf_source_df.index[i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-1.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-2.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-3.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-4.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-5.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-6.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-7.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-8.png" width="555" height="307"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-6-output-9.png" width="555" height="307"></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Looking at the wordclouds
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The words are not misspelled. Stemmed words were used for the creation of the wordclouds to avoid repetition of common words that just appear in various tenses.</p>
</div>
</div>
</div>
<p>The wordclouds for each source look mostly similar, with the most common word throughout them all being “said.” This is unsurprising. The only exception here is Fox News, whose most frequently appearing word throughout all the articles is “Trump.” Trump is a frequently appearing word for all sources because one of the topics chosen for the articles is explicitly about Trump. However, for Fox News, it is even more commonly used than the word “said.” This is interesting, but this alone should not provoke any conclusions about this source being biased towards Trump, for example. All that this means is that “Trump” is the word that appeared the most frequently throughout Fox News’ articles, which could potentially be because Fox News had the longest article about Trump. Longer articles will certainly have a higher count of words overall. This can be investigated further.</p>
<section id="does-the-length-of-an-article-have-to-do-with-how-frequently-a-certain-word-appears" class="level3">
<h3 class="anchored" data-anchor-id="does-the-length-of-an-article-have-to-do-with-how-frequently-a-certain-word-appears">Does the length of an article have to do with how frequently a certain word appears?</h3>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of unstemmed article document texts</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># looping through all text files to apply preprocessing functions</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotnine <span class="im">import</span> <span class="op">*</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>article_docs_unstemmed <span class="op">=</span> []</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> os.listdir(<span class="st">'data/text/'</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span>.sort()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> filename <span class="kw">in</span> <span class="bu">dir</span>:</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    filepath <span class="op">=</span> os.path.join(<span class="st">'data/text/'</span>, filename)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> filename.split(<span class="st">"."</span>)[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">"txt"</span>:</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        article_string <span class="op">=</span> file_to_string(filepath)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        new_string <span class="op">=</span> clean_text(article_string, <span class="dv">0</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        article_docs_unstemmed.append(new_string)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># investigating why Fox News has Trump as most frequent word,</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># to see if it's related to length of article </span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># create lists for the lengths of trump articles and the frequencies of word</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># "Trump" within the articles</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>trump_article_lengths <span class="op">=</span> []</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>trump_word_counts <span class="op">=</span> []</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> article_docs_unstemmed:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">8</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        trump_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word <span class="kw">in</span> article.split():</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word <span class="op">==</span> <span class="st">"trump"</span>:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>                trump_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        trump_word_counts.append(trump_count)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        article_length <span class="op">=</span> <span class="bu">len</span>(article.split())</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        trump_article_lengths.append(article_length)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co"># let's do the same for the word "biden" in the biden articles and see if there are outliers </span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># create lists for the lengths of biden articles and the frequencies of word</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co"># "Biden" within the articles</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>biden_article_lengths <span class="op">=</span> []</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>biden_word_counts <span class="op">=</span> []</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> article_docs_unstemmed:</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> article.split()[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">"bidenarticle"</span>:</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        biden_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word <span class="kw">in</span> article.split():</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word <span class="op">==</span> <span class="st">"biden"</span>:</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>                biden_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        biden_word_counts.append(biden_count)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        article_length <span class="op">=</span> <span class="bu">len</span>(article.split())</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        biden_article_lengths.append(article_length)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df with sources, their trump article lengths, and the frequency of the word "trump"</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>trump_words <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'source'</span>, <span class="st">'article_length'</span>, <span class="st">'trump_word_count'</span>])</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>trump_words[<span class="st">'source'</span>] <span class="op">=</span> tfidf_source_df.index</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>trump_words[<span class="st">'article_length'</span>] <span class="op">=</span> trump_article_lengths</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>trump_words[<span class="st">'trump_word_count'</span>] <span class="op">=</span> trump_word_counts</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>trump_words</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df with sources, their biden article lengths, and the frequency of the word "biden"</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>biden_words <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'source'</span>, <span class="st">'article_length'</span>, <span class="st">'biden_word_count'</span>])</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>biden_words[<span class="st">'source'</span>] <span class="op">=</span> tfidf_source_df.index</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>biden_words[<span class="st">'article_length'</span>] <span class="op">=</span> biden_article_lengths</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>biden_words[<span class="st">'biden_word_count'</span>] <span class="op">=</span> biden_word_counts</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a><span class="co"># create scatterplot of length of trump article vs frequency of the word "Trump"</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>ggplot(data<span class="op">=</span>trump_words,</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>       mapping<span class="op">=</span>aes(x<span class="op">=</span><span class="st">'article_length'</span>, y<span class="op">=</span><span class="st">'trump_word_count'</span>, color<span class="op">=</span><span class="st">'source'</span>))</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_point(show_legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_smooth(method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">"darkgrey"</span>)</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> labs(title<span class="op">=</span><span class="st">"Trump Article Length vs. Mentions of Trump"</span>,</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>              x<span class="op">=</span><span class="st">'# of Words in Article'</span>,</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>              y<span class="op">=</span><span class="st">'# of Times "Trump" Mentioned'</span>)</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a><span class="co"># create scatterplot of length of biden article vs frequency of the word "Biden"</span></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>ggplot(data<span class="op">=</span>biden_words,</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>       mapping<span class="op">=</span>aes(x<span class="op">=</span><span class="st">'article_length'</span>, y<span class="op">=</span><span class="st">'biden_word_count'</span>, color<span class="op">=</span><span class="st">'source'</span>))</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_point(show_legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_smooth(method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">"darkgrey"</span>)</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> labs(title<span class="op">=</span><span class="st">"Biden Article Length vs. Mentions of Biden"</span>,</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>              x<span class="op">=</span><span class="st">'# of Words in Article'</span>,</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>              y<span class="op">=</span><span class="st">'# of Times "Biden" Mentioned'</span>)</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure Size: (640 x 480)&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-7-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;Figure Size: (640 x 480)&gt;</code></pre>
</div>
</div>
<p>Above we used scatterplots to see how the length of the Trump articles are related to the frequency of the word “Trump” within them, by source. We do, in fact, see that the point for Fox News, corresponding to the article <span class="citation" data-cites="foxtrump">Singman and Dhanis (<a href="#ref-foxtrump" role="doc-biblioref">2023</a>)</span>, is an outlier. Given its article length, it does mention Trump a lot more frequently than the Trump articles from other sources. Out of curiosity, we look at the same visualization for the word “Biden” in the Biden articles. Interestingly, Fox News again appears to be an outlier (see <span class="citation" data-cites="foxbiden">Steinhauser (<a href="#ref-foxbiden" role="doc-biblioref">2023</a>)</span>). Given its article length, it also mentions Biden a lot more frequently than the remaining sources. Now, it is not as easy to make assumptions about the partiality of Fox News just based on these word frequencies. They could be more telling of the writing styles of each source, or it could also merely be a result of using such a small sample size of articles.</p>
</section>
</section>
</section>
<section id="sentiment-analysis" class="level1">
<h1>Sentiment Analysis</h1>
<p>Next, the connotation of each article and source overall is studied. The TextBlob package in Python provides polarity and subjectivity scores for any text string. A polarity score, on a scale of [-1, 1], tells if the text has a positive or negative connotation to it. Subjectivity scores, on a scale of [0, 1] tells how opinionated or subjective a text sounds as opposed to being factual. The higher the score, the more subjective the text is. A dataframe is created that displays the polarity and subjectivity scores of each article, with various other columns being created that help place each article’s score in the context of the entire corpus of documents and that will be used in following visualizations.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate polarity and subjectivity scores, create dataframe</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textblob <span class="im">import</span> TextBlob</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>scores_df <span class="op">=</span> pd.DataFrame({<span class="st">'source'</span>:tfidf_df[<span class="st">'article_source'</span>], <span class="st">'topic'</span>:tfidf_df[<span class="st">'article_topic'</span>]})</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># shorten some source names to fit on graph later</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'source'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'source'</span>] <span class="op">==</span> <span class="st">"The New York Times"</span>, <span class="st">"New York Times"</span>, scores_df[<span class="st">'source'</span>])</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'source'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'source'</span>] <span class="op">==</span> <span class="st">"The Washington Post"</span>, <span class="st">"Washington Post"</span>, scores_df[<span class="st">'source'</span>])</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'source'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'source'</span>] <span class="op">==</span> <span class="st">"The Wall Street Journal"</span>, <span class="st">"Wall Street Journal"</span>, scores_df[<span class="st">'source'</span>])</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize list of polarity and subjectivity scores</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>polarity_scores <span class="op">=</span> []</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>subjectivity_scores <span class="op">=</span> []</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the scores for each article and add them to the corresponding list of scores</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> article_docs_unstemmed:</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    polarity_scores.append(<span class="bu">round</span>(TextBlob(<span class="st">" "</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    .join(article.split()[:<span class="op">-</span><span class="dv">2</span>]))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    .sentiment.polarity, <span class="dv">2</span>))</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    subjectivity_scores.append(<span class="bu">round</span>(TextBlob(<span class="st">" "</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    .join(article.split()[:<span class="op">-</span><span class="dv">2</span>]))</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    .sentiment.subjectivity, <span class="dv">2</span>))</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># add a column in the df for the polarity and subjectivity scores, based on the lists</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_score'</span>] <span class="op">=</span> polarity_scores</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'subjectivity_score'</span>] <span class="op">=</span> subjectivity_scores</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize list of average polarity scores for each topic and source</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>average_topic_polarity_scores <span class="op">=</span> []</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>average_source_polarity_scores <span class="op">=</span> []</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the average polarity scores for each topic, add to that list</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> scores_df[<span class="st">'topic'</span>].value_counts().index:</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> topic][<span class="st">'polarity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    average_topic_polarity_scores.append(mean_score)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the average polarity scores for each source, add to that list</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'polarity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        average_source_polarity_scores.append(mean_score)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="co"># add columns for the two lists above, average polarity scores by topic and source</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_polarity_for_topic'</span>] <span class="op">=</span> average_topic_polarity_scores <span class="op">*</span> <span class="dv">9</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_polarity_for_source'</span>] <span class="op">=</span> average_source_polarity_scores</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize lists for average subjectivity scores by topic and source</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>average_topic_subjectivity_scores <span class="op">=</span> []</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>average_source_subjectivity_scores <span class="op">=</span> []</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate average subjectivity score by topic, add to that list</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> scores_df[<span class="st">'topic'</span>].value_counts().index:</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> topic][<span class="st">'subjectivity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    average_topic_subjectivity_scores.append(mean_score)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate average subjectivity score by source, add to that list</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'subjectivity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        average_source_subjectivity_scores.append(mean_score)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="co"># create columns for the two lists above, average subjectivity scores by topic and source</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_subjectivity_for_topic'</span>] <span class="op">=</span> average_topic_subjectivity_scores <span class="op">*</span> <span class="dv">9</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_subjectivity_for_source'</span>] <span class="op">=</span> average_source_subjectivity_scores</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="co"># create various new columns, for each score type's difference from the mean</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a><span class="co"># for that topic or source</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_diff_from_topic_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'polarity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_polarity_for_topic'</span>]</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'subjectivity_diff_from_topic_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'subjectivity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_subjectivity_for_topic'</span>]</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_diff_from_source_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'polarity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_polarity_for_source'</span>]</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'subjectivity_diff_from_source_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'subjectivity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_subjectivity_for_source'</span>]</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate how much on average each source deviates from the average polarity</span></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a><span class="co"># or subjectivity score for that topic, create columns for those calculations</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>sources_polarity_dev <span class="op">=</span> []</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>    total_dev <span class="op">=</span> scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'polarity_diff_from_topic_mean'</span>].<span class="bu">sum</span>()</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>        sources_polarity_dev.append(total_dev<span class="op">/</span><span class="dv">8</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>sources_subjectivity_dev <span class="op">=</span> []</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>    total_dev <span class="op">=</span> scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'subjectivity_diff_from_topic_mean'</span>].<span class="bu">sum</span>()</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>        sources_subjectivity_dev.append(total_dev<span class="op">/</span><span class="dv">8</span>)</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_source_polarity_deviation_from_topic_mean'</span>] <span class="op">=</span> sources_polarity_dev</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_source_subjectivity_deviation_from_topic_mean'</span>] <span class="op">=</span> sources_subjectivity_dev</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a><span class="co"># create column to indicate whether the polarity score is positive or negative,</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a><span class="co"># to color graphs by later</span></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_sign'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'polarity_score'</span>] <span class="op">&gt;</span> <span class="dv">0</span>, <span class="st">'pos'</span>, <span class="st">'neg'</span>)</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a><span class="co"># create column for absolute value of the polarity score</span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_magnitude'</span>] <span class="op">=</span> <span class="bu">abs</span>(scores_df[<span class="st">'polarity_score'</span>])</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>scores_df.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">source</th>
<th data-quarto-table-cell-role="th">topic</th>
<th data-quarto-table-cell-role="th">polarity_score</th>
<th data-quarto-table-cell-role="th">subjectivity_score</th>
<th data-quarto-table-cell-role="th">average_polarity_for_topic</th>
<th data-quarto-table-cell-role="th">average_polarity_for_source</th>
<th data-quarto-table-cell-role="th">average_subjectivity_for_topic</th>
<th data-quarto-table-cell-role="th">average_subjectivity_for_source</th>
<th data-quarto-table-cell-role="th">polarity_diff_from_topic_mean</th>
<th data-quarto-table-cell-role="th">subjectivity_diff_from_topic_mean</th>
<th data-quarto-table-cell-role="th">polarity_diff_from_source_mean</th>
<th data-quarto-table-cell-role="th">subjectivity_diff_from_source_mean</th>
<th data-quarto-table-cell-role="th">average_source_polarity_deviation_from_topic_mean</th>
<th data-quarto-table-cell-role="th">average_source_subjectivity_deviation_from_topic_mean</th>
<th data-quarto-table-cell-role="th">polarity_sign</th>
<th data-quarto-table-cell-role="th">polarity_magnitude</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>ABC News</td>
<td>Supreme Court Ruling on Affirmative Action</td>
<td>0.13</td>
<td>0.42</td>
<td>0.11</td>
<td>0.06</td>
<td>0.40</td>
<td>0.4</td>
<td>0.02</td>
<td>0.02</td>
<td>0.07</td>
<td>0.02</td>
<td>0.01</td>
<td>0.01375</td>
<td>pos</td>
<td>0.13</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>ABC News</td>
<td>Chinese Surveillance Balloon</td>
<td>0.05</td>
<td>0.36</td>
<td>0.04</td>
<td>0.06</td>
<td>0.37</td>
<td>0.4</td>
<td>0.01</td>
<td>-0.01</td>
<td>-0.01</td>
<td>-0.04</td>
<td>0.01</td>
<td>0.01375</td>
<td>pos</td>
<td>0.05</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>ABC News</td>
<td>Biden's Low Approval Rates in Polls</td>
<td>0.06</td>
<td>0.43</td>
<td>0.08</td>
<td>0.06</td>
<td>0.39</td>
<td>0.4</td>
<td>-0.02</td>
<td>0.04</td>
<td>0.00</td>
<td>0.03</td>
<td>0.01</td>
<td>0.01375</td>
<td>pos</td>
<td>0.06</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>ABC News</td>
<td>The Deadliest Attack by Hamas</td>
<td>0.10</td>
<td>0.39</td>
<td>0.02</td>
<td>0.06</td>
<td>0.36</td>
<td>0.4</td>
<td>0.08</td>
<td>0.03</td>
<td>0.04</td>
<td>-0.01</td>
<td>0.01</td>
<td>0.01375</td>
<td>pos</td>
<td>0.10</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>ABC News</td>
<td>Pentagon Documents Leak</td>
<td>0.03</td>
<td>0.32</td>
<td>0.05</td>
<td>0.06</td>
<td>0.40</td>
<td>0.4</td>
<td>-0.02</td>
<td>-0.08</td>
<td>-0.03</td>
<td>-0.08</td>
<td>0.01</td>
<td>0.01375</td>
<td>pos</td>
<td>0.03</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Since the documentation of the calculation of these scores is limited, one may wonder if they are somehow related. That is, are polarity scores related to subjectivity scores in some way? A scatterplot is created to see if a higher polarity score magnitude (absolute value) may mean a higher subjectivity score as well.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to better understand subjectivity scores: are polarity score magnitudes and subjectivity scores correlated?</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># this may suggest that polarity score is used in the calculation of a subjectivity score</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># colored by topic</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>ggplot(scores_df, aes(x<span class="op">=</span><span class="st">"polarity_magnitude"</span>, y<span class="op">=</span><span class="st">"subjectivity_score"</span>, color<span class="op">=</span><span class="st">'topic'</span>))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_point()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">"Absolute Value of Polarity Score"</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>       y<span class="op">=</span><span class="st">'Subjectivity Score'</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>       title<span class="op">=</span><span class="st">'Are Polarity Score Magnitudes Correlated with Subjectivity Scores?'</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>&lt;Figure Size: (640 x 480)&gt;</code></pre>
</div>
</div>
<p>There does not appear to be a linear or any other sort of relationship between the polarity and subjectivity scores of the articles used in this project. However, the graph does show almost a cluster of points of the same color, telling that the articles about the Supreme Court ruling against affirmative action tend to have stronger polarity scores. The same graph may be visualized colored by source instead, to see if any clusters of sources are observed.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># scatterplot to see if polarity magnitudes and subjectivity scores are</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># correlated, colored by source</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>ggplot(scores_df, aes(x<span class="op">=</span><span class="st">"polarity_magnitude"</span>, y<span class="op">=</span><span class="st">"subjectivity_score"</span>, color<span class="op">=</span><span class="st">'source'</span>))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_point()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">"Absolute Value of Polarity Score"</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>       y<span class="op">=</span><span class="st">'Subjectivity Score'</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>       title<span class="op">=</span><span class="st">'Are Polarity Score Magnitudes Correlated with Subjectivity Scores?'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>&lt;Figure Size: (640 x 480)&gt;</code></pre>
</div>
</div>
<p>Something that can be observed on the scatterplot above is that the green dots, both for CNN and Fox News, tend to be on the lower half of the plot, meaning their articles tend to have lower subjectivity scores. In addition, the dark blue dots, representing The New York Times, tend to be on the upper half, signifying higher subjectivity scores for its articles.</p>
<p>These scores will now be compared across sources in a different, more comprehensive way. It is intuitive that some of the more grim topics, like the deadly Hamas attack, should have a negative polarity score. Other more neutral topics may have positive polarity scores. Taking an average of the polarity scores would not be a good measure since the negative and positive scores could cancel each other out and not give a clear picture of the connotation of each source compared to others. Instead, the average polarity score is computed for each <em>topic</em>. Then, the average polarity score for the article’s topic is subtracted from the article’s polarity score to get its deviation from the mean. For each source, the average of these deviations is computed, which will tell if a source tends to write in a more negative or positive connotation, even given the nature of the topic it is writing about.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df for each source and its polarity score total deviations from the topic means</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas.api.types <span class="im">import</span> CategoricalDtype</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>polarity_devs <span class="op">=</span> pd.DataFrame({<span class="st">'source'</span>:scores_df[<span class="st">'source'</span>].value_counts().index, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                              <span class="st">'polarity_dev'</span>:scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> <span class="st">'Chinese Surveillance Balloon'</span>]<span class="op">\</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                                [<span class="st">'average_source_polarity_deviation_from_topic_mean'</span>]})</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create a column denoting if deviation is above or below mean to color graph by later</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>polarity_devs[<span class="st">'sign'</span>] <span class="op">=</span> np.where(polarity_devs[<span class="st">'polarity_dev'</span>] <span class="op">&gt;</span> <span class="dv">0</span>, <span class="st">'pos'</span>, <span class="st">'neg'</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>polarity_devs <span class="op">=</span> polarity_devs.reindex(polarity_devs[<span class="st">'polarity_dev'</span>].<span class="bu">abs</span>().sort_values(ascending<span class="op">=</span><span class="va">False</span>).index)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>polarity_devs.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>source_order <span class="op">=</span> CategoricalDtype(</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"New York Times"</span>, <span class="st">"Wall Street Journal"</span>, <span class="st">"ABC News"</span>, <span class="st">"NBC News"</span>, </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>     <span class="st">"CNN"</span>, <span class="st">"New York Post"</span>, <span class="st">"Washington Post"</span>, <span class="st">"Fox News"</span>, <span class="st">"BBC"</span>], </span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    ordered<span class="op">=</span><span class="va">False</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>polarity_devs[<span class="st">'source'</span>] <span class="op">=</span> polarity_devs[<span class="st">'source'</span>].astype(source_order)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>polarity_devs</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># create bar chart representing how much, on average, a source's article</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># deviates away from that topic's mean polarity score</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># intended to show if a source tends to be more negative/positive than average, </span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co"># even accounting for the nature of the topic</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {<span class="st">"pos"</span>:<span class="st">'#ffbf00'</span>, <span class="st">"neg"</span>:<span class="st">'#b7141c'</span>}  </span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>ggplot(polarity_devs, aes(x<span class="op">=</span><span class="st">"source"</span>, y<span class="op">=</span><span class="st">"polarity_dev"</span>, fill<span class="op">=</span><span class="st">"sign"</span>))</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_col(stat<span class="op">=</span><span class="st">"identity"</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">"Source"</span>, y<span class="op">=</span><span class="st">'Average Article Polarity Score Deviation'</span>,</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">'Average Source Polarity Score Deviations from Mean'</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> theme(figure_size <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">4</span>), axis_text_x<span class="op">=</span>element_text(size<span class="op">=</span><span class="dv">7</span>, face<span class="op">=</span><span class="st">'bold'</span>), </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> element_text(size<span class="op">=</span><span class="dv">13</span>),legend_position <span class="op">=</span> <span class="st">"none"</span>)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> scale_fill_manual(values <span class="op">=</span> colors)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>&lt;Figure Size: (1000 x 400)&gt;</code></pre>
</div>
</div>
<p>The visualization above tells us that The New York Times articles tend to be the most negative on average. They have the largest deviations from the topic’s average polarity scores, which may indicate stronger wording. Each individual may infer what they wish with these results, but definitive conclusions should not be drawn from them, which will be explained a bit later. ABC News tends to write their articles more positive than average for the topics’ mean polarity scores. BBC’s articles are the closest to average polarity scores for each topic. The polarity scores may be viewed individually for the set of articles of any topic. Below, we see the polarity scores for the articles about the ruling against affirmative action. The New York Times was the only source with a negative polarity score for this topic, which could already be suggestive of its political views.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create bar chart of polarity scores of articles about a specific topic</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>ggplot(scores_df[scores_df[<span class="st">'topic'</span>]<span class="op">==</span><span class="st">"Supreme Court Ruling on Affirmative Action"</span>],</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>       aes(x<span class="op">=</span><span class="st">"reorder(source, -polarity_score)"</span>, y<span class="op">=</span><span class="st">"polarity_score"</span>, fill<span class="op">=</span><span class="st">"polarity_sign"</span>))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_col(stat<span class="op">=</span><span class="st">"identity"</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">'Source'</span>, y<span class="op">=</span><span class="st">'Polarity Score'</span>, title<span class="op">=</span><span class="st">'Polarity Scores for Ban on Affirmative Action Articles'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> theme(figure_size <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">4</span>), axis_text_x<span class="op">=</span>element_text(size<span class="op">=</span><span class="fl">8.5</span>, face<span class="op">=</span><span class="st">'bold'</span>), </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> element_text(size<span class="op">=</span><span class="dv">13</span>), legend_position <span class="op">=</span> <span class="st">"none"</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> scale_fill_manual(values <span class="op">=</span> colors)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>&lt;Figure Size: (1000 x 400)&gt;</code></pre>
</div>
</div>
<p>Now let’s look at a comparison of each source’s average subjectivity scores. These do not need to be broken up by topic, since the scale for the score is [0,1].</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df with average subjectivity scores per source</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>source_avg_subj <span class="op">=</span> pd.DataFrame({<span class="st">'source'</span>:scores_df[<span class="st">'source'</span>].value_counts().index, </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                              <span class="st">'avg_subj'</span>:scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> <span class="st">'Chinese Surveillance Balloon'</span>]<span class="op">\</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                                [<span class="st">'average_subjectivity_for_source'</span>]})</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>source_avg_subj.sort_values(by<span class="op">=</span><span class="st">'avg_subj'</span>, ascending<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>source_avg_subj.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># order in ascending order of subjectivity</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>source_order <span class="op">=</span> CategoricalDtype(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Fox News"</span>, <span class="st">"Wall Street Journal"</span>, <span class="st">"CNN"</span>, </span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>     <span class="st">"Washington Post"</span>, <span class="st">"BBC"</span>, <span class="st">"New York Post"</span>, <span class="st">"ABC News"</span>, <span class="st">"NBC News"</span>, <span class="st">"New York Times"</span>], </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    ordered<span class="op">=</span><span class="va">False</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>source_avg_subj[<span class="st">'source'</span>] <span class="op">=</span> source_avg_subj[<span class="st">'source'</span>].astype(source_order)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>source_avg_subj</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co"># bar chart showing avg subjectivity per source</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>ggplot(source_avg_subj, aes(x<span class="op">=</span><span class="st">"source"</span>, y<span class="op">=</span><span class="st">"avg_subj"</span>))</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_col(stat<span class="op">=</span><span class="st">"identity"</span>, fill<span class="op">=</span><span class="st">"#ffa140"</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">'Source'</span>, y<span class="op">=</span><span class="st">'Average Subjectivity Score'</span>, title<span class="op">=</span><span class="st">'Average Subjectivity Score by Source'</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> theme(figure_size <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">4</span>), axis_text_x<span class="op">=</span>element_text(size<span class="op">=</span><span class="fl">8.5</span>, face<span class="op">=</span><span class="st">'bold'</span>), </span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> element_text(size<span class="op">=</span><span class="dv">13</span>), legend_position <span class="op">=</span> <span class="st">"none"</span>)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>&lt;Figure Size: (1000 x 400)&gt;</code></pre>
</div>
</div>
<p>The bar chart above shows that Fox News has the lowest subjectivity scores on average, while the New York Times has the highest subjectivity score on average. Surprised? Do these results align with your opinions or what you commonly hear about these sources? Regardless, it is important to remember what these scores mean exactly and what we are able to reasonably infer from these results. The package that computes these scores has limited documentation, so the exact formulas are not known for the computation of a polarity or subjectivity score of a text. Each individual word has a scoring of its own and is marked as positive or negative, and the overall polarity score is some sort of aggregation of these scores. This does limit us in the interpretation of the scores. Moreover, this is only a small sample of articles collected from each source and their insights should not represent a source as a whole. It is entirely possible that a larger sample of articles from a wider variety of topics could yield different results.</p>
<p>To further analyze the polarity scores, we may want to know which ones are outliers. So, for each topic, the interquartile range formula is used to find any articles with an outlier score. The articles and their polarity scores that were identified as outliers are shown in the table below.</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of tuples that contain articles that have an outlier polarity score,</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># compared to other articles about the same topic</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>articles <span class="op">=</span>[]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># check for outliers topic by topic</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> scores_df[<span class="st">'topic'</span>].value_counts().index:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    topic_df <span class="op">=</span> scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> topic]</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    average_polarity <span class="op">=</span> topic_df[<span class="st">'average_polarity_for_topic'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use IQR formula to detect outliers</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    Q1 <span class="op">=</span> topic_df[<span class="st">'polarity_score'</span>].describe()[<span class="dv">4</span>]</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    Q3 <span class="op">=</span> topic_df[<span class="st">'polarity_score'</span>].describe()[<span class="dv">6</span>]</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> Q3<span class="op">-</span>Q1</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    lower <span class="op">=</span> Q1<span class="op">-</span>(<span class="fl">1.5</span><span class="op">*</span>IQR)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    upper <span class="op">=</span> Q3<span class="op">+</span>(<span class="fl">1.5</span><span class="op">*</span>IQR)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the line below would print the lower and upper bounds for non-outliers</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   for each topic</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(topic, "lower bound:", round(lower, 2), "upper bound:", round(upper,2))</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check if an article has an outlier polarity score (beyond lower/upper limits)</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> topic_df.itertuples():</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> row.polarity_score <span class="op">&lt;</span> lower <span class="kw">or</span> row.polarity_score <span class="op">&gt;</span> upper:</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            articles.append((row.source, row.topic, row.polarity_score, average_polarity))</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="co"># create lists about outlier article information to create dataframe for them</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>outlier_source_list <span class="op">=</span> []</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>outlier_topic_list <span class="op">=</span> []</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>outlier_polarity_list <span class="op">=</span> []</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>average_polarity_list <span class="op">=</span> []</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> articles:</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>  outlier_source_list.append(article[<span class="dv">0</span>])</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>  outlier_topic_list.append(article[<span class="dv">1</span>])</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>  outlier_polarity_list.append(article[<span class="dv">2</span>])</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>  average_polarity_list.append(article[<span class="dv">3</span>])</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a><span class="co"># display in dataframe the articles with an outlier polarity score</span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>outlier_df <span class="op">=</span> pd.DataFrame({<span class="st">'Source'</span>:outlier_source_list,</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>                           <span class="st">'Topic'</span>:outlier_topic_list,</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>                           <span class="st">'Polarity Score'</span>:outlier_polarity_list,</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>                           <span class="st">'Average Polarity for Topic'</span>:average_polarity_list})</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>display(outlier_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Source</th>
<th data-quarto-table-cell-role="th">Topic</th>
<th data-quarto-table-cell-role="th">Polarity Score</th>
<th data-quarto-table-cell-role="th">Average Polarity for Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>BBC</td>
<td>Supreme Court Ruling on Affirmative Action</td>
<td>0.16</td>
<td>0.11</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>New York Times</td>
<td>Supreme Court Ruling on Affirmative Action</td>
<td>-0.02</td>
<td>0.11</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Wall Street Journal</td>
<td>Supreme Court Ruling on Affirmative Action</td>
<td>0.07</td>
<td>0.11</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>New York Post</td>
<td>Chinese Surveillance Balloon</td>
<td>-0.01</td>
<td>0.04</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Wall Street Journal</td>
<td>Chinese Surveillance Balloon</td>
<td>0.02</td>
<td>0.04</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>Wall Street Journal</td>
<td>George Santos' Expulsion from Congress</td>
<td>-0.04</td>
<td>0.02</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>From looking at outlier polarity scores, the articles about the Supreme Court ruling against affirmative action have several outlier scores, both positive and negative. This could be telling of the major difference in opinions regarding this topic. As for sources, The Wall Street Journal had the most amount of articles with an outlier polarity score.</p>
</section>
<section id="predictive-modeling" class="level1">
<h1>Predictive Modeling</h1>
<section id="naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes">Naive Bayes</h2>
<p>Now, for the main part of this project, a few different classifiers will be used to attempt to predict the source of a given article. If this can be done with high accuracy, it will show that the wording alone of the studied sources is distinct enough to be an identifying factor. First, Naive Bayes classifiers will be trained and tested on the articles included. Six topics will be used for training, and two for testing. As many classifiers will be created as needed (28) in order to use every possible split of topics for the training and testing sets. In addition, this process will be repeated for each of the three dataframes created (using binary values, frequencies, and TF-IDF scores of tokens) for a total of 84 Naive Bayes classifiers. This means that the models will be trained on the tokens (words) that appear in each model and their values (one of the three possible mentioned). The average accuracy scores of Naive Bayes models trained on any of the three value types are displayed in the table below. <a href="https://www.datacamp.com/tutorial/naive-bayes-scikit-learn">Data Camp</a> was used as a source to code the Naive Bayes model, including fitting the model and making predictions (see <span class="citation" data-cites="nb_confusion"><span>“Naive Bayes Classification Tutorial Using Scikit-Learn”</span> (<a href="#ref-nb_confusion" role="doc-biblioref">2023</a>)</span>).</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import libraries</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics  </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a dictionary that will show the best accuracy for each kind of classifier used</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>best_accuracies_by_cf <span class="op">=</span> {}</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># define list of topics to later choose training/testing sets with</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>topic_list <span class="op">=</span> topic_choices  <span class="op">=</span> [</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize lists that will later be used to create a df showing the</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># average model accuracy for each type of value the models were trained on</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>average_accuracies <span class="op">=</span> []</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>value_types <span class="op">=</span> []</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># function to apply Naive Bayes model using different holdout article topics each time</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> naive_bayes_cf(df, value_type):</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Takes in a dataframe of tokens and the desired values </span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="co">    to train the model on (i.e. tfidf scores, frequencies)</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="co">    and a string representing what the values are.</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="co">    Trains, tests, and outputs performance metrics of a series</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co">    of Multinomial Naive Bayes classifiers, one for each</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="co">    possible holdout set of two article topics.'''</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize lists where each model's accuracy/f1 score will be appended</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to later be able to determine the min/max of each</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>    accuracy_list <span class="op">=</span> []</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    f1_score_list <span class="op">=</span> []</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loops to create a model with each combination of a testing set</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># consisting of two topics</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(topic_list)):</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="bu">len</span>(topic_list)):</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below to print the holdout topics for the model</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("Holdout article topics: ")</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(topic_list[i])</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(topic_list[j])</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># split into training/testing df's for predictor and target variables</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>            X_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>            X_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>            y_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>            y_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># transform the training/testing df's to arrays</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>            X_train <span class="op">=</span> X_train_df.to_numpy()</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>            X_test <span class="op">=</span> X_test_df.to_numpy()</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> y_train_df.to_numpy()</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>            y_test <span class="op">=</span> y_test_df.to_numpy()</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create classifier object and train it on our training data</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>            nb_model <span class="op">=</span> MultinomialNB()</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>            nb_model.fit(X_train, y_train)</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make predictions on our testing set, output as an array</span></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>            predictions_array <span class="op">=</span> nb_model.predict(X_test)</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>            <span class="co"># turn the array with predicted sources into a list</span></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>            prediction_list <span class="op">=</span> []</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> pred <span class="kw">in</span> predictions_array:</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>                prediction_list.append(pred)</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a list of the true sources</span></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>            actuals_list <span class="op">=</span> []</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> source <span class="kw">in</span> y_test:</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>                actuals_list.append(source)</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a dataframe from the two lists created above to show the actual vs predicted sources</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>            preds_actuals_df <span class="op">=</span> pd.DataFrame({<span class="st">'Actual'</span>:actuals_list, <span class="st">'Predicted'</span>:prediction_list})</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment line below if want to see the dataframe of actual vs predicted sources for each model</span></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>            <span class="co"># display(preds_actuals_df)</span></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate performance metrics</span></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> <span class="bu">round</span>(accuracy_score(predictions_array, y_test), <span class="dv">2</span>)</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>            f1 <span class="op">=</span> <span class="bu">round</span>(f1_score(predictions_array, y_test, average<span class="op">=</span><span class="st">"weighted"</span>), <span class="dv">2</span>)</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the model's accuracy/f1 score to the list of accuracies/f1 scores</span></span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>            accuracy_list.append(accuracy)</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>            f1_score_list.append(f1)</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if this model's accuracy is better than the current best accuracy</span></span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for Naive Bayes, and if it is, then set its accuracy to be the</span></span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a>            <span class="co"># value for Naive Bayes in the dictionary initialized earlier</span></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> accuracy <span class="op">&gt;</span> best_accuracies_by_cf[<span class="st">'Naive Bayes'</span>]:</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a>                    best_accuracies_by_cf[<span class="st">'Naive Bayes'</span>] <span class="op">=</span> accuracy</span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span>:</span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>                best_accuracies_by_cf[<span class="st">'Naive Bayes'</span>] <span class="op">=</span> accuracy</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below to display results of every model</span></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("Accuracy of Multinomial Naive Bayes Model:", accuracy)</span></span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("F1 score of Multinomial Naive Bayes Model:", f1)</span></span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("\n\n")</span></span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize a cumulative sum of accuracies for models trained</span></span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a>    <span class="co"># on the dataframe/value types defined in the function</span></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>    cum_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add each model's accuracy to the cumulative sum</span></span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> acc <span class="kw">in</span> accuracy_list:</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a>        cum_acc <span class="op">+=</span> acc</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the average accuracy for a model trained on this</span></span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dataframe/value type and add it to the list of average accuracies</span></span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a>    average_accuracies.append(<span class="bu">round</span>(cum_acc<span class="op">/</span><span class="bu">len</span>(accuracy_list), <span class="dv">2</span>))</span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add the value type used to train these models to the list of value</span></span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a>    <span class="co"># types, which will be used to create a df of average accuracies by value type</span></span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a>    value_types.append(value_type)</span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uncomment lines below if interested in displaying min/max</span></span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   accuracy scores for the models trained within the function</span></span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"Minimum Accuracy Score of any Multinomial Naive Bayes Model using {value_type}:",</span></span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    min(accuracy_list))</span></span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"Maximum Accuracy Score of any Multinomial Naive Bayes Model using {value_type}:",</span></span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    max(accuracy_list))</span></span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"Maximum F1 Score of any Multinomial Naive Bayes Model using {value_type}:",</span></span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    max(f1_score_list))</span></span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of all the dataframes/value types we want to train our classifiers on</span></span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>dfs <span class="op">=</span> [(tfidf_df, <span class="st">"TFIDF Score"</span>), (binary_df, <span class="st">"Binary Values"</span>), (freq_df, <span class="st">"Frequencies"</span>)]</span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the list above to create many models</span></span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df <span class="kw">in</span> dfs:</span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a>    naive_bayes_cf(df[<span class="dv">0</span>], df[<span class="dv">1</span>])</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe showing each value type Naive Bayes models were trained on and the average accuracies for each</span></span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a>nb_results_df <span class="op">=</span> pd.DataFrame({<span class="st">'Values Trained On'</span>:value_types, <span class="st">'Average Accuracy'</span>:average_accuracies})</span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a>display(nb_results_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Values Trained On</th>
<th data-quarto-table-cell-role="th">Average Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>TFIDF Score</td>
<td>0.29</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Binary Values</td>
<td>0.23</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Frequencies</td>
<td>0.22</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>For the series of Naive Bayes models created, the maximum accuracy score reached for any model is 44%. This was for a model trained on TF-IDF scores. The average accuracy scores for Naive Bayes models trained on the different value types of tokens range between 22-29%. These accuracy scores do not seem very high. Even the best of these models’ predictions are wrong more often than they are right. However, since there are nine possible predictions that could be made (nine different sources), a random guess would expect to result in a 11% accuracy rate on average. This happens to also be the minimum accuracy score of any of these models. Therefore, all of these classifiers are still just as good or better than guessing which source an article comes from, simply given its text.</p>
</section>
<section id="random-forest" class="level2">
<h2 class="anchored" data-anchor-id="random-forest">Random Forest</h2>
<p>The same will be repeated with Random Forest models. In addition to using various holdout topic sets and dataframes to train and test these models on, a couple parameters will also be tested to see which ones result in the best performing model. Again, a table below is displayed with the average accuracy scores of models trained on a particular dataframe (value types) and set of parameters.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import library</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a dictionary that will hold the parameters that lead to best</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracies of a Random Forest Model</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>best_rf_params <span class="op">=</span> {}</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize lists that will be used to create a dataframe showing the</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># average accuracies of Random Forest models trained on a particular</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># dataframe and with a particular set of parameters</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>average_accuracies <span class="op">=</span> []</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>value_types <span class="op">=</span> []</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>n_estimators <span class="op">=</span> []</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>criterions <span class="op">=</span> []</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>best_accuracies_by_cf[<span class="st">'Random Forest'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co"># create a function to apply a Random Forest model using different</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co"># holdout sets and given parameters and value types to train on</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_forest_cf(df, value_type, n, crit):</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Takes in a dataframe of tokens and the desired values </span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co">    to train the model on (i.e. tfidf scores, frequencies)</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co">    and a string representing what the values are. </span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Also takes in specifications for n_estimators and criterion</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters of the Random Forest classifier. Trains, tests,</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co">    and outputs performance metrics of a series of RF classifiers, </span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co">    one for each possible holdout set of two article topics.'''</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initalize lists for accuracy/f1 scores to later be able to determine the min/max of each </span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    accuracy_list <span class="op">=</span> []</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    f1_score_list <span class="op">=</span> []</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loops to create a model with each combination of a testing set</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># consisting of two topics </span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(topic_list)):</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="bu">len</span>(topic_list)):</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below to print the holdout topics</span></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>            <span class="co">#   and parameters used for the model</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("Holdout article topics: ")</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(topic_list[i])</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(topic_list[j])</span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("n_estimators and criterion:")</span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(n, crit)</span></span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># split into training/testing df's for predictor and target variables</span></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>            X_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>            X_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>            y_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>            y_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># transform the training/testing df's to arrays</span></span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>            X_train <span class="op">=</span> X_train_df.to_numpy()</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>            X_test <span class="op">=</span> X_test_df.to_numpy()</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> y_train_df.to_numpy()</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>            y_test <span class="op">=</span> y_test_df.to_numpy()</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a classifier object and train it on the training data</span></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>            rf_model <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> n, criterion<span class="op">=</span>crit, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>            rf_model.fit(X_train, y_train)</span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make predictions on our testing set, output as an array</span></span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>            predictions_array <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>            <span class="co"># turn the array with predicted sources into a list</span></span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a>            prediction_list <span class="op">=</span> []</span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> pred <span class="kw">in</span> predictions_array:</span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>                prediction_list.append(pred)</span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a list of the true sources</span></span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a>            actuals_list <span class="op">=</span> []</span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> source <span class="kw">in</span> y_test:</span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true" tabindex="-1"></a>                actuals_list.append(source)</span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a dataframe from the two lists above to show the actual</span></span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true" tabindex="-1"></a>            <span class="co"># vs predicted sources for this model</span></span>
<span id="cb21-82"><a href="#cb21-82" aria-hidden="true" tabindex="-1"></a>            preds_actuals_df <span class="op">=</span> pd.DataFrame({<span class="st">'Actual'</span>:actuals_list, <span class="st">'Predicted'</span>:prediction_list})</span>
<span id="cb21-83"><a href="#cb21-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-84"><a href="#cb21-84" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment line below if want to see the dataframe of actual</span></span>
<span id="cb21-85"><a href="#cb21-85" aria-hidden="true" tabindex="-1"></a>            <span class="co">#   vs predicted sources for each model</span></span>
<span id="cb21-86"><a href="#cb21-86" aria-hidden="true" tabindex="-1"></a>            <span class="co"># display(preds_actuals_df)</span></span>
<span id="cb21-87"><a href="#cb21-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-88"><a href="#cb21-88" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate performance metrics</span></span>
<span id="cb21-89"><a href="#cb21-89" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> <span class="bu">round</span>(accuracy_score(predictions_array, y_test), <span class="dv">2</span>)</span>
<span id="cb21-90"><a href="#cb21-90" aria-hidden="true" tabindex="-1"></a>            f1 <span class="op">=</span> <span class="bu">round</span>(f1_score(predictions_array, y_test, average<span class="op">=</span><span class="st">"weighted"</span>), <span class="dv">2</span>)</span>
<span id="cb21-91"><a href="#cb21-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-92"><a href="#cb21-92" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the accuracy/f1 scores for this model to the lists of accuracy/f1 scores </span></span>
<span id="cb21-93"><a href="#cb21-93" aria-hidden="true" tabindex="-1"></a>            accuracy_list.append(accuracy)</span>
<span id="cb21-94"><a href="#cb21-94" aria-hidden="true" tabindex="-1"></a>            f1_score_list.append(f1)</span>
<span id="cb21-95"><a href="#cb21-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-96"><a href="#cb21-96" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if this model's accuracy is better than the current best </span></span>
<span id="cb21-97"><a href="#cb21-97" aria-hidden="true" tabindex="-1"></a>            <span class="co"># accuracy for any Random Forest model, and if it is, then set its </span></span>
<span id="cb21-98"><a href="#cb21-98" aria-hidden="true" tabindex="-1"></a>            <span class="co"># accuracy to be the value for Random Forest in the dictionary we </span></span>
<span id="cb21-99"><a href="#cb21-99" aria-hidden="true" tabindex="-1"></a>            <span class="co"># initialized earlier and update the best parameters dictionary</span></span>
<span id="cb21-100"><a href="#cb21-100" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb21-101"><a href="#cb21-101" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> accuracy <span class="op">&gt;</span> best_accuracies_by_cf[<span class="st">'Random Forest'</span>]:</span>
<span id="cb21-102"><a href="#cb21-102" aria-hidden="true" tabindex="-1"></a>                    best_accuracies_by_cf[<span class="st">'Random Forest'</span>] <span class="op">=</span> accuracy</span>
<span id="cb21-103"><a href="#cb21-103" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'n_estimators'</span>] <span class="op">=</span> n</span>
<span id="cb21-104"><a href="#cb21-104" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'criterion'</span>] <span class="op">=</span> crit </span>
<span id="cb21-105"><a href="#cb21-105" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'value'</span>] <span class="op">=</span> value_type</span>
<span id="cb21-106"><a href="#cb21-106" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'holdout_topics'</span>] <span class="op">=</span> (topic_list[i], topic_list[j])</span>
<span id="cb21-107"><a href="#cb21-107" aria-hidden="true" tabindex="-1"></a>                    best_model_pred_vs_actuals <span class="op">=</span> preds_actuals_df.copy()</span>
<span id="cb21-108"><a href="#cb21-108" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span>:</span>
<span id="cb21-109"><a href="#cb21-109" aria-hidden="true" tabindex="-1"></a>                best_accuracies_by_cf[<span class="st">'Random Forest'</span>] <span class="op">=</span> accuracy</span>
<span id="cb21-110"><a href="#cb21-110" aria-hidden="true" tabindex="-1"></a>                best_rf_params[<span class="st">'n_estimators'</span>] <span class="op">=</span> n</span>
<span id="cb21-111"><a href="#cb21-111" aria-hidden="true" tabindex="-1"></a>                best_rf_params[<span class="st">'criterion'</span>] <span class="op">=</span> crit </span>
<span id="cb21-112"><a href="#cb21-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-113"><a href="#cb21-113" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below if want to see the performance metrics</span></span>
<span id="cb21-114"><a href="#cb21-114" aria-hidden="true" tabindex="-1"></a>            <span class="co">#   for the model</span></span>
<span id="cb21-115"><a href="#cb21-115" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("Accuracy of Random Forest Model:", accuracy)</span></span>
<span id="cb21-116"><a href="#cb21-116" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("F1 score of Random Forest Model:", f1)</span></span>
<span id="cb21-117"><a href="#cb21-117" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("\n\n")</span></span>
<span id="cb21-118"><a href="#cb21-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-119"><a href="#cb21-119" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize a cumulative sum of accuracies for models trained on the</span></span>
<span id="cb21-120"><a href="#cb21-120" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dataframe/value types defined in the function</span></span>
<span id="cb21-121"><a href="#cb21-121" aria-hidden="true" tabindex="-1"></a>    cum_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-122"><a href="#cb21-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-123"><a href="#cb21-123" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add each model's accuracy to the cumulative sum</span></span>
<span id="cb21-124"><a href="#cb21-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> acc <span class="kw">in</span> accuracy_list:</span>
<span id="cb21-125"><a href="#cb21-125" aria-hidden="true" tabindex="-1"></a>        cum_acc <span class="op">+=</span> acc</span>
<span id="cb21-126"><a href="#cb21-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-127"><a href="#cb21-127" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the average accuracy of a model trained on this dataframe/value</span></span>
<span id="cb21-128"><a href="#cb21-128" aria-hidden="true" tabindex="-1"></a>    <span class="co"># type and with these parameters and add it to the list of average accuracies</span></span>
<span id="cb21-129"><a href="#cb21-129" aria-hidden="true" tabindex="-1"></a>    average_accuracies.append(<span class="bu">round</span>(cum_acc<span class="op">/</span><span class="bu">len</span>(accuracy_list), <span class="dv">2</span>))</span>
<span id="cb21-130"><a href="#cb21-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-131"><a href="#cb21-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add the value type used for training this model and its parameters to</span></span>
<span id="cb21-132"><a href="#cb21-132" aria-hidden="true" tabindex="-1"></a>    <span class="co"># their lists, which will be used to create a dataframe of average</span></span>
<span id="cb21-133"><a href="#cb21-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># accuracies for each of these options</span></span>
<span id="cb21-134"><a href="#cb21-134" aria-hidden="true" tabindex="-1"></a>    value_types.append(value_type)</span>
<span id="cb21-135"><a href="#cb21-135" aria-hidden="true" tabindex="-1"></a>    n_estimators.append(n)</span>
<span id="cb21-136"><a href="#cb21-136" aria-hidden="true" tabindex="-1"></a>    criterions.append(crit)</span>
<span id="cb21-137"><a href="#cb21-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-138"><a href="#cb21-138" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uncomment lines below if interested in displaying min/max accuracies of</span></span>
<span id="cb21-139"><a href="#cb21-139" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   models trained within this function.</span></span>
<span id="cb21-140"><a href="#cb21-140" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Minimum Accuracy Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",</span></span>
<span id="cb21-141"><a href="#cb21-141" aria-hidden="true" tabindex="-1"></a>    <span class="co">#      min(accuracy_list))</span></span>
<span id="cb21-142"><a href="#cb21-142" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Maximum Accuracy Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:", max(accuracy_list))</span></span>
<span id="cb21-143"><a href="#cb21-143" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Maximum F1 Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",</span></span>
<span id="cb21-144"><a href="#cb21-144" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       max(f1_score_list))</span></span>
<span id="cb21-145"><a href="#cb21-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("\n")</span></span>
<span id="cb21-146"><a href="#cb21-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-147"><a href="#cb21-147" aria-hidden="true" tabindex="-1"></a><span class="co"># specify all parameters that we want to test models using</span></span>
<span id="cb21-148"><a href="#cb21-148" aria-hidden="true" tabindex="-1"></a>crits <span class="op">=</span> [<span class="st">'gini'</span>, <span class="st">'entropy'</span>]</span>
<span id="cb21-149"><a href="#cb21-149" aria-hidden="true" tabindex="-1"></a>n_ests <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">200</span>]</span>
<span id="cb21-150"><a href="#cb21-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-151"><a href="#cb21-151" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the desired dataframes and parameters to create models with and</span></span>
<span id="cb21-152"><a href="#cb21-152" aria-hidden="true" tabindex="-1"></a><span class="co"># create multiple models with various holdout sets using the function defined above</span></span>
<span id="cb21-153"><a href="#cb21-153" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df <span class="kw">in</span> dfs:</span>
<span id="cb21-154"><a href="#cb21-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> crit <span class="kw">in</span> crits:</span>
<span id="cb21-155"><a href="#cb21-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> n_ests:</span>
<span id="cb21-156"><a href="#cb21-156" aria-hidden="true" tabindex="-1"></a>            random_forest_cf(df[<span class="dv">0</span>], df[<span class="dv">1</span>], n, crit)</span>
<span id="cb21-157"><a href="#cb21-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-158"><a href="#cb21-158" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe showing each value type that Random Forest models were trained on,</span></span>
<span id="cb21-159"><a href="#cb21-159" aria-hidden="true" tabindex="-1"></a><span class="co"># each parameter used, and the average accuracies for each of these combinations</span></span>
<span id="cb21-160"><a href="#cb21-160" aria-hidden="true" tabindex="-1"></a>rf_results_df <span class="op">=</span> pd.DataFrame({<span class="st">'Values Trained On'</span>:value_types, <span class="st">'n_estimators'</span>:n_estimators, <span class="st">'criterion'</span>:criterions, <span class="st">'Average Accuracy'</span>: average_accuracies}).sort_values(<span class="st">'Average Accuracy'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-161"><a href="#cb21-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-162"><a href="#cb21-162" aria-hidden="true" tabindex="-1"></a>rf_results_df <span class="op">=</span> rf_results_df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-163"><a href="#cb21-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-164"><a href="#cb21-164" aria-hidden="true" tabindex="-1"></a>display(rf_results_df)</span>
<span id="cb21-165"><a href="#cb21-165" aria-hidden="true" tabindex="-1"></a><span class="co"># if interested in displaying best parameters and holdout set for random forest models,</span></span>
<span id="cb21-166"><a href="#cb21-166" aria-hidden="true" tabindex="-1"></a><span class="co"># uncomment line below. there could have also been other parameters</span></span>
<span id="cb21-167"><a href="#cb21-167" aria-hidden="true" tabindex="-1"></a><span class="co"># and holdout sets that led to the same accuracy score.</span></span>
<span id="cb21-168"><a href="#cb21-168" aria-hidden="true" tabindex="-1"></a><span class="co"># best_rf_params</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Values Trained On</th>
<th data-quarto-table-cell-role="th">n_estimators</th>
<th data-quarto-table-cell-role="th">criterion</th>
<th data-quarto-table-cell-role="th">Average Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>TFIDF Score</td>
<td>200</td>
<td>gini</td>
<td>0.42</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Frequencies</td>
<td>200</td>
<td>gini</td>
<td>0.41</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Binary Values</td>
<td>200</td>
<td>gini</td>
<td>0.41</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>TFIDF Score</td>
<td>100</td>
<td>gini</td>
<td>0.39</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Frequencies</td>
<td>100</td>
<td>gini</td>
<td>0.38</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>Binary Values</td>
<td>100</td>
<td>gini</td>
<td>0.36</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>TFIDF Score</td>
<td>200</td>
<td>entropy</td>
<td>0.34</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>Frequencies</td>
<td>200</td>
<td>entropy</td>
<td>0.33</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>Binary Values</td>
<td>200</td>
<td>entropy</td>
<td>0.31</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>TFIDF Score</td>
<td>100</td>
<td>entropy</td>
<td>0.30</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>Frequencies</td>
<td>100</td>
<td>entropy</td>
<td>0.28</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>Binary Values</td>
<td>100</td>
<td>entropy</td>
<td>0.28</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>TFIDF Score</td>
<td>10</td>
<td>gini</td>
<td>0.26</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>Frequencies</td>
<td>10</td>
<td>gini</td>
<td>0.24</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>Binary Values</td>
<td>10</td>
<td>gini</td>
<td>0.21</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>Frequencies</td>
<td>10</td>
<td>entropy</td>
<td>0.17</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>TFIDF Score</td>
<td>10</td>
<td>entropy</td>
<td>0.17</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>Binary Values</td>
<td>10</td>
<td>entropy</td>
<td>0.17</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The best random forest classifier had an accuracy score of 61%. A few models with various parameters and holdout sets resulted in this accuracy score. The same parameters that have resulted in a model with a 61% accuracy have also resulted in models with an accuracy score as low as 11% (with a different holdout set). A few of the models that performed the worst (with worse parameters) had an accuracy score of 0%. As seen, using even the best parameters in the random forest models, the accuracy can wildly differ (11-61%) depending on which topics were used for training and testing. This is why the average accuracies are also displayed above for each combination of dataframe used and n_estimators and criterion parameters. It is reasonable to assume that this highly varied accuracy score that depends on the training/testing split could be remedied with a much larger sample of articles in the future, which could provide a rich set for both training and testing data. Still, a lot of the models’ results can be considered good for our purposes! Using the right parameters and training/testing sets, we could correctly predict the source of an article about three out of every five times. It is worth pointing out that our goal here wasn’t necessarily to determine with certainty which source a given article text comes from. Instead, the point of looking at the accuracies of these models is to be able to conclude whether the information about the words in an article is useful in determining the source of the article. I think we can say that this has been achieved. The vast majority of the time, the models had an accuracy score that was above the expected accuracy if one were to guess the source of an article (1/9, or 11%). This tells us that the wording between sources differs significantly enough to help us identify the source of an article.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create confusion matrix of model with best parameters</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set the training/testing datasets</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>X_train_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">&amp;</span> </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'Pentagon Documents Leak'</span>)].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>X_test_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">|</span> </span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'Pentagon Documents Leak'</span>)].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>y_train_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">&amp;</span> </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'Pentagon Documents Leak'</span>)][<span class="st">'article_source'</span>]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>y_test_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">|</span> </span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'Pentagon Documents Leak'</span>)][<span class="st">'article_source'</span>]</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># convert training/testing datasets to arrays </span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train_df.to_numpy()</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test_df.to_numpy()</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train_df.to_numpy()</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test_df.to_numpy()</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co"># train the Random Forest model with the best parameters</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> <span class="dv">200</span>, criterion<span class="op">=</span><span class="st">"gini"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co"># make source predictions</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>predictions_array <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># specify the labels to make the confusion matrix</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">"ABC News"</span>, <span class="st">"BBC"</span>, <span class="st">"CNN"</span>, <span class="st">"Fox News"</span>, <span class="st">"NBC News"</span>, <span class="st">"New York Post"</span>,</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>          <span class="st">"The New York Times"</span>, <span class="st">"The Washington Post"</span>, <span class="st">"The Wall Street Journal"</span>]</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="co"># shorten the labels so that the source names can easily fit on the confusion matrix</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>labels_short <span class="op">=</span> [<span class="st">"ABC"</span>, <span class="st">"BBC"</span>, <span class="st">"CNN"</span>, <span class="st">"Fox"</span>, <span class="st">"NBC"</span>, <span class="st">"NYP"</span>,</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>          <span class="st">"NYT"</span>, <span class="st">"WP"</span>, <span class="st">"WSJ"</span>]</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="co"># create the plot of the confusion matrix </span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, predictions_array, labels<span class="op">=</span>labels)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>labels_short)</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>disp.plot()</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> disp.ax_.get_figure() </span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>fig.set_figwidth(<span class="dv">5</span>)</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>fig.set_figheight(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="blog_files/figure-html/cell-17-output-1.png" width="480" height="480"></p>
</div>
</div>
<p>We may also look at the confusion matrix above that was generated from the results of a Random Forest model that uses one of the parameters that led to the highest accuracy score of 61%. <a href="https://www.datacamp.com/tutorial/naive-bayes-scikit-learn">Data Camp</a> was used as a source to code the confusion matrix (see <span class="citation" data-cites="nb_confusion"><span>“Naive Bayes Classification Tutorial Using Scikit-Learn”</span> (<a href="#ref-nb_confusion" role="doc-biblioref">2023</a>)</span>). It shows us which sources the classifier predicted for an article, versus what the correct source actually was. For more than half of the articles used in the holdout set (11/18), the predicted and actual source were the same (as seen on the diagonal of the confusion matrix). We can see, for example, that it twice incorrectly predicted Fox News to be the source, when the true source was New York Post. Neither of the New York Post or New York Times articles were correctly classified as such. Again, these are the predictions for just one of the many Random Forest models created. Unfortunately, it is difficult to infer the reasoning for these predictions, especially with a black box model like Random Forest which does not provide interpretable reasoning behind prediction decisions.</p>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>To summarize what was revealed in this project, here are a few takeaways:</p>
<ul>
<li><p>Fox News uniquely mentions Trump and Biden much more frequently than other news sources, given its article lengths.</p></li>
<li><p>The New York Times articles in the sample have the strongest (negative) tone.</p></li>
<li><p>With a Random Forest model, we were able to correctly predict the source of an article up to 61% of the time, leading us to conclude that the wording between news sources is, in fact, distinct.</p></li>
</ul>
</section>
<section id="future-work" class="level1">
<h1>Future Work</h1>
<ul>
<li>Other kinds of classifiers could also be tested, such as Stochastic Gradient Descent (SGD).</li>
<li>Using a much larger sample of articles could lead to better results, as they would better represent each source.</li>
<li>Create a definition of bias in terms of word usage. Can we classify an article or source as biased?</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Articles used in project
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="abcaffirmative">Dwyer and Hutzler (<a href="#ref-abcaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abcballoon">Raddatz, Martinez, and Yiu (<a href="#ref-abcballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abcbiden">Axelrod (<a href="#ref-abcbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abchamas">Haworth and Chile (<a href="#ref-abchamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abcpentagon">Martinez, Crawford, and Mallin (<a href="#ref-abcpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abcsantos">Peller and Hutzler (<a href="#ref-abcsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abctanks">Gittleson et al. (<a href="#ref-abctanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="abctrump">Katersky, Santucci, and Faulders (<a href="#ref-abctrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbcaffirmative">Jr (<a href="#ref-bbcaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbcballoon">Matza (<a href="#ref-bbcballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbcbiden">Cabral (<a href="#ref-bbcbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbchamas">Knell, Berg, and Gritten (<a href="#ref-bbchamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbcpentagon">Adams (<a href="#ref-bbcpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbcsantos">Halpert and Cabral (<a href="#ref-bbcsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbctanks">Evans (<a href="#ref-bbctanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="bbctrump">Zurcher and Sheerin (<a href="#ref-bbctrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnnaffirmative">Vogue, Cole, and Sneed (<a href="#ref-cnnaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnnballoon">Liebermann et al. (<a href="#ref-cnnballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnnbiden">Edwards-Levy (<a href="#ref-cnnbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnnhamas">Dahman et al. (<a href="#ref-cnnhamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnnpentagon">Cohen, Bertrand, and Atwood (<a href="#ref-cnnpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnnsantos">Foran and Talbot (<a href="#ref-cnnsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnntanks">Liptak et al. (<a href="#ref-cnntanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="cnntrump">Scannell et al. (<a href="#ref-cnntrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxaffirmative">Hagstrom et al. (<a href="#ref-foxaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxballoon">Sabes, Friden, and Griffin (<a href="#ref-foxballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxbiden">Steinhauser (<a href="#ref-foxbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxhamas">Aitken and Pandolfo (<a href="#ref-foxhamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxpentagon">Aitken (<a href="#ref-foxpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxsantos">Elkind (<a href="#ref-foxsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxtanks">McFall (<a href="#ref-foxtanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="foxtrump">Singman and Dhanis (<a href="#ref-foxtrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbcaffirmative">Hurley (<a href="#ref-nbcaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbcballoon">Kube and Lee (<a href="#ref-nbcballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbcbiden">Murray (<a href="#ref-nbcbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbchamas">Sanchez and Arkin (<a href="#ref-nbchamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbcpentagon">Luce et al. (<a href="#ref-nbcpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbcsantos">Wong et al. (<a href="#ref-nbcsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbctanks">Smith (<a href="#ref-nbctanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nbctrump">Dienst, Reiss, and Gregorian (<a href="#ref-nbctrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nytaffirmative">Montague (<a href="#ref-nytaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nytballoon">Robbins (<a href="#ref-nytballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nytbiden">Epstein and Lerer (<a href="#ref-nytbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nythamas">Kershner (<a href="#ref-nythamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nytpentagon">Cooper, Barnes, and Schmitt (<a href="#ref-nytpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nytsantos">Gold and Ashford (<a href="#ref-nytsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nyttanks">Solomon, Baker, and Nagourney (<a href="#ref-nyttanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nyttrump">Protess et al. (<a href="#ref-nyttrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nypaffirmative">Chamberlain (<a href="#ref-nypaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nypballoon">Reilly (<a href="#ref-nypballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nypbiden">Christenson (<a href="#ref-nypbiden" role="doc-biblioref">2023a</a>)</span> <span class="citation" data-cites="nyphamas"><span>“Netanyahu Tells Israel <span>‘We Are at War’</span> After Hamas Launches an Unprecedented Attack, Killing at Least 300 People”</span> (<a href="#ref-nyphamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nyppentagon">Chamberlain and Crane (<a href="#ref-nyppentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nypsantos">Christenson (<a href="#ref-nypsantos" role="doc-biblioref">2023b</a>)</span> <span class="citation" data-cites="nyptanks">Rohac (<a href="#ref-nyptanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="nyptrump">Rosner et al. (<a href="#ref-nyptrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjaffirmative">Bravin (<a href="#ref-wsjaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjballoon">Salama, Youssef, and Gordon (<a href="#ref-wsjballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjbiden">Galsto (<a href="#ref-wsjbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjhamas">Raice and Nissenbaum (<a href="#ref-wsjhamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjpentagon">Youssef (<a href="#ref-wsjpentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjsantos">Ferek and Vielkind (<a href="#ref-wsjsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjtanks">Gordon, Lubold, and Pancevski (<a href="#ref-wsjtanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wsjtrump">Ramey and Palazzolo (<a href="#ref-wsjtrump" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wpaffirmative">Barnes (<a href="#ref-wpaffirmative" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wpballoon">Lamothe and Horton (<a href="#ref-wpballoon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wpbiden">Pager (<a href="#ref-wpbiden" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wphamas">George et al. (<a href="#ref-wphamas" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wppentagon">Harris and Lamothe (<a href="#ref-wppentagon" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wpsantos">Wang and Alfaro (<a href="#ref-wpsantos" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wptanks">DeYoung et al. (<a href="#ref-wptanks" role="doc-biblioref">2023</a>)</span> <span class="citation" data-cites="wptrump">Jacobs et al. (<a href="#ref-wptrump" role="doc-biblioref">2023</a>)</span></p>
</div>
</div>
</div>
<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bbcpentagon" class="csl-entry" role="listitem">
Adams, Paul. 2023. <span>“Ukraine War: Who Leaked Top Secret US Documents - and Why?”</span> <em>BBC</em>.
</div>
<div id="ref-foxpentagon" class="csl-entry" role="listitem">
Aitken, Peter. 2023. <span>“Pentagon Document Leak Has Immediate Impact on National Security: Sucks up ’a Lot of Oxygen’.”</span> <em>Fox News</em>.
</div>
<div id="ref-foxhamas" class="csl-entry" role="listitem">
Aitken, Peter, and Chris Pandolfo. 2023. <span>“Israel’s Military Says Force Is ’at War’ with Hamas as IDF Hits Back at Terror Targets.”</span> <em>Fox News</em>.
</div>
<div id="ref-abcbiden" class="csl-entry" role="listitem">
Axelrod, Tal. 2023. <span>“Biden Keeps Polling Poorly and Other Democrats Keep Winning Anyway. Why?”</span> <em>ABC News</em>.
</div>
<div id="ref-wpaffirmative" class="csl-entry" role="listitem">
Barnes, Robert. 2023. <span>“Supreme Court Rejects Race-Based Affirmative Action in College Admissions.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-wsjaffirmative" class="csl-entry" role="listitem">
Bravin, Jess. 2023. <span>“Supreme Court Strikes down Affirmative Action in College Admissions.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-preprocessing" class="csl-entry" role="listitem">
Brownlee, Jason. 2020. <span>“How to Develop a Deep Learning Bag-of-Words Model for Sentiment Analysis (Text Classification).”</span> <em>Machine Learning Mastery</em>.
</div>
<div id="ref-bbcbiden" class="csl-entry" role="listitem">
Cabral, Sam. 2023. <span>“US Presidential Election 2024: Joe Biden’s Democratic Challengers.”</span> <em>BBC</em>.
</div>
<div id="ref-nypaffirmative" class="csl-entry" role="listitem">
Chamberlain, Samuel. 2023. <span>“Supreme Court Outlaws Affirmative Action in College Admissions in Landmark Decision.”</span> <em>New York Post</em>.
</div>
<div id="ref-nyppentagon" class="csl-entry" role="listitem">
Chamberlain, Samuel, and Emily Crane. 2023. <span>“Law Enforcement Search Home of Jack Teixeira, Mass. Air National Guardsman at Center of <span>‘Digileaks’</span> Scandal.”</span> <em>New York Post</em>.
</div>
<div id="ref-nypbiden" class="csl-entry" role="listitem">
Christenson, Josh. 2023a. <span>“Biden Polls Lower Than Any President in over 40 Years with Two-Thirds of Americans Saying Economy Getting Worse.”</span> <em>New York Post</em>.
</div>
<div id="ref-nypsantos" class="csl-entry" role="listitem">
———. 2023b. <span>“Lying NY Rep. George Santos Becomes 6th Member Expelled from House Ever.”</span> <em>New York Post</em>.
</div>
<div id="ref-cnnpentagon" class="csl-entry" role="listitem">
Cohen, Zachary, Natasha Bertrand, and Kylie Atwood. 2023. <span>“What We Know about the Major Pentagon Intelligence Leak.”</span> <em>CNN</em>.
</div>
<div id="ref-nytpentagon" class="csl-entry" role="listitem">
Cooper, Helene, Julian E. Barnes, and Eric Schmitt. 2023. <span>“When <span>‘Top Secret’</span> Is Not so Secret.”</span> <em>The New York Times</em>.
</div>
<div id="ref-cnnhamas" class="csl-entry" role="listitem">
Dahman, Ibrahim, Hadas Gold, Lauren Iszo, Amir Tal, Abeer Salman, Kareem Khadder, Richard Allen Greene, and Hande Atay Alam. 2023. <span>“Netanyahu Says Israel Is <span>‘at War’</span> After Hamas Launches Surprise Air and Ground Attack from Gaza.”</span> <em>CNN</em>.
</div>
<div id="ref-wptanks" class="csl-entry" role="listitem">
DeYoung, Karen, Loveday Morris, Emily Rauhala, and Dan Lamothe. 2023. <span>“U.s. Will Supply M1 Tanks to Ukraine; Germany Approves Leopards.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-nbctrump" class="csl-entry" role="listitem">
Dienst, Jonathan, Adam Reiss, and Dareh Gregorian. 2023. <span>“Trump Indicted by Manhattan Grand Jury.”</span> <em>NBC News</em>.
</div>
<div id="ref-abcaffirmative" class="csl-entry" role="listitem">
Dwyer, Devin, and Alexandra Hutzler. 2023. <span>“Supreme Court Effectively Ends Affirmative Action at Colleges in Landmark Ruling.”</span> <em>ABC News</em>.
</div>
<div id="ref-cnnbiden" class="csl-entry" role="listitem">
Edwards-Levy, Ariel. 2023. <span>“Biden Gets Low Marks on Israel, Particularly from Young Voters, New Polling Finds.”</span> <em>CNN</em>.
</div>
<div id="ref-foxsantos" class="csl-entry" role="listitem">
Elkind, Elizabeth. 2023. <span>“Embattled GOP Rep George Santos Expelled from House.”</span> <em>Fox News</em>.
</div>
<div id="ref-nytbiden" class="csl-entry" role="listitem">
Epstein, Reid J., and Lisa Lerer. 2023. <span>“Democrats in Key States Worry Biden Could Be a Drag on Their Races.”</span> <em>The New York Times</em>.
</div>
<div id="ref-bbctanks" class="csl-entry" role="listitem">
Evans, Gareth. 2023. <span>“US Joins Germany in Sending Battle Tanks to Ukraine.”</span> <em>BBC</em>.
</div>
<div id="ref-wsjsantos" class="csl-entry" role="listitem">
Ferek, Katy Stech, and Jimmy Vielkind. 2023. <span>“George Santos Expelled from Congress in Historic House Vote.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-cnnsantos" class="csl-entry" role="listitem">
Foran, Clare, and Haley Talbot. 2023. <span>“House Votes to Expel Santos from Congress in Historic Vote.”</span> <em>CNN</em>.
</div>
<div id="ref-wsjbiden" class="csl-entry" role="listitem">
Galsto, William A. 2023. <span>“Amid Dismal Polls, the Optimist’s Case for Biden Falters.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-wphamas" class="csl-entry" role="listitem">
George, Susannah, Sarah Dadouch, Claire Parker, and Shira Rubin. 2023. <span>“Israel Formally Declares War Against Hamas as More Than 1,000 Killed on Both Sides.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-abctanks" class="csl-entry" role="listitem">
Gittleson, Ben, Justin Gomez, Sarah Kolinovsky, Molly Nagle, and Isabella Murray. 2023. <span>“Biden Approves Sending 31 Abrams Tanks to Ukraine.”</span> <em>ABC News</em>.
</div>
<div id="ref-nytsantos" class="csl-entry" role="listitem">
Gold, Michael, and Grace Ashford. 2023. <span>“George Santos Is Kicked Out of Congress in a Historic Vote.”</span> <em>The New York Times</em>.
</div>
<div id="ref-wsjtanks" class="csl-entry" role="listitem">
Gordon, Michael R., Gordon Lubold, and Bojan Pancevski. 2023. <span>“U.s., Germany Approve Sending Tanks to Ukraine.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-foxaffirmative" class="csl-entry" role="listitem">
Hagstrom, Anders, Brianna Herlihy, Bill Mears, Shannon Bream, and Haley Chi-Sing. 2023. <span>“Supreme Court Rejects Affirmative Action in Ruling on Universities Using Race in Admissions Decisions.”</span> <em>Fox News</em>.
</div>
<div id="ref-bbcsantos" class="csl-entry" role="listitem">
Halpert, Madeline, and Sam Cabral. 2023. <span>“George Santos Expelled from Congress in Historic Vote.”</span> <em>BBC</em>.
</div>
<div id="ref-wppentagon" class="csl-entry" role="listitem">
Harris, Shane, and Dan Lamothe. 2023. <span>“Intelligence Leak Exposes u.s. Spying on Adversaries and Allies.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-abchamas" class="csl-entry" role="listitem">
Haworth, Jon, and Patricio Chile. 2023. <span>“Hundreds Dead, Thousands Injured in Israel and Gaza After Hamas Launches Rockets; Israel Declares War.”</span> <em>ABC News</em>.
</div>
<div id="ref-nbcaffirmative" class="csl-entry" role="listitem">
Hurley, Lawrence. 2023. <span>“Supreme Court Strikes down College Affirmative Action Programs.”</span> <em>NBC News</em>.
</div>
<div id="ref-wptrump" class="csl-entry" role="listitem">
Jacobs, Shayna, Josh Dawsey, Devlin Barrett, and Jacqueline Alemany. 2023. <span>“Trump Indicted by n.y. Grand Jury, First Ex-President Charged with Crime.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-bbcaffirmative" class="csl-entry" role="listitem">
Jr, Bernd Debusmann. 2023. <span>“Affirmative Action: US Supreme Court Overturns Race-Based College Admissions.”</span> <em>BBC</em>.
</div>
<div id="ref-abctrump" class="csl-entry" role="listitem">
Katersky, Aaron, John Santucci, and Katherine Faulders. 2023. <span>“Trump Becomes 1st Current or Former President to Be Indicted.”</span> <em>ABC News</em>.
</div>
<div id="ref-nythamas" class="csl-entry" role="listitem">
Kershner, Isabel. 2023. <span>“For Israelis, Scale of Tragedy Starts to Set In.”</span> <em>The New York Times</em>.
</div>
<div id="ref-bbchamas" class="csl-entry" role="listitem">
Knell, Yolande, Raffi Berg, and David Gritten. 2023. <span>“Israel Attack: PM Says Israel at War After 250 Killed in Attack from Gaza.”</span> <em>BBC</em>.
</div>
<div id="ref-nbcballoon" class="csl-entry" role="listitem">
Kube, Courtney, and Carol E. Lee. 2023. <span>“Suspected Chinese Spy Balloon Found over Northern u.s.”</span> <em>NBC News</em>.
</div>
<div id="ref-wpballoon" class="csl-entry" role="listitem">
Lamothe, Dan, and Alex Horton. 2023. <span>“Chinese Spy Balloon Flying over u.s. <span>‘Right Now,’</span> Pentagon Says.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-cnnballoon" class="csl-entry" role="listitem">
Liebermann, Oren, Haley Britzky, Michael Conte, and Nectar Gan. 2023. <span>“Pentagon Tracking Suspected Chinese Spy Balloon over the US.”</span> <em>CNN</em>.
</div>
<div id="ref-cnntanks" class="csl-entry" role="listitem">
Liptak, Kevin, Stephanie Halasz, Sophie Tanno, and Sugam Pokharel. 2023. <span>“Germany and US Announce Plans to Send Tanks to Ukraine in Major Sign of Support for Kyiv.”</span> <em>CNN</em>.
</div>
<div id="ref-nbcpentagon" class="csl-entry" role="listitem">
Luce, Dan De, Kevin Collier, Phil McCausland, and Ken Dilanian. 2023. <span>“Leaked Secret Pentagon Documents Lift the Lid on u.s. Spying on Russia’s War in Ukraine.”</span> <em>NBC News</em>.
</div>
<div id="ref-abcpentagon" class="csl-entry" role="listitem">
Martinez, Luis, Shannon K. Crawford, and Alexander Mallin. 2023. <span>“What You Need to Know about the Leaked US Secret Documents.”</span> <em>ABC News</em>.
</div>
<div id="ref-bbcballoon" class="csl-entry" role="listitem">
Matza, Max. 2023. <span>“Chinese Spy Balloon: US Tracks Suspected Surveillance Device.”</span> <em>BBC</em>.
</div>
<div id="ref-foxtanks" class="csl-entry" role="listitem">
McFall, Caitlin. 2023. <span>“Ukraine-Russia War: Germany Agrees to Send 2 Battalions of Leopard 2 Tanks After Heavy Pressure.”</span> <em>Fox News</em>.
</div>
<div id="ref-nytaffirmative" class="csl-entry" role="listitem">
Montague, Zach. 2023. <span>“Rejection of Affirmative Action Draws Strong Reactions from Right and Left.”</span> <em>The New York Times</em>.
</div>
<div id="ref-nbcbiden" class="csl-entry" role="listitem">
Murray, Mark. 2023. <span>“What Biden’s Rough 2023 in the Polls Means — and Doesn’t Mean — for 2024.”</span> <em>NBC News</em>.
</div>
<div id="ref-nb_confusion" class="csl-entry" role="listitem">
<span>“Naive Bayes Classification Tutorial Using Scikit-Learn.”</span> 2023. <em>Datacamp</em>.
</div>
<div id="ref-nyphamas" class="csl-entry" role="listitem">
<span>“Netanyahu Tells Israel <span>‘We Are at War’</span> After Hamas Launches an Unprecedented Attack, Killing at Least 300 People.”</span> 2023. <em>New York Post</em>.
</div>
<div id="ref-wpbiden" class="csl-entry" role="listitem">
Pager, Tyler. 2023. <span>“Biden Said to Be Increasingly Frustrated by Dismal Poll Numbers.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-abcsantos" class="csl-entry" role="listitem">
Peller, Lauren, and Alexandra Hutzler. 2023. <span>“Republican George Santos Becomes First House Member Expelled in More Than 20 Years.”</span> <em>ABC News</em>.
</div>
<div id="ref-nyttrump" class="csl-entry" role="listitem">
Protess, Ben, Jonah E. Bromwich, William K. Rashbaum, Kate Christobek, Nate Schweber, and Sean Piccoli. 2023. <span>“Trump Indicted in New York.”</span> <em>The New York Times</em>.
</div>
<div id="ref-abcballoon" class="csl-entry" role="listitem">
Raddatz, Martha, Luis Martinez, and Karson Yiu. 2023. <span>“Large Chinese Reconnaissance Balloon Spotted over the US, Officials Say.”</span> <em>ABC News</em>.
</div>
<div id="ref-wsjhamas" class="csl-entry" role="listitem">
Raice, Shayndi, and Dion Nissenbaum. 2023. <span>“Israel Declares War on Hamas After Surprise Assault from Gaza.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-wsjtrump" class="csl-entry" role="listitem">
Ramey, Corinne, and Joe Palazzolo. 2023. <span>“Grand Jury Votes to Indict Donald Trump.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-nypballoon" class="csl-entry" role="listitem">
Reilly, Patrick. 2023. <span>“Chinese Spy Balloon Tracked over Northern US: Pentagon.”</span> <em>New York Post</em>.
</div>
<div id="ref-nytballoon" class="csl-entry" role="listitem">
Robbins, Jim. 2023. <span>“A Giant Balloon Floats into Town, and It’s All Anyone Can Talk About.”</span> <em>The New York Times</em>.
</div>
<div id="ref-nyptanks" class="csl-entry" role="listitem">
Rohac, Dalibor. 2023. <span>“Fuss over Ukraine Tanks Shows the World Has No Substitute for America’s Defense Industry.”</span> <em>New York Post</em>.
</div>
<div id="ref-nyptrump" class="csl-entry" role="listitem">
Rosner, Elizabeth, Ben Feuerherd, Joe Marino, and Patrick Reilly. 2023. <span>“Donald Trump Indicted in Stormy Daniels Hush Money Probe by Manhattan Grand Jury.”</span> <em>New York Post</em>.
</div>
<div id="ref-foxballoon" class="csl-entry" role="listitem">
Sabes, Adam, Liz Friden, and Jennifer Griffin. 2023. <span>“US Government Monitoring Suspected Chinese Spy Balloon over Northern States.”</span> <em>Fox News</em>.
</div>
<div id="ref-wsjballoon" class="csl-entry" role="listitem">
Salama, Vivian, Nancy A. Youssef, and Michael R. Gordon. 2023. <span>“Chinese Spy Balloon Tracked over u.s. This Week.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-nbchamas" class="csl-entry" role="listitem">
Sanchez, Raf, and Daniel Arkin. 2023. <span>“’We Are in a War,’ Netanyahu Says After Hamas Launches Surprise Attack.”</span> <em>NBC News</em>.
</div>
<div id="ref-cnntrump" class="csl-entry" role="listitem">
Scannell, Kara, John Miller, Jeremy Herb, and Devan Cole. 2023. <span>“Donald Trump Indicted by Manhattan Grand Jury on More Than 30 Counts Related to Business Fraud.”</span> <em>CNN</em>.
</div>
<div id="ref-foxtrump" class="csl-entry" role="listitem">
Singman, Brooke, and Marta Dhanis. 2023. <span>“Trump Indicted After Manhattan DA Probe for Hush Money Payments.”</span> <em>Fox News</em>.
</div>
<div id="ref-nbctanks" class="csl-entry" role="listitem">
Smith, Patrick. 2023. <span>“Ukraine to Get Leopard Tanks from Germany, Ending a Rift Among Western Allies.”</span> <em>NBC News</em>.
</div>
<div id="ref-nyttanks" class="csl-entry" role="listitem">
Solomon, Erika, Peter Baker, and Eric Nagourney. 2023. <span>“Germany and u.s., Dropping Resistance, Will Send Battle Tanks to Ukraine.”</span> <em>The New York Times</em>.
</div>
<div id="ref-foxbiden" class="csl-entry" role="listitem">
Steinhauser, Paul. 2023. <span>“Biden Approval Rating Sinks to All-Time Low in New National Poll.”</span> <em>Fox News</em>.
</div>
<div id="ref-cnnaffirmative" class="csl-entry" role="listitem">
Vogue, Ariane de, Devan Cole, and Tierney Sneed. 2023. <span>“Supreme Court Guts Affirmative Action in College Admissions.”</span> <em>CNN</em>.
</div>
<div id="ref-wpsantos" class="csl-entry" role="listitem">
Wang, Amy B., and Mariana Alfaro. 2023. <span>“Rep. George Santos Expelled from Congress on Bipartisan Vote.”</span> <em>The Washington Post</em>.
</div>
<div id="ref-nbcsantos" class="csl-entry" role="listitem">
Wong, Scott, Dareh Gregorian, Kate Santaliz, and Kyle Stewart. 2023. <span>“House Votes to Expel Indicted Rep. George Santos from Congress.”</span> <em>NBC News</em>.
</div>
<div id="ref-wsjpentagon" class="csl-entry" role="listitem">
Youssef, Nancy A. 2023. <span>“Pentagon Document Leak: What We Know so Far and the Biggest Takeaways.”</span> <em>The Wall Street Journal</em>.
</div>
<div id="ref-bbctrump" class="csl-entry" role="listitem">
Zurcher, Anthony, and Jude Sheerin. 2023. <span>“Donald Trump Indictment: Ex-US President to Be Charged over Hush Money.”</span> <em>BBC</em>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb23" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "How Differently Do News Sources Report on the Same Events?"</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Spring 2024"</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Kamila Palys"</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> false</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">    theme: default</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co">    rendering: embed-resources</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="al">![Source: Unsplash](newspaper.jpg)</span>{fig-alt="A man reading a newspaper."}</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Language of News Sources</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>By now, you've surely heard many opinions and likely have one yourself about which news sources are trustworthy and not. These opinions are usually divided by political party identification. People's views on these matters are also largely subjective, but can they be confirmed or changed with a more technical analysis on news sources? What can we tell about each news source just based on their choice of words? This project takes a closer look at the language that various news sources use and examines if these sources really report on the same events in a different manner. This will be done through sentiment analysis and creating classifiers to see if we can determine the source of an article, given the article text. </span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="fu"># Data Collection</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a> Nine total news sites are studied for a mix of political affiliations, including CNN, The New York Times, The Washington Post, Fox News, New York Post, NBC News, The Wall Street Journal, BBC, and ABC News. Eight political events and topics from 2023 were chosen as the subjects for the articles to be studied for a higher potential for bias. For each topic, one article from each source was collected, making for a total of 72 articles. To ensure as little room for extraneous variables as possible, the articles for a particular topic across sources were chosen in a way so that they were published on the same day or as closely together in time as possible. This would mean that each source should have had the same information available to them when writing the article. The text from each article was collected through copying and pasting into its own text file. In addition, a string denoting each article's topic and source name was added to the end of each article's text file to be able to identify it. </span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Topics chosen for article selection</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Indictment of Trump</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Supreme Court's ruling against affirmative action</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Biden's low approval rates in polls</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Deadliest attack by Hamas to date</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Chinese surveillance balloon</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Pentagon document leaks</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Sending tanks to Ukraine</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Expulsion of George Santos</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a><span class="fu"># Data Preprocessing</span></span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>The text had to go through a lot of preparation before it could be ready for analyzing. Functions were created to make this process faster for each article. One function accesses an article's text file and returns a string of all the text in there. Another function cleans the text, which includes making all text lowercase, removing punctuation, removing stopwords (words like "and," "the," etc.), and stemming words, if desired. Stemming words keeps only the root form of a word, removing suffixes so that words like "jumps" and "jumping" are both reduced to "jump." This is desired for some parts of the analysis in this project, but not others. The idea to create the two preprocessing functions used and what they may include was taken from <span class="co">[</span><span class="ot">Machine Learning Mastery</span><span class="co">](https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/)</span> (see @preprocessing).</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a><span class="co"># don't show future warnings</span></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a><span class="co">#warnings.simplefilter(action='ignore', category=FutureWarning)</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize dictionary to be able to trace back a stemmed word to its original form</span></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>stemmed_dict <span class="op">=</span> {}</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> file_to_string(filename):</span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>  <span class="co">'''Opens the input text file and</span></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a><span class="co">  returns a string of all its text.'''</span></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>  <span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>)</span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>  <span class="bu">file</span>.close()</span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>, <span class="st">' '</span>)</span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'  '</span>, <span class="st">' '</span>)</span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> text</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text, stem):</span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>  <span class="co">'''Takes in a string of text and cleans it by converting</span></span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a><span class="co">  to lowercase, removing punctuation, and removing stopwords. </span></span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a><span class="co">  Also takes in a binary value to indicate if stemming should</span></span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a><span class="co">  be performed. Returns the new string.'''</span></span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> stem <span class="kw">not</span> <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a>      <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Stem must be a binary value (0 or 1)"</span>)</span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>  ps <span class="op">=</span> PorterStemmer()</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>  stemmed_dict <span class="op">=</span> {}</span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create list of stopwords </span></span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a>  stopwords_list <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a>  <span class="co"># make the text lowercase</span></span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.lower()</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'—'</span>, <span class="st">' '</span>)</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the three below would be stemmed to "u", rather than "us" for example</span></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'u.s.'</span>, <span class="st">'us'</span>)</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'u.k.'</span>, <span class="st">'uk'</span>)</span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'u.n.'</span>, <span class="st">'un'</span>)</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>  <span class="co"># convert to ascii characters</span></span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.encode(<span class="st">"ascii"</span>, <span class="st">"ignore"</span>).decode()</span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="bu">chr</span> <span class="kw">in</span> text:</span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a>      <span class="co"># only keep characters in the string that are not punctuation symbols</span></span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (<span class="bu">chr</span> <span class="kw">in</span> string.punctuation <span class="kw">or</span> <span class="bu">chr</span> <span class="kw">in</span> string.digits):</span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a>          text <span class="op">=</span> text.replace(<span class="bu">chr</span>, <span class="st">' '</span>)</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.replace(<span class="st">'  '</span>, <span class="st">' '</span>)</span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a>  <span class="co"># stem the tokens within the text</span></span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a>  tokens <span class="op">=</span> text.split()</span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a>  new_tokens <span class="op">=</span> []</span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> token <span class="kw">in</span> tokens[:<span class="op">-</span><span class="dv">2</span>]: <span class="co"># last two tokens identify source and topic, we do not want to stem them</span></span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a>      <span class="co"># only include new token in the cleaned list if not a stopword</span></span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> stopwords_list:</span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span> stem <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a>              stemmed_word <span class="op">=</span> ps.stem(token)</span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a>              new_tokens.append(stemmed_word)</span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true" tabindex="-1"></a>              <span class="co"># to be able to map each token to the resulting stemmed word</span></span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a>              <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> stemmed_dict:</span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a>                  stemmed_dict[token] <span class="op">=</span> stemmed_word</span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a>          <span class="cf">else</span>:</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a>              new_tokens.append(token)</span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a>  <span class="co"># add back in last two tokens</span></span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a>  new_tokens.append(tokens[<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a>  new_tokens.append(tokens[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true" tabindex="-1"></a>  cleaned_text <span class="op">=</span> <span class="st">" "</span>.join(new_tokens)</span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true" tabindex="-1"></a>  cleaned_text <span class="op">=</span> cleaned_text.replace(<span class="st">'  '</span>, <span class="st">' '</span>)</span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> cleaned_text</span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true" tabindex="-1"></a><span class="fu"># Dataframes</span></span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-122"><a href="#cb23-122" aria-hidden="true" tabindex="-1"></a>All of the article text collected is then transformed into multiple dataframe versions. One way of representing textual data within a dataframe is with binary values. In this case, each column is a token, or a word, that appears in any of the documents (articles) collected. Each row represents one document. The values are either a 0 or 1, denoting whether or not the token appears within the article. Another  dataframe representation has the values instead be the frequencies of each token within an article. Finally, a third dataframe is created with the Term Frequency-Inverse Document Frequency (TF-IDF) scores of the tokens within an article as the values. A TF-IDF score signifies the importance and uniqueness of a word within an article, as it compares to the remaining documents. Some of these dataframes may work better than others when it comes to the predictive modeling portion of the project and revealing patterns within the text of a news source. An example dataframe is shown below, with the TF-IDF scores of each stemmed token shown for the first five articles (rows).</span>
<span id="cb23-123"><a href="#cb23-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-124"><a href="#cb23-124" aria-hidden="true" tabindex="-1"></a>For the clustering part of the analysis, a new dataframe is also created with each row representing not just one article, but all articles from a single source. This is because we will be clustering entire sources and not individual articles. </span>
<span id="cb23-125"><a href="#cb23-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-128"><a href="#cb23-128" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-129"><a href="#cb23-129" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb23-130"><a href="#cb23-130" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb23-131"><a href="#cb23-131" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb23-132"><a href="#cb23-132" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'TF_CPP_MIN_LOG_LEVEL'</span>] <span class="op">=</span> <span class="st">'1'</span></span>
<span id="cb23-133"><a href="#cb23-133" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> listdir</span>
<span id="cb23-134"><a href="#cb23-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-135"><a href="#cb23-135" aria-hidden="true" tabindex="-1"></a><span class="co"># we want to be in the capstone folder, not in the doc folder</span></span>
<span id="cb23-136"><a href="#cb23-136" aria-hidden="true" tabindex="-1"></a>os.chdir(<span class="st">".."</span>)</span>
<span id="cb23-137"><a href="#cb23-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-138"><a href="#cb23-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-141"><a href="#cb23-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-142"><a href="#cb23-142" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-143"><a href="#cb23-143" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-144"><a href="#cb23-144" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb23-145"><a href="#cb23-145" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb23-146"><a href="#cb23-146" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> PorterStemmer</span>
<span id="cb23-147"><a href="#cb23-147" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb23-148"><a href="#cb23-148" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> string <span class="im">import</span> punctuation</span>
<span id="cb23-149"><a href="#cb23-149" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb23-150"><a href="#cb23-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-151"><a href="#cb23-151" aria-hidden="true" tabindex="-1"></a><span class="co"># looping through all text files to apply preprocessing functions</span></span>
<span id="cb23-152"><a href="#cb23-152" aria-hidden="true" tabindex="-1"></a>article_docs <span class="op">=</span> []</span>
<span id="cb23-153"><a href="#cb23-153" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> os.listdir(<span class="st">'data/text/'</span>)</span>
<span id="cb23-154"><a href="#cb23-154" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span>.sort()</span>
<span id="cb23-155"><a href="#cb23-155" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> filename <span class="kw">in</span> <span class="bu">dir</span>:</span>
<span id="cb23-156"><a href="#cb23-156" aria-hidden="true" tabindex="-1"></a>    filepath <span class="op">=</span> os.path.join(<span class="st">'data/text/'</span>, filename)</span>
<span id="cb23-157"><a href="#cb23-157" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> filename.split(<span class="st">"."</span>)[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">"txt"</span>:</span>
<span id="cb23-158"><a href="#cb23-158" aria-hidden="true" tabindex="-1"></a>        article_string <span class="op">=</span> file_to_string(filepath)</span>
<span id="cb23-159"><a href="#cb23-159" aria-hidden="true" tabindex="-1"></a>        new_string <span class="op">=</span> clean_text(article_string, <span class="dv">1</span>)</span>
<span id="cb23-160"><a href="#cb23-160" aria-hidden="true" tabindex="-1"></a>        article_docs.append(new_string)</span>
<span id="cb23-161"><a href="#cb23-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-162"><a href="#cb23-162" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the list of article strings into a binary-value dataframe</span></span>
<span id="cb23-163"><a href="#cb23-163" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> Tokenizer()</span>
<span id="cb23-164"><a href="#cb23-164" aria-hidden="true" tabindex="-1"></a>t.fit_on_texts(article_docs)</span>
<span id="cb23-165"><a href="#cb23-165" aria-hidden="true" tabindex="-1"></a>encoded_docs <span class="op">=</span> t.texts_to_matrix(article_docs, mode<span class="op">=</span><span class="st">'binary'</span>)</span>
<span id="cb23-166"><a href="#cb23-166" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> t.word_index.keys()]</span>
<span id="cb23-167"><a href="#cb23-167" aria-hidden="true" tabindex="-1"></a>binary_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_docs[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb23-168"><a href="#cb23-168" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb23-169"><a href="#cb23-169" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb23-170"><a href="#cb23-170" aria-hidden="true" tabindex="-1"></a>      binary_df[<span class="st">'abcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-171"><a href="#cb23-171" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'bbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-172"><a href="#cb23-172" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'cnnarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-173"><a href="#cb23-173" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'foxarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-174"><a href="#cb23-174" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'nbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-175"><a href="#cb23-175" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'nyparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-176"><a href="#cb23-176" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'nytarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-177"><a href="#cb23-177" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'wparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-178"><a href="#cb23-178" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'wsjarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-179"><a href="#cb23-179" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-180"><a href="#cb23-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-181"><a href="#cb23-181" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb23-182"><a href="#cb23-182" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb23-183"><a href="#cb23-183" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb23-184"><a href="#cb23-184" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb23-185"><a href="#cb23-185" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb23-186"><a href="#cb23-186" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb23-187"><a href="#cb23-187" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb23-188"><a href="#cb23-188" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb23-189"><a href="#cb23-189" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb23-190"><a href="#cb23-190" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb23-191"><a href="#cb23-191" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb23-192"><a href="#cb23-192" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-193"><a href="#cb23-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-194"><a href="#cb23-194" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb23-195"><a href="#cb23-195" aria-hidden="true" tabindex="-1"></a>topic_conditions <span class="op">=</span> [</span>
<span id="cb23-196"><a href="#cb23-196" aria-hidden="true" tabindex="-1"></a>      binary_df[<span class="st">'affirmativearticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-197"><a href="#cb23-197" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'balloonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-198"><a href="#cb23-198" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'bidenarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-199"><a href="#cb23-199" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'hamasarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-200"><a href="#cb23-200" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'pentagonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-201"><a href="#cb23-201" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'santosarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-202"><a href="#cb23-202" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'tanksarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-203"><a href="#cb23-203" aria-hidden="true" tabindex="-1"></a>    , binary_df[<span class="st">'trumparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-204"><a href="#cb23-204" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-205"><a href="#cb23-205" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb23-206"><a href="#cb23-206" aria-hidden="true" tabindex="-1"></a>topic_choices  <span class="op">=</span> [</span>
<span id="cb23-207"><a href="#cb23-207" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb23-208"><a href="#cb23-208" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb23-209"><a href="#cb23-209" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb23-210"><a href="#cb23-210" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb23-211"><a href="#cb23-211" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb23-212"><a href="#cb23-212" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb23-213"><a href="#cb23-213" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb23-214"><a href="#cb23-214" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb23-215"><a href="#cb23-215" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-216"><a href="#cb23-216" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb23-217"><a href="#cb23-217" aria-hidden="true" tabindex="-1"></a>binary_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb23-218"><a href="#cb23-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-219"><a href="#cb23-219" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new topic column</span></span>
<span id="cb23-220"><a href="#cb23-220" aria-hidden="true" tabindex="-1"></a>binary_df[<span class="st">"article_topic"</span>] <span class="op">=</span> np.select(topic_conditions, topic_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb23-221"><a href="#cb23-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-222"><a href="#cb23-222" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the token columns that identify the topic/source, not part of actual article text</span></span>
<span id="cb23-223"><a href="#cb23-223" aria-hidden="true" tabindex="-1"></a>binary_df.drop(columns<span class="op">=</span>[<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>, <span class="st">'nbcarticle'</span>,</span>
<span id="cb23-224"><a href="#cb23-224" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>, <span class="st">'wsjarticle'</span>, <span class="st">'affirmativearticle'</span>,</span>
<span id="cb23-225"><a href="#cb23-225" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'balloonarticle'</span>, <span class="st">'bidenarticle'</span>, <span class="st">'hamasarticle'</span>, <span class="st">'pentagonarticle'</span>,</span>
<span id="cb23-226"><a href="#cb23-226" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'santosarticle'</span>, <span class="st">'tanksarticle'</span>, <span class="st">'trumparticle'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-227"><a href="#cb23-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-228"><a href="#cb23-228" aria-hidden="true" tabindex="-1"></a>binary_df.head()</span>
<span id="cb23-229"><a href="#cb23-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-230"><a href="#cb23-230" aria-hidden="true" tabindex="-1"></a>encoded_docs_freq <span class="op">=</span> t.texts_to_matrix(article_docs, mode<span class="op">=</span><span class="st">'count'</span>)</span>
<span id="cb23-231"><a href="#cb23-231" aria-hidden="true" tabindex="-1"></a>freq_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_docs_freq[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb23-232"><a href="#cb23-232" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb23-233"><a href="#cb23-233" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb23-234"><a href="#cb23-234" aria-hidden="true" tabindex="-1"></a>      freq_df[<span class="st">'abcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-235"><a href="#cb23-235" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'bbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-236"><a href="#cb23-236" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'cnnarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-237"><a href="#cb23-237" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'foxarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-238"><a href="#cb23-238" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'nbcarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-239"><a href="#cb23-239" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'nyparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-240"><a href="#cb23-240" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'nytarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-241"><a href="#cb23-241" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'wparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-242"><a href="#cb23-242" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'wsjarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-243"><a href="#cb23-243" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-244"><a href="#cb23-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-245"><a href="#cb23-245" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb23-246"><a href="#cb23-246" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb23-247"><a href="#cb23-247" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb23-248"><a href="#cb23-248" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb23-249"><a href="#cb23-249" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb23-250"><a href="#cb23-250" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb23-251"><a href="#cb23-251" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb23-252"><a href="#cb23-252" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb23-253"><a href="#cb23-253" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb23-254"><a href="#cb23-254" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb23-255"><a href="#cb23-255" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb23-256"><a href="#cb23-256" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-257"><a href="#cb23-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-258"><a href="#cb23-258" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb23-259"><a href="#cb23-259" aria-hidden="true" tabindex="-1"></a>topic_conditions <span class="op">=</span> [</span>
<span id="cb23-260"><a href="#cb23-260" aria-hidden="true" tabindex="-1"></a>      freq_df[<span class="st">'affirmativearticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-261"><a href="#cb23-261" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'balloonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-262"><a href="#cb23-262" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'bidenarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-263"><a href="#cb23-263" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'hamasarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-264"><a href="#cb23-264" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'pentagonarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-265"><a href="#cb23-265" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'santosarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-266"><a href="#cb23-266" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'tanksarticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-267"><a href="#cb23-267" aria-hidden="true" tabindex="-1"></a>    , freq_df[<span class="st">'trumparticle'</span>] <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb23-268"><a href="#cb23-268" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-269"><a href="#cb23-269" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb23-270"><a href="#cb23-270" aria-hidden="true" tabindex="-1"></a>topic_choices  <span class="op">=</span> [</span>
<span id="cb23-271"><a href="#cb23-271" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb23-272"><a href="#cb23-272" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb23-273"><a href="#cb23-273" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb23-274"><a href="#cb23-274" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb23-275"><a href="#cb23-275" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb23-276"><a href="#cb23-276" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb23-277"><a href="#cb23-277" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb23-278"><a href="#cb23-278" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb23-279"><a href="#cb23-279" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-280"><a href="#cb23-280" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb23-281"><a href="#cb23-281" aria-hidden="true" tabindex="-1"></a>freq_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb23-282"><a href="#cb23-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-283"><a href="#cb23-283" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new topic column</span></span>
<span id="cb23-284"><a href="#cb23-284" aria-hidden="true" tabindex="-1"></a>freq_df[<span class="st">"article_topic"</span>] <span class="op">=</span> np.select(topic_conditions, topic_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb23-285"><a href="#cb23-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-286"><a href="#cb23-286" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the token columns that identify the topic/source, not part of actual article text</span></span>
<span id="cb23-287"><a href="#cb23-287" aria-hidden="true" tabindex="-1"></a>freq_df.drop(columns<span class="op">=</span>[<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>, <span class="st">'nbcarticle'</span>,</span>
<span id="cb23-288"><a href="#cb23-288" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>, <span class="st">'wsjarticle'</span>, <span class="st">'affirmativearticle'</span>,</span>
<span id="cb23-289"><a href="#cb23-289" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'balloonarticle'</span>, <span class="st">'bidenarticle'</span>, <span class="st">'hamasarticle'</span>, <span class="st">'pentagonarticle'</span>,</span>
<span id="cb23-290"><a href="#cb23-290" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'santosarticle'</span>, <span class="st">'tanksarticle'</span>, <span class="st">'trumparticle'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-291"><a href="#cb23-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-292"><a href="#cb23-292" aria-hidden="true" tabindex="-1"></a>freq_df.head()</span>
<span id="cb23-293"><a href="#cb23-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-294"><a href="#cb23-294" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataframe with tf-idf values</span></span>
<span id="cb23-295"><a href="#cb23-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-296"><a href="#cb23-296" aria-hidden="true" tabindex="-1"></a>encoded_docs_tfidf <span class="op">=</span> t.texts_to_matrix(article_docs, mode<span class="op">=</span><span class="st">'tfidf'</span>)</span>
<span id="cb23-297"><a href="#cb23-297" aria-hidden="true" tabindex="-1"></a>tfidf_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> encoded_docs_tfidf[:, <span class="dv">1</span>:], columns<span class="op">=</span>words)</span>
<span id="cb23-298"><a href="#cb23-298" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb23-299"><a href="#cb23-299" aria-hidden="true" tabindex="-1"></a>source_conditions <span class="op">=</span> [</span>
<span id="cb23-300"><a href="#cb23-300" aria-hidden="true" tabindex="-1"></a>      tfidf_df[<span class="st">'abcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-301"><a href="#cb23-301" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'bbcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-302"><a href="#cb23-302" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'cnnarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-303"><a href="#cb23-303" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'foxarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-304"><a href="#cb23-304" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'nbcarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-305"><a href="#cb23-305" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'nyparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-306"><a href="#cb23-306" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'nytarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-307"><a href="#cb23-307" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'wparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-308"><a href="#cb23-308" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'wsjarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-309"><a href="#cb23-309" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-310"><a href="#cb23-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-311"><a href="#cb23-311" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb23-312"><a href="#cb23-312" aria-hidden="true" tabindex="-1"></a>source_choices  <span class="op">=</span> [</span>
<span id="cb23-313"><a href="#cb23-313" aria-hidden="true" tabindex="-1"></a>      <span class="st">"ABC News"</span></span>
<span id="cb23-314"><a href="#cb23-314" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"BBC"</span></span>
<span id="cb23-315"><a href="#cb23-315" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"CNN"</span></span>
<span id="cb23-316"><a href="#cb23-316" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Fox News"</span></span>
<span id="cb23-317"><a href="#cb23-317" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"NBC News"</span></span>
<span id="cb23-318"><a href="#cb23-318" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"New York Post"</span></span>
<span id="cb23-319"><a href="#cb23-319" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The New York Times"</span></span>
<span id="cb23-320"><a href="#cb23-320" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Washington Post"</span></span>
<span id="cb23-321"><a href="#cb23-321" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Wall Street Journal"</span></span>
<span id="cb23-322"><a href="#cb23-322" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-323"><a href="#cb23-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-324"><a href="#cb23-324" aria-hidden="true" tabindex="-1"></a><span class="co"># List of conditions</span></span>
<span id="cb23-325"><a href="#cb23-325" aria-hidden="true" tabindex="-1"></a>topic_conditions <span class="op">=</span> [</span>
<span id="cb23-326"><a href="#cb23-326" aria-hidden="true" tabindex="-1"></a>      tfidf_df[<span class="st">'affirmativearticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-327"><a href="#cb23-327" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'balloonarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-328"><a href="#cb23-328" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'bidenarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-329"><a href="#cb23-329" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'hamasarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-330"><a href="#cb23-330" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'pentagonarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-331"><a href="#cb23-331" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'santosarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-332"><a href="#cb23-332" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'tanksarticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-333"><a href="#cb23-333" aria-hidden="true" tabindex="-1"></a>    , tfidf_df[<span class="st">'trumparticle'</span>] <span class="op">!=</span> <span class="dv">0</span></span>
<span id="cb23-334"><a href="#cb23-334" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-335"><a href="#cb23-335" aria-hidden="true" tabindex="-1"></a><span class="co"># List of values to return</span></span>
<span id="cb23-336"><a href="#cb23-336" aria-hidden="true" tabindex="-1"></a>topic_choices  <span class="op">=</span> [</span>
<span id="cb23-337"><a href="#cb23-337" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb23-338"><a href="#cb23-338" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb23-339"><a href="#cb23-339" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb23-340"><a href="#cb23-340" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb23-341"><a href="#cb23-341" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb23-342"><a href="#cb23-342" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb23-343"><a href="#cb23-343" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb23-344"><a href="#cb23-344" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb23-345"><a href="#cb23-345" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-346"><a href="#cb23-346" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new source column </span></span>
<span id="cb23-347"><a href="#cb23-347" aria-hidden="true" tabindex="-1"></a>tfidf_df[<span class="st">"article_source"</span>] <span class="op">=</span> np.select(source_conditions, source_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb23-348"><a href="#cb23-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-349"><a href="#cb23-349" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new topic column</span></span>
<span id="cb23-350"><a href="#cb23-350" aria-hidden="true" tabindex="-1"></a>tfidf_df[<span class="st">"article_topic"</span>] <span class="op">=</span> np.select(topic_conditions, topic_choices, <span class="st">"ERROR"</span>)</span>
<span id="cb23-351"><a href="#cb23-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-352"><a href="#cb23-352" aria-hidden="true" tabindex="-1"></a><span class="co"># drop the token columns that identify the topic/source, not part of actual article text</span></span>
<span id="cb23-353"><a href="#cb23-353" aria-hidden="true" tabindex="-1"></a>tfidf_df.drop(columns<span class="op">=</span>[<span class="st">'abcarticle'</span>, <span class="st">'bbcarticle'</span>, <span class="st">'cnnarticle'</span>, <span class="st">'foxarticle'</span>, <span class="st">'nbcarticle'</span>,</span>
<span id="cb23-354"><a href="#cb23-354" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'nyparticle'</span>, <span class="st">'nytarticle'</span>, <span class="st">'wparticle'</span>, <span class="st">'wsjarticle'</span>, <span class="st">'affirmativearticle'</span>,</span>
<span id="cb23-355"><a href="#cb23-355" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'balloonarticle'</span>, <span class="st">'bidenarticle'</span>, <span class="st">'hamasarticle'</span>, <span class="st">'pentagonarticle'</span>,</span>
<span id="cb23-356"><a href="#cb23-356" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'santosarticle'</span>, <span class="st">'tanksarticle'</span>, <span class="st">'trumparticle'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-357"><a href="#cb23-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-358"><a href="#cb23-358" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of strings where each string is all articles from one source (for dendogram)</span></span>
<span id="cb23-359"><a href="#cb23-359" aria-hidden="true" tabindex="-1"></a>source_docs <span class="op">=</span> []</span>
<span id="cb23-360"><a href="#cb23-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-361"><a href="#cb23-361" aria-hidden="true" tabindex="-1"></a>j <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-362"><a href="#cb23-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-363"><a href="#cb23-363" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>):</span>
<span id="cb23-364"><a href="#cb23-364" aria-hidden="true" tabindex="-1"></a>    <span class="co"># looping through each article of each source, and only taking up until and not including second to last</span></span>
<span id="cb23-365"><a href="#cb23-365" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token, bc last two tokens represent the name of the source and topic of article</span></span>
<span id="cb23-366"><a href="#cb23-366" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for last article, we index up until and not including only the last token bc we don't want to include</span></span>
<span id="cb23-367"><a href="#cb23-367" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the topic of the article, but we do want to include the source name, which is the second to last token</span></span>
<span id="cb23-368"><a href="#cb23-368" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> <span class="st">" "</span>.join(article_docs[j].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> <span class="st">" "</span>.join(article_docs[j<span class="op">+</span><span class="dv">1</span>].split()[:<span class="op">-</span><span class="dv">2</span>]) <span class="op">+</span> <span class="st">" "</span>\</span>
<span id="cb23-369"><a href="#cb23-369" aria-hidden="true" tabindex="-1"></a>        + " ".join(article_docs<span class="co">[</span><span class="ot">j+2</span><span class="co">]</span>.split()<span class="co">[</span><span class="ot">:-2</span><span class="co">]</span>) + " " + " ".join(article_docs<span class="co">[</span><span class="ot">j+3</span><span class="co">]</span>.split()<span class="co">[</span><span class="ot">:-2</span><span class="co">]</span>) + " "\</span>
<span id="cb23-370"><a href="#cb23-370" aria-hidden="true" tabindex="-1"></a><span class="ss">        + </span>" ".join(article_docs<span class="co">[</span><span class="ot">j+4</span><span class="co">]</span>.split()<span class="co">[</span><span class="ot">:-2</span><span class="co">]</span>) + " " + " ".join(article_docs<span class="co">[</span><span class="ot">j+5</span><span class="co">]</span>.split()<span class="co">[</span><span class="ot">:-2</span><span class="co">]</span>) + " "\</span>
<span id="cb23-371"><a href="#cb23-371" aria-hidden="true" tabindex="-1"></a><span class="ss">        + </span>" ".join(article_docs<span class="co">[</span><span class="ot">j+6</span><span class="co">]</span>.split()<span class="co">[</span><span class="ot">:-2</span><span class="co">]</span>) + " " + " ".join(article_docs<span class="co">[</span><span class="ot">j+7</span><span class="co">]</span>.split()<span class="co">[</span><span class="ot">:-1</span><span class="co">]</span>)</span>
<span id="cb23-372"><a href="#cb23-372" aria-hidden="true" tabindex="-1"></a>    source_docs.append(source)</span>
<span id="cb23-373"><a href="#cb23-373" aria-hidden="true" tabindex="-1"></a>    j += 8</span>
<span id="cb23-374"><a href="#cb23-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-375"><a href="#cb23-375" aria-hidden="true" tabindex="-1"></a>source_docs</span>
<span id="cb23-376"><a href="#cb23-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-377"><a href="#cb23-377" aria-hidden="true" tabindex="-1"></a><span class="fu"># create a dataframe of token tf-idf's with each row representing all articles of one source</span></span>
<span id="cb23-378"><a href="#cb23-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-379"><a href="#cb23-379" aria-hidden="true" tabindex="-1"></a><span class="fu"># convert the list of article strings into a tf-idf-value dataframe</span></span>
<span id="cb23-380"><a href="#cb23-380" aria-hidden="true" tabindex="-1"></a>t = Tokenizer()</span>
<span id="cb23-381"><a href="#cb23-381" aria-hidden="true" tabindex="-1"></a>t.fit_on_texts(source_docs)</span>
<span id="cb23-382"><a href="#cb23-382" aria-hidden="true" tabindex="-1"></a>encoded_source_docs = t.texts_to_matrix(source_docs, mode='tfidf')</span>
<span id="cb23-383"><a href="#cb23-383" aria-hidden="true" tabindex="-1"></a>words = <span class="co">[</span><span class="ot">x for x in t.word_index.keys()</span><span class="co">]</span></span>
<span id="cb23-384"><a href="#cb23-384" aria-hidden="true" tabindex="-1"></a>tfidf_source_df = pd.DataFrame(data = encoded_source_docs<span class="co">[</span><span class="ot">:, 1:</span><span class="co">]</span>, columns=words)</span>
<span id="cb23-385"><a href="#cb23-385" aria-hidden="true" tabindex="-1"></a><span class="fu"># List of conditions</span></span>
<span id="cb23-386"><a href="#cb23-386" aria-hidden="true" tabindex="-1"></a>source_conditions = [</span>
<span id="cb23-387"><a href="#cb23-387" aria-hidden="true" tabindex="-1"></a>      tfidf_source_df<span class="co">[</span><span class="ot">'abcarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-388"><a href="#cb23-388" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'bbcarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-389"><a href="#cb23-389" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'cnnarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-390"><a href="#cb23-390" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'foxarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-391"><a href="#cb23-391" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'nbcarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-392"><a href="#cb23-392" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'nyparticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-393"><a href="#cb23-393" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'nytarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-394"><a href="#cb23-394" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'wparticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-395"><a href="#cb23-395" aria-hidden="true" tabindex="-1"></a>    , tfidf_source_df<span class="co">[</span><span class="ot">'wsjarticle'</span><span class="co">]</span> != 0</span>
<span id="cb23-396"><a href="#cb23-396" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-397"><a href="#cb23-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-398"><a href="#cb23-398" aria-hidden="true" tabindex="-1"></a><span class="fu"># List of values to return</span></span>
<span id="cb23-399"><a href="#cb23-399" aria-hidden="true" tabindex="-1"></a>source_choices  = [</span>
<span id="cb23-400"><a href="#cb23-400" aria-hidden="true" tabindex="-1"></a>      "ABC News"</span>
<span id="cb23-401"><a href="#cb23-401" aria-hidden="true" tabindex="-1"></a>    , "BBC"</span>
<span id="cb23-402"><a href="#cb23-402" aria-hidden="true" tabindex="-1"></a>    , "CNN"</span>
<span id="cb23-403"><a href="#cb23-403" aria-hidden="true" tabindex="-1"></a>    , "Fox News"</span>
<span id="cb23-404"><a href="#cb23-404" aria-hidden="true" tabindex="-1"></a>    , "NBC News"</span>
<span id="cb23-405"><a href="#cb23-405" aria-hidden="true" tabindex="-1"></a>    , "New York Post"</span>
<span id="cb23-406"><a href="#cb23-406" aria-hidden="true" tabindex="-1"></a>    , "The New York Times"</span>
<span id="cb23-407"><a href="#cb23-407" aria-hidden="true" tabindex="-1"></a>    , "The Washington Post"</span>
<span id="cb23-408"><a href="#cb23-408" aria-hidden="true" tabindex="-1"></a>    , "The Wall Street Journal"</span>
<span id="cb23-409"><a href="#cb23-409" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-410"><a href="#cb23-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-411"><a href="#cb23-411" aria-hidden="true" tabindex="-1"></a><span class="fu"># create a new source column </span></span>
<span id="cb23-412"><a href="#cb23-412" aria-hidden="true" tabindex="-1"></a>tfidf_source_df<span class="co">[</span><span class="ot">"article_source"</span><span class="co">]</span> = np.select(source_conditions, source_choices, "ERROR")</span>
<span id="cb23-413"><a href="#cb23-413" aria-hidden="true" tabindex="-1"></a><span class="fu"># remove article_source column and make it the index</span></span>
<span id="cb23-414"><a href="#cb23-414" aria-hidden="true" tabindex="-1"></a>tfidf_source_df.set_index('article_source', inplace=True) </span>
<span id="cb23-415"><a href="#cb23-415" aria-hidden="true" tabindex="-1"></a><span class="fu"># drop the columns for the tokens that were used to identify the articles</span></span>
<span id="cb23-416"><a href="#cb23-416" aria-hidden="true" tabindex="-1"></a>tfidf_source_df.drop(['abcarticle', 'bbcarticle', 'cnnarticle', 'foxarticle',</span>
<span id="cb23-417"><a href="#cb23-417" aria-hidden="true" tabindex="-1"></a>                      'nbcarticle', 'nyparticle', 'nytarticle', 'wparticle',</span>
<span id="cb23-418"><a href="#cb23-418" aria-hidden="true" tabindex="-1"></a>                      'wsjarticle'], axis=1, inplace=True)</span>
<span id="cb23-419"><a href="#cb23-419" aria-hidden="true" tabindex="-1"></a><span class="fu"># tfidf_source_df</span></span>
<span id="cb23-420"><a href="#cb23-420" aria-hidden="true" tabindex="-1"></a>tfidf_df.head()</span>
<span id="cb23-421"><a href="#cb23-421" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-422"><a href="#cb23-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-423"><a href="#cb23-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-424"><a href="#cb23-424" aria-hidden="true" tabindex="-1"></a><span class="in"># Exploratory Analysis</span></span>
<span id="cb23-425"><a href="#cb23-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-426"><a href="#cb23-426" aria-hidden="true" tabindex="-1"></a><span class="in">## Clustering</span></span>
<span id="cb23-427"><a href="#cb23-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-428"><a href="#cb23-428" aria-hidden="true" tabindex="-1"></a><span class="in">To begin exploring this data, we may be interested in clustering news sources together to see which ones are most similar to each other. This can be done in a dendrogram using the TF-IDF scores of their tokens, for example. </span></span>
<span id="cb23-429"><a href="#cb23-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-430"><a href="#cb23-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-433"><a href="#cb23-433" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-434"><a href="#cb23-434" aria-hidden="true" tabindex="-1"></a><span class="in"># create dendrogram with complete linkage from tfidf scores df,</span></span>
<span id="cb23-435"><a href="#cb23-435" aria-hidden="true" tabindex="-1"></a><span class="in"># whose rows each represent one source</span></span>
<span id="cb23-436"><a href="#cb23-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-437"><a href="#cb23-437" aria-hidden="true" tabindex="-1"></a><span class="in">from scipy.cluster.hierarchy import dendrogram, linkage</span></span>
<span id="cb23-438"><a href="#cb23-438" aria-hidden="true" tabindex="-1"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb23-439"><a href="#cb23-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-440"><a href="#cb23-440" aria-hidden="true" tabindex="-1"></a><span class="in">Z = linkage(tfidf_source_df, 'complete')</span></span>
<span id="cb23-441"><a href="#cb23-441" aria-hidden="true" tabindex="-1"></a><span class="in">fig = plt.figure(figsize=(14, 4))</span></span>
<span id="cb23-442"><a href="#cb23-442" aria-hidden="true" tabindex="-1"></a><span class="in">fig.suptitle("Complete Linkage", fontsize=14)</span></span>
<span id="cb23-443"><a href="#cb23-443" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel('Source', fontsize=6)</span></span>
<span id="cb23-444"><a href="#cb23-444" aria-hidden="true" tabindex="-1"></a><span class="in">plt.yticks(fontsize = 8) </span></span>
<span id="cb23-445"><a href="#cb23-445" aria-hidden="true" tabindex="-1"></a><span class="in">dn = dendrogram(Z, labels=tfidf_source_df.index)</span></span>
<span id="cb23-446"><a href="#cb23-446" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xticks(fontsize = 8)</span></span>
<span id="cb23-447"><a href="#cb23-447" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb23-448"><a href="#cb23-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-449"><a href="#cb23-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-450"><a href="#cb23-450" aria-hidden="true" tabindex="-1"></a>According to the dendrogram, ABC News and NBC News as well as BBC and Fox News are the two pairs of sources that are most similar to one another. Again, the articles were compared by looking at how similar their TF-IDF scores were for the same tokens. We also see that there aren't two distinct groups of sources clustered together, while we may have expected sources to be clustered together based on their political affiliation. However, it's important to keep in mind that these results are based on only the small sample of articles used for each source and may not be generalizable to the sources as wholes. It is also important to keep in mind that the similarity of these sources is solely being based on the vocabulary usage of each source and not any other factors.  </span>
<span id="cb23-451"><a href="#cb23-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-452"><a href="#cb23-452" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wordclouds</span></span>
<span id="cb23-453"><a href="#cb23-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-454"><a href="#cb23-454" aria-hidden="true" tabindex="-1"></a>Another thing that could be interesting to look at are the wordclouds for each source. On a wordcloud, the bigger the word is, the more frequently it appeared within the text. Looking at these could give a sense of the difference in vocabulary between each source. This is why topic coverage was kept the same across all sources, so that comparisons like these may be made to scope out differences in wording, given that the sources are each writing about the same topics.</span>
<span id="cb23-455"><a href="#cb23-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-458"><a href="#cb23-458" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-459"><a href="#cb23-459" aria-hidden="true" tabindex="-1"></a><span class="co"># create a function to be able to make a series of wordclouds, one for each source</span></span>
<span id="cb23-460"><a href="#cb23-460" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb23-461"><a href="#cb23-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-462"><a href="#cb23-462" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_wordcloud(text, title):</span>
<span id="cb23-463"><a href="#cb23-463" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Given a string of all text and a string</span></span>
<span id="cb23-464"><a href="#cb23-464" aria-hidden="true" tabindex="-1"></a><span class="co">    for the title, creates a wordcloud.'''</span></span>
<span id="cb23-465"><a href="#cb23-465" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize <span class="op">=</span> (<span class="dv">6</span>,<span class="dv">3</span>))</span>
<span id="cb23-466"><a href="#cb23-466" aria-hidden="true" tabindex="-1"></a>    wc <span class="op">=</span> WordCloud(colormap <span class="op">=</span> <span class="st">"YlOrRd"</span>, background_color<span class="op">=</span><span class="st">'white'</span>, width<span class="op">=</span><span class="dv">1600</span>, height<span class="op">=</span><span class="dv">800</span>, max_font_size <span class="op">=</span> <span class="dv">400</span>).generate(text)</span>
<span id="cb23-467"><a href="#cb23-467" aria-hidden="true" tabindex="-1"></a>    plt.title(title, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb23-468"><a href="#cb23-468" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb23-469"><a href="#cb23-469" aria-hidden="true" tabindex="-1"></a>    title_snake <span class="op">=</span> title.lower().replace(<span class="st">" "</span>, <span class="st">"_"</span>)</span>
<span id="cb23-470"><a href="#cb23-470" aria-hidden="true" tabindex="-1"></a>    wc.to_file(<span class="ss">f'wordcloud_</span><span class="sc">{</span>title_snake<span class="sc">}</span><span class="ss">.png'</span>)</span>
<span id="cb23-471"><a href="#cb23-471" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout(pad<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-472"><a href="#cb23-472" aria-hidden="true" tabindex="-1"></a>    plt.imshow(wc)</span>
<span id="cb23-473"><a href="#cb23-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-474"><a href="#cb23-474" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the list of strings, each one representing all articles'</span></span>
<span id="cb23-475"><a href="#cb23-475" aria-hidden="true" tabindex="-1"></a><span class="co"># text from a given source, and create the wordcloud for it. </span></span>
<span id="cb23-476"><a href="#cb23-476" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(source_docs)):</span>
<span id="cb23-477"><a href="#cb23-477" aria-hidden="true" tabindex="-1"></a>    create_wordcloud(source_docs[i], tfidf_source_df.index[i])</span>
<span id="cb23-478"><a href="#cb23-478" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-479"><a href="#cb23-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-480"><a href="#cb23-480" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb23-481"><a href="#cb23-481" aria-hidden="true" tabindex="-1"></a><span class="fu">## Looking at the wordclouds</span></span>
<span id="cb23-482"><a href="#cb23-482" aria-hidden="true" tabindex="-1"></a>The words are not misspelled. Stemmed words were used for the creation of the wordclouds to avoid repetition of common words that just appear in various tenses.</span>
<span id="cb23-483"><a href="#cb23-483" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb23-484"><a href="#cb23-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-485"><a href="#cb23-485" aria-hidden="true" tabindex="-1"></a>The wordclouds for each source look mostly similar, with the most common word throughout them all being "said." This is unsurprising. The only exception here is Fox News, whose most frequently appearing word throughout all the articles is "Trump." Trump is a frequently appearing word for all sources because one of the topics chosen for the articles is explicitly about Trump. However, for Fox News, it is even more commonly used than the word "said." This is interesting, but this alone should not provoke any conclusions about this source being biased towards Trump, for example. All that this means is that "Trump" is the word that appeared the most frequently throughout Fox News' articles, which could potentially be because Fox News had the longest article about Trump. Longer articles will certainly have a higher count of words overall. This can be investigated further.</span>
<span id="cb23-486"><a href="#cb23-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-487"><a href="#cb23-487" aria-hidden="true" tabindex="-1"></a><span class="fu">### Does the length of an article have to do with how frequently a certain word appears?</span></span>
<span id="cb23-488"><a href="#cb23-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-491"><a href="#cb23-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-492"><a href="#cb23-492" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of unstemmed article document texts</span></span>
<span id="cb23-493"><a href="#cb23-493" aria-hidden="true" tabindex="-1"></a><span class="co"># looping through all text files to apply preprocessing functions</span></span>
<span id="cb23-494"><a href="#cb23-494" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotnine <span class="im">import</span> <span class="op">*</span></span>
<span id="cb23-495"><a href="#cb23-495" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image</span>
<span id="cb23-496"><a href="#cb23-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-497"><a href="#cb23-497" aria-hidden="true" tabindex="-1"></a>article_docs_unstemmed <span class="op">=</span> []</span>
<span id="cb23-498"><a href="#cb23-498" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> os.listdir(<span class="st">'data/text/'</span>)</span>
<span id="cb23-499"><a href="#cb23-499" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span>.sort()</span>
<span id="cb23-500"><a href="#cb23-500" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> filename <span class="kw">in</span> <span class="bu">dir</span>:</span>
<span id="cb23-501"><a href="#cb23-501" aria-hidden="true" tabindex="-1"></a>    filepath <span class="op">=</span> os.path.join(<span class="st">'data/text/'</span>, filename)</span>
<span id="cb23-502"><a href="#cb23-502" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> filename.split(<span class="st">"."</span>)[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">"txt"</span>:</span>
<span id="cb23-503"><a href="#cb23-503" aria-hidden="true" tabindex="-1"></a>        article_string <span class="op">=</span> file_to_string(filepath)</span>
<span id="cb23-504"><a href="#cb23-504" aria-hidden="true" tabindex="-1"></a>        new_string <span class="op">=</span> clean_text(article_string, <span class="dv">0</span>)</span>
<span id="cb23-505"><a href="#cb23-505" aria-hidden="true" tabindex="-1"></a>        article_docs_unstemmed.append(new_string)</span>
<span id="cb23-506"><a href="#cb23-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-507"><a href="#cb23-507" aria-hidden="true" tabindex="-1"></a><span class="co"># investigating why Fox News has Trump as most frequent word,</span></span>
<span id="cb23-508"><a href="#cb23-508" aria-hidden="true" tabindex="-1"></a><span class="co"># to see if it's related to length of article </span></span>
<span id="cb23-509"><a href="#cb23-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-510"><a href="#cb23-510" aria-hidden="true" tabindex="-1"></a><span class="co"># create lists for the lengths of trump articles and the frequencies of word</span></span>
<span id="cb23-511"><a href="#cb23-511" aria-hidden="true" tabindex="-1"></a><span class="co"># "Trump" within the articles</span></span>
<span id="cb23-512"><a href="#cb23-512" aria-hidden="true" tabindex="-1"></a>trump_article_lengths <span class="op">=</span> []</span>
<span id="cb23-513"><a href="#cb23-513" aria-hidden="true" tabindex="-1"></a>trump_word_counts <span class="op">=</span> []</span>
<span id="cb23-514"><a href="#cb23-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-515"><a href="#cb23-515" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-516"><a href="#cb23-516" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> article_docs_unstemmed:</span>
<span id="cb23-517"><a href="#cb23-517" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">8</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb23-518"><a href="#cb23-518" aria-hidden="true" tabindex="-1"></a>        trump_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-519"><a href="#cb23-519" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word <span class="kw">in</span> article.split():</span>
<span id="cb23-520"><a href="#cb23-520" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word <span class="op">==</span> <span class="st">"trump"</span>:</span>
<span id="cb23-521"><a href="#cb23-521" aria-hidden="true" tabindex="-1"></a>                trump_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-522"><a href="#cb23-522" aria-hidden="true" tabindex="-1"></a>        trump_word_counts.append(trump_count)</span>
<span id="cb23-523"><a href="#cb23-523" aria-hidden="true" tabindex="-1"></a>        article_length <span class="op">=</span> <span class="bu">len</span>(article.split())</span>
<span id="cb23-524"><a href="#cb23-524" aria-hidden="true" tabindex="-1"></a>        trump_article_lengths.append(article_length)</span>
<span id="cb23-525"><a href="#cb23-525" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-526"><a href="#cb23-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-527"><a href="#cb23-527" aria-hidden="true" tabindex="-1"></a><span class="co"># let's do the same for the word "biden" in the biden articles and see if there are outliers </span></span>
<span id="cb23-528"><a href="#cb23-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-529"><a href="#cb23-529" aria-hidden="true" tabindex="-1"></a><span class="co"># create lists for the lengths of biden articles and the frequencies of word</span></span>
<span id="cb23-530"><a href="#cb23-530" aria-hidden="true" tabindex="-1"></a><span class="co"># "Biden" within the articles</span></span>
<span id="cb23-531"><a href="#cb23-531" aria-hidden="true" tabindex="-1"></a>biden_article_lengths <span class="op">=</span> []</span>
<span id="cb23-532"><a href="#cb23-532" aria-hidden="true" tabindex="-1"></a>biden_word_counts <span class="op">=</span> []</span>
<span id="cb23-533"><a href="#cb23-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-534"><a href="#cb23-534" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-535"><a href="#cb23-535" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> article_docs_unstemmed:</span>
<span id="cb23-536"><a href="#cb23-536" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> article.split()[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">"bidenarticle"</span>:</span>
<span id="cb23-537"><a href="#cb23-537" aria-hidden="true" tabindex="-1"></a>        biden_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-538"><a href="#cb23-538" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word <span class="kw">in</span> article.split():</span>
<span id="cb23-539"><a href="#cb23-539" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> word <span class="op">==</span> <span class="st">"biden"</span>:</span>
<span id="cb23-540"><a href="#cb23-540" aria-hidden="true" tabindex="-1"></a>                biden_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-541"><a href="#cb23-541" aria-hidden="true" tabindex="-1"></a>        biden_word_counts.append(biden_count)</span>
<span id="cb23-542"><a href="#cb23-542" aria-hidden="true" tabindex="-1"></a>        article_length <span class="op">=</span> <span class="bu">len</span>(article.split())</span>
<span id="cb23-543"><a href="#cb23-543" aria-hidden="true" tabindex="-1"></a>        biden_article_lengths.append(article_length)</span>
<span id="cb23-544"><a href="#cb23-544" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-545"><a href="#cb23-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-546"><a href="#cb23-546" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df with sources, their trump article lengths, and the frequency of the word "trump"</span></span>
<span id="cb23-547"><a href="#cb23-547" aria-hidden="true" tabindex="-1"></a>trump_words <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'source'</span>, <span class="st">'article_length'</span>, <span class="st">'trump_word_count'</span>])</span>
<span id="cb23-548"><a href="#cb23-548" aria-hidden="true" tabindex="-1"></a>trump_words[<span class="st">'source'</span>] <span class="op">=</span> tfidf_source_df.index</span>
<span id="cb23-549"><a href="#cb23-549" aria-hidden="true" tabindex="-1"></a>trump_words[<span class="st">'article_length'</span>] <span class="op">=</span> trump_article_lengths</span>
<span id="cb23-550"><a href="#cb23-550" aria-hidden="true" tabindex="-1"></a>trump_words[<span class="st">'trump_word_count'</span>] <span class="op">=</span> trump_word_counts</span>
<span id="cb23-551"><a href="#cb23-551" aria-hidden="true" tabindex="-1"></a>trump_words</span>
<span id="cb23-552"><a href="#cb23-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-553"><a href="#cb23-553" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df with sources, their biden article lengths, and the frequency of the word "biden"</span></span>
<span id="cb23-554"><a href="#cb23-554" aria-hidden="true" tabindex="-1"></a>biden_words <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'source'</span>, <span class="st">'article_length'</span>, <span class="st">'biden_word_count'</span>])</span>
<span id="cb23-555"><a href="#cb23-555" aria-hidden="true" tabindex="-1"></a>biden_words[<span class="st">'source'</span>] <span class="op">=</span> tfidf_source_df.index</span>
<span id="cb23-556"><a href="#cb23-556" aria-hidden="true" tabindex="-1"></a>biden_words[<span class="st">'article_length'</span>] <span class="op">=</span> biden_article_lengths</span>
<span id="cb23-557"><a href="#cb23-557" aria-hidden="true" tabindex="-1"></a>biden_words[<span class="st">'biden_word_count'</span>] <span class="op">=</span> biden_word_counts</span>
<span id="cb23-558"><a href="#cb23-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-559"><a href="#cb23-559" aria-hidden="true" tabindex="-1"></a><span class="co"># create scatterplot of length of trump article vs frequency of the word "Trump"</span></span>
<span id="cb23-560"><a href="#cb23-560" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb23-561"><a href="#cb23-561" aria-hidden="true" tabindex="-1"></a>ggplot(data<span class="op">=</span>trump_words,</span>
<span id="cb23-562"><a href="#cb23-562" aria-hidden="true" tabindex="-1"></a>       mapping<span class="op">=</span>aes(x<span class="op">=</span><span class="st">'article_length'</span>, y<span class="op">=</span><span class="st">'trump_word_count'</span>, color<span class="op">=</span><span class="st">'source'</span>))</span>
<span id="cb23-563"><a href="#cb23-563" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_point(show_legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-564"><a href="#cb23-564" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_smooth(method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">"darkgrey"</span>)</span>
<span id="cb23-565"><a href="#cb23-565" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> labs(title<span class="op">=</span><span class="st">"Trump Article Length vs. Mentions of Trump"</span>,</span>
<span id="cb23-566"><a href="#cb23-566" aria-hidden="true" tabindex="-1"></a>              x<span class="op">=</span><span class="st">'# of Words in Article'</span>,</span>
<span id="cb23-567"><a href="#cb23-567" aria-hidden="true" tabindex="-1"></a>              y<span class="op">=</span><span class="st">'# of Times "Trump" Mentioned'</span>)</span>
<span id="cb23-568"><a href="#cb23-568" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-569"><a href="#cb23-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-570"><a href="#cb23-570" aria-hidden="true" tabindex="-1"></a><span class="co"># create scatterplot of length of biden article vs frequency of the word "Biden"</span></span>
<span id="cb23-571"><a href="#cb23-571" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb23-572"><a href="#cb23-572" aria-hidden="true" tabindex="-1"></a>ggplot(data<span class="op">=</span>biden_words,</span>
<span id="cb23-573"><a href="#cb23-573" aria-hidden="true" tabindex="-1"></a>       mapping<span class="op">=</span>aes(x<span class="op">=</span><span class="st">'article_length'</span>, y<span class="op">=</span><span class="st">'biden_word_count'</span>, color<span class="op">=</span><span class="st">'source'</span>))</span>
<span id="cb23-574"><a href="#cb23-574" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_point(show_legend<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-575"><a href="#cb23-575" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> geom_smooth(method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">"darkgrey"</span>)</span>
<span id="cb23-576"><a href="#cb23-576" aria-hidden="true" tabindex="-1"></a>       <span class="op">+</span> labs(title<span class="op">=</span><span class="st">"Biden Article Length vs. Mentions of Biden"</span>,</span>
<span id="cb23-577"><a href="#cb23-577" aria-hidden="true" tabindex="-1"></a>              x<span class="op">=</span><span class="st">'# of Words in Article'</span>,</span>
<span id="cb23-578"><a href="#cb23-578" aria-hidden="true" tabindex="-1"></a>              y<span class="op">=</span><span class="st">'# of Times "Biden" Mentioned'</span>)</span>
<span id="cb23-579"><a href="#cb23-579" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-580"><a href="#cb23-580" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-581"><a href="#cb23-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-582"><a href="#cb23-582" aria-hidden="true" tabindex="-1"></a>Above we used scatterplots to see how the length of the Trump articles are related to the frequency of the word "Trump" within them, by source. We do, in fact, see that the point for Fox News, corresponding to the article @foxtrump, is an outlier. Given its article length, it does mention Trump a lot more frequently than the Trump articles from other sources. Out of curiosity, we look at the same visualization for the word "Biden" in the Biden articles. Interestingly, Fox News again appears to be an outlier (see @foxbiden). Given its article length, it also mentions Biden a lot more frequently than the remaining sources. Now, it is not as easy to make assumptions about the partiality of Fox News just based on these word frequencies. They could be more telling of the writing styles of each source, or it could also merely be a result of using such a small sample size of articles.</span>
<span id="cb23-583"><a href="#cb23-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-584"><a href="#cb23-584" aria-hidden="true" tabindex="-1"></a><span class="fu"># Sentiment Analysis</span></span>
<span id="cb23-585"><a href="#cb23-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-586"><a href="#cb23-586" aria-hidden="true" tabindex="-1"></a>Next, the connotation of each article and source overall is studied. The TextBlob package in Python provides polarity and subjectivity scores for any text string. A polarity score, on a scale of <span class="co">[</span><span class="ot">-1, 1</span><span class="co">]</span>, tells if the text has a positive or negative connotation to it. Subjectivity scores, on a scale of <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> tells how opinionated or subjective a text sounds as opposed to being factual. The higher the score, the more subjective the text is. A dataframe is created that displays the polarity and subjectivity scores of each article, with various other columns being created that help place each article's score in the context of the entire corpus of documents and that will be used in following visualizations.</span>
<span id="cb23-587"><a href="#cb23-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-590"><a href="#cb23-590" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-591"><a href="#cb23-591" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate polarity and subjectivity scores, create dataframe</span></span>
<span id="cb23-592"><a href="#cb23-592" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textblob <span class="im">import</span> TextBlob</span>
<span id="cb23-593"><a href="#cb23-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-594"><a href="#cb23-594" aria-hidden="true" tabindex="-1"></a>scores_df <span class="op">=</span> pd.DataFrame({<span class="st">'source'</span>:tfidf_df[<span class="st">'article_source'</span>], <span class="st">'topic'</span>:tfidf_df[<span class="st">'article_topic'</span>]})</span>
<span id="cb23-595"><a href="#cb23-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-596"><a href="#cb23-596" aria-hidden="true" tabindex="-1"></a><span class="co"># shorten some source names to fit on graph later</span></span>
<span id="cb23-597"><a href="#cb23-597" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'source'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'source'</span>] <span class="op">==</span> <span class="st">"The New York Times"</span>, <span class="st">"New York Times"</span>, scores_df[<span class="st">'source'</span>])</span>
<span id="cb23-598"><a href="#cb23-598" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'source'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'source'</span>] <span class="op">==</span> <span class="st">"The Washington Post"</span>, <span class="st">"Washington Post"</span>, scores_df[<span class="st">'source'</span>])</span>
<span id="cb23-599"><a href="#cb23-599" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'source'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'source'</span>] <span class="op">==</span> <span class="st">"The Wall Street Journal"</span>, <span class="st">"Wall Street Journal"</span>, scores_df[<span class="st">'source'</span>])</span>
<span id="cb23-600"><a href="#cb23-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-601"><a href="#cb23-601" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize list of polarity and subjectivity scores</span></span>
<span id="cb23-602"><a href="#cb23-602" aria-hidden="true" tabindex="-1"></a>polarity_scores <span class="op">=</span> []</span>
<span id="cb23-603"><a href="#cb23-603" aria-hidden="true" tabindex="-1"></a>subjectivity_scores <span class="op">=</span> []</span>
<span id="cb23-604"><a href="#cb23-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-605"><a href="#cb23-605" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the scores for each article and add them to the corresponding list of scores</span></span>
<span id="cb23-606"><a href="#cb23-606" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> article_docs_unstemmed:</span>
<span id="cb23-607"><a href="#cb23-607" aria-hidden="true" tabindex="-1"></a>    polarity_scores.append(<span class="bu">round</span>(TextBlob(<span class="st">" "</span></span>
<span id="cb23-608"><a href="#cb23-608" aria-hidden="true" tabindex="-1"></a>    .join(article.split()[:<span class="op">-</span><span class="dv">2</span>]))</span>
<span id="cb23-609"><a href="#cb23-609" aria-hidden="true" tabindex="-1"></a>    .sentiment.polarity, <span class="dv">2</span>))</span>
<span id="cb23-610"><a href="#cb23-610" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-611"><a href="#cb23-611" aria-hidden="true" tabindex="-1"></a>    subjectivity_scores.append(<span class="bu">round</span>(TextBlob(<span class="st">" "</span></span>
<span id="cb23-612"><a href="#cb23-612" aria-hidden="true" tabindex="-1"></a>    .join(article.split()[:<span class="op">-</span><span class="dv">2</span>]))</span>
<span id="cb23-613"><a href="#cb23-613" aria-hidden="true" tabindex="-1"></a>    .sentiment.subjectivity, <span class="dv">2</span>))</span>
<span id="cb23-614"><a href="#cb23-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-615"><a href="#cb23-615" aria-hidden="true" tabindex="-1"></a><span class="co"># add a column in the df for the polarity and subjectivity scores, based on the lists</span></span>
<span id="cb23-616"><a href="#cb23-616" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_score'</span>] <span class="op">=</span> polarity_scores</span>
<span id="cb23-617"><a href="#cb23-617" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'subjectivity_score'</span>] <span class="op">=</span> subjectivity_scores</span>
<span id="cb23-618"><a href="#cb23-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-619"><a href="#cb23-619" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize list of average polarity scores for each topic and source</span></span>
<span id="cb23-620"><a href="#cb23-620" aria-hidden="true" tabindex="-1"></a>average_topic_polarity_scores <span class="op">=</span> []</span>
<span id="cb23-621"><a href="#cb23-621" aria-hidden="true" tabindex="-1"></a>average_source_polarity_scores <span class="op">=</span> []</span>
<span id="cb23-622"><a href="#cb23-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-623"><a href="#cb23-623" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the average polarity scores for each topic, add to that list</span></span>
<span id="cb23-624"><a href="#cb23-624" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> scores_df[<span class="st">'topic'</span>].value_counts().index:</span>
<span id="cb23-625"><a href="#cb23-625" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> topic][<span class="st">'polarity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb23-626"><a href="#cb23-626" aria-hidden="true" tabindex="-1"></a>    average_topic_polarity_scores.append(mean_score)</span>
<span id="cb23-627"><a href="#cb23-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-628"><a href="#cb23-628" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the average polarity scores for each source, add to that list</span></span>
<span id="cb23-629"><a href="#cb23-629" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb23-630"><a href="#cb23-630" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'polarity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb23-631"><a href="#cb23-631" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb23-632"><a href="#cb23-632" aria-hidden="true" tabindex="-1"></a>        average_source_polarity_scores.append(mean_score)</span>
<span id="cb23-633"><a href="#cb23-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-634"><a href="#cb23-634" aria-hidden="true" tabindex="-1"></a><span class="co"># add columns for the two lists above, average polarity scores by topic and source</span></span>
<span id="cb23-635"><a href="#cb23-635" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_polarity_for_topic'</span>] <span class="op">=</span> average_topic_polarity_scores <span class="op">*</span> <span class="dv">9</span></span>
<span id="cb23-636"><a href="#cb23-636" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_polarity_for_source'</span>] <span class="op">=</span> average_source_polarity_scores</span>
<span id="cb23-637"><a href="#cb23-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-638"><a href="#cb23-638" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize lists for average subjectivity scores by topic and source</span></span>
<span id="cb23-639"><a href="#cb23-639" aria-hidden="true" tabindex="-1"></a>average_topic_subjectivity_scores <span class="op">=</span> []</span>
<span id="cb23-640"><a href="#cb23-640" aria-hidden="true" tabindex="-1"></a>average_source_subjectivity_scores <span class="op">=</span> []</span>
<span id="cb23-641"><a href="#cb23-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-642"><a href="#cb23-642" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate average subjectivity score by topic, add to that list</span></span>
<span id="cb23-643"><a href="#cb23-643" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> scores_df[<span class="st">'topic'</span>].value_counts().index:</span>
<span id="cb23-644"><a href="#cb23-644" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> topic][<span class="st">'subjectivity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb23-645"><a href="#cb23-645" aria-hidden="true" tabindex="-1"></a>    average_topic_subjectivity_scores.append(mean_score)</span>
<span id="cb23-646"><a href="#cb23-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-647"><a href="#cb23-647" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate average subjectivity score by source, add to that list</span></span>
<span id="cb23-648"><a href="#cb23-648" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb23-649"><a href="#cb23-649" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> <span class="bu">round</span>(scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'subjectivity_score'</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb23-650"><a href="#cb23-650" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb23-651"><a href="#cb23-651" aria-hidden="true" tabindex="-1"></a>        average_source_subjectivity_scores.append(mean_score)</span>
<span id="cb23-652"><a href="#cb23-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-653"><a href="#cb23-653" aria-hidden="true" tabindex="-1"></a><span class="co"># create columns for the two lists above, average subjectivity scores by topic and source</span></span>
<span id="cb23-654"><a href="#cb23-654" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_subjectivity_for_topic'</span>] <span class="op">=</span> average_topic_subjectivity_scores <span class="op">*</span> <span class="dv">9</span></span>
<span id="cb23-655"><a href="#cb23-655" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_subjectivity_for_source'</span>] <span class="op">=</span> average_source_subjectivity_scores</span>
<span id="cb23-656"><a href="#cb23-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-657"><a href="#cb23-657" aria-hidden="true" tabindex="-1"></a><span class="co"># create various new columns, for each score type's difference from the mean</span></span>
<span id="cb23-658"><a href="#cb23-658" aria-hidden="true" tabindex="-1"></a><span class="co"># for that topic or source</span></span>
<span id="cb23-659"><a href="#cb23-659" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_diff_from_topic_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'polarity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_polarity_for_topic'</span>]</span>
<span id="cb23-660"><a href="#cb23-660" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'subjectivity_diff_from_topic_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'subjectivity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_subjectivity_for_topic'</span>]</span>
<span id="cb23-661"><a href="#cb23-661" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_diff_from_source_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'polarity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_polarity_for_source'</span>]</span>
<span id="cb23-662"><a href="#cb23-662" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'subjectivity_diff_from_source_mean'</span>] <span class="op">=</span> scores_df[<span class="st">'subjectivity_score'</span>] <span class="op">-</span> scores_df[<span class="st">'average_subjectivity_for_source'</span>]</span>
<span id="cb23-663"><a href="#cb23-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-664"><a href="#cb23-664" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate how much on average each source deviates from the average polarity</span></span>
<span id="cb23-665"><a href="#cb23-665" aria-hidden="true" tabindex="-1"></a><span class="co"># or subjectivity score for that topic, create columns for those calculations</span></span>
<span id="cb23-666"><a href="#cb23-666" aria-hidden="true" tabindex="-1"></a>sources_polarity_dev <span class="op">=</span> []</span>
<span id="cb23-667"><a href="#cb23-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-668"><a href="#cb23-668" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb23-669"><a href="#cb23-669" aria-hidden="true" tabindex="-1"></a>    total_dev <span class="op">=</span> scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'polarity_diff_from_topic_mean'</span>].<span class="bu">sum</span>()</span>
<span id="cb23-670"><a href="#cb23-670" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb23-671"><a href="#cb23-671" aria-hidden="true" tabindex="-1"></a>        sources_polarity_dev.append(total_dev<span class="op">/</span><span class="dv">8</span>)</span>
<span id="cb23-672"><a href="#cb23-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-673"><a href="#cb23-673" aria-hidden="true" tabindex="-1"></a>sources_subjectivity_dev <span class="op">=</span> []</span>
<span id="cb23-674"><a href="#cb23-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-675"><a href="#cb23-675" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> source <span class="kw">in</span> scores_df[<span class="st">'source'</span>].value_counts().index:</span>
<span id="cb23-676"><a href="#cb23-676" aria-hidden="true" tabindex="-1"></a>    total_dev <span class="op">=</span> scores_df[scores_df[<span class="st">'source'</span>] <span class="op">==</span> source][<span class="st">'subjectivity_diff_from_topic_mean'</span>].<span class="bu">sum</span>()</span>
<span id="cb23-677"><a href="#cb23-677" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb23-678"><a href="#cb23-678" aria-hidden="true" tabindex="-1"></a>        sources_subjectivity_dev.append(total_dev<span class="op">/</span><span class="dv">8</span>)</span>
<span id="cb23-679"><a href="#cb23-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-680"><a href="#cb23-680" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_source_polarity_deviation_from_topic_mean'</span>] <span class="op">=</span> sources_polarity_dev</span>
<span id="cb23-681"><a href="#cb23-681" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'average_source_subjectivity_deviation_from_topic_mean'</span>] <span class="op">=</span> sources_subjectivity_dev</span>
<span id="cb23-682"><a href="#cb23-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-683"><a href="#cb23-683" aria-hidden="true" tabindex="-1"></a><span class="co"># create column to indicate whether the polarity score is positive or negative,</span></span>
<span id="cb23-684"><a href="#cb23-684" aria-hidden="true" tabindex="-1"></a><span class="co"># to color graphs by later</span></span>
<span id="cb23-685"><a href="#cb23-685" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_sign'</span>] <span class="op">=</span> np.where(scores_df[<span class="st">'polarity_score'</span>] <span class="op">&gt;</span> <span class="dv">0</span>, <span class="st">'pos'</span>, <span class="st">'neg'</span>)</span>
<span id="cb23-686"><a href="#cb23-686" aria-hidden="true" tabindex="-1"></a><span class="co"># create column for absolute value of the polarity score</span></span>
<span id="cb23-687"><a href="#cb23-687" aria-hidden="true" tabindex="-1"></a>scores_df[<span class="st">'polarity_magnitude'</span>] <span class="op">=</span> <span class="bu">abs</span>(scores_df[<span class="st">'polarity_score'</span>])</span>
<span id="cb23-688"><a href="#cb23-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-689"><a href="#cb23-689" aria-hidden="true" tabindex="-1"></a>scores_df.head(<span class="dv">5</span>)</span>
<span id="cb23-690"><a href="#cb23-690" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-691"><a href="#cb23-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-692"><a href="#cb23-692" aria-hidden="true" tabindex="-1"></a>Since the documentation of the calculation of these scores is limited, one may wonder if they are somehow related. That is, are polarity scores related to subjectivity scores in some way? A scatterplot is created to see if a higher polarity score magnitude (absolute value) may mean a higher subjectivity score as well.</span>
<span id="cb23-693"><a href="#cb23-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-696"><a href="#cb23-696" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-697"><a href="#cb23-697" aria-hidden="true" tabindex="-1"></a><span class="co"># to better understand subjectivity scores: are polarity score magnitudes and subjectivity scores correlated?</span></span>
<span id="cb23-698"><a href="#cb23-698" aria-hidden="true" tabindex="-1"></a><span class="co"># this may suggest that polarity score is used in the calculation of a subjectivity score</span></span>
<span id="cb23-699"><a href="#cb23-699" aria-hidden="true" tabindex="-1"></a><span class="co"># colored by topic</span></span>
<span id="cb23-700"><a href="#cb23-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-701"><a href="#cb23-701" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb23-702"><a href="#cb23-702" aria-hidden="true" tabindex="-1"></a>ggplot(scores_df, aes(x<span class="op">=</span><span class="st">"polarity_magnitude"</span>, y<span class="op">=</span><span class="st">"subjectivity_score"</span>, color<span class="op">=</span><span class="st">'topic'</span>))</span>
<span id="cb23-703"><a href="#cb23-703" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_point()</span>
<span id="cb23-704"><a href="#cb23-704" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">"Absolute Value of Polarity Score"</span>,</span>
<span id="cb23-705"><a href="#cb23-705" aria-hidden="true" tabindex="-1"></a>       y<span class="op">=</span><span class="st">'Subjectivity Score'</span>,</span>
<span id="cb23-706"><a href="#cb23-706" aria-hidden="true" tabindex="-1"></a>       title<span class="op">=</span><span class="st">'Are Polarity Score Magnitudes Correlated with Subjectivity Scores?'</span>)</span>
<span id="cb23-707"><a href="#cb23-707" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-708"><a href="#cb23-708" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-709"><a href="#cb23-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-710"><a href="#cb23-710" aria-hidden="true" tabindex="-1"></a>There does not appear to be a linear or any other sort of relationship between the polarity and subjectivity scores of the articles used in this project. However, the graph does show almost a cluster of points of the same color, telling that the articles about the Supreme Court ruling against affirmative action tend to have stronger polarity scores. The same graph may be visualized colored by source instead, to see if any clusters of sources are observed.</span>
<span id="cb23-711"><a href="#cb23-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-712"><a href="#cb23-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-715"><a href="#cb23-715" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-716"><a href="#cb23-716" aria-hidden="true" tabindex="-1"></a><span class="co"># scatterplot to see if polarity magnitudes and subjectivity scores are</span></span>
<span id="cb23-717"><a href="#cb23-717" aria-hidden="true" tabindex="-1"></a><span class="co"># correlated, colored by source</span></span>
<span id="cb23-718"><a href="#cb23-718" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb23-719"><a href="#cb23-719" aria-hidden="true" tabindex="-1"></a>ggplot(scores_df, aes(x<span class="op">=</span><span class="st">"polarity_magnitude"</span>, y<span class="op">=</span><span class="st">"subjectivity_score"</span>, color<span class="op">=</span><span class="st">'source'</span>))</span>
<span id="cb23-720"><a href="#cb23-720" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_point()</span>
<span id="cb23-721"><a href="#cb23-721" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">"Absolute Value of Polarity Score"</span>,</span>
<span id="cb23-722"><a href="#cb23-722" aria-hidden="true" tabindex="-1"></a>       y<span class="op">=</span><span class="st">'Subjectivity Score'</span>,</span>
<span id="cb23-723"><a href="#cb23-723" aria-hidden="true" tabindex="-1"></a>       title<span class="op">=</span><span class="st">'Are Polarity Score Magnitudes Correlated with Subjectivity Scores?'</span>)</span>
<span id="cb23-724"><a href="#cb23-724" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-725"><a href="#cb23-725" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-726"><a href="#cb23-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-727"><a href="#cb23-727" aria-hidden="true" tabindex="-1"></a>Something that can be observed on the scatterplot above is that the green dots, both for CNN and Fox News, tend to be on the lower half of the plot, meaning their articles tend to have lower subjectivity scores. In addition, the dark blue dots, representing The New York Times, tend to be on the upper half, signifying higher subjectivity scores for its articles. </span>
<span id="cb23-728"><a href="#cb23-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-729"><a href="#cb23-729" aria-hidden="true" tabindex="-1"></a>These scores will now be compared across sources in a different, more comprehensive way. It is intuitive that some of the more grim topics, like the deadly Hamas attack, should have a negative polarity score. Other more neutral topics may have positive polarity scores. Taking an average of the polarity scores would not be a good measure since the negative and positive scores could cancel each other out and not give a clear picture of the connotation of each source compared to others. Instead, the average polarity score is computed for each *topic*. Then, the average polarity score for the article's topic is subtracted from the article's polarity score to get its deviation from the mean. For each source, the average of these deviations is computed, which will tell if a source tends to write in a more negative or positive connotation, even given the nature of the topic it is writing about.</span>
<span id="cb23-730"><a href="#cb23-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-733"><a href="#cb23-733" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-734"><a href="#cb23-734" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df for each source and its polarity score total deviations from the topic means</span></span>
<span id="cb23-735"><a href="#cb23-735" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas.api.types <span class="im">import</span> CategoricalDtype</span>
<span id="cb23-736"><a href="#cb23-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-737"><a href="#cb23-737" aria-hidden="true" tabindex="-1"></a>polarity_devs <span class="op">=</span> pd.DataFrame({<span class="st">'source'</span>:scores_df[<span class="st">'source'</span>].value_counts().index, </span>
<span id="cb23-738"><a href="#cb23-738" aria-hidden="true" tabindex="-1"></a>                              <span class="st">'polarity_dev'</span>:scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> <span class="st">'Chinese Surveillance Balloon'</span>]<span class="op">\</span></span>
<span id="cb23-739"><a href="#cb23-739" aria-hidden="true" tabindex="-1"></a>                                [<span class="st">'average_source_polarity_deviation_from_topic_mean'</span>]})</span>
<span id="cb23-740"><a href="#cb23-740" aria-hidden="true" tabindex="-1"></a><span class="co"># create a column denoting if deviation is above or below mean to color graph by later</span></span>
<span id="cb23-741"><a href="#cb23-741" aria-hidden="true" tabindex="-1"></a>polarity_devs[<span class="st">'sign'</span>] <span class="op">=</span> np.where(polarity_devs[<span class="st">'polarity_dev'</span>] <span class="op">&gt;</span> <span class="dv">0</span>, <span class="st">'pos'</span>, <span class="st">'neg'</span>)</span>
<span id="cb23-742"><a href="#cb23-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-743"><a href="#cb23-743" aria-hidden="true" tabindex="-1"></a>polarity_devs <span class="op">=</span> polarity_devs.reindex(polarity_devs[<span class="st">'polarity_dev'</span>].<span class="bu">abs</span>().sort_values(ascending<span class="op">=</span><span class="va">False</span>).index)</span>
<span id="cb23-744"><a href="#cb23-744" aria-hidden="true" tabindex="-1"></a>polarity_devs.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-745"><a href="#cb23-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-746"><a href="#cb23-746" aria-hidden="true" tabindex="-1"></a>source_order <span class="op">=</span> CategoricalDtype(</span>
<span id="cb23-747"><a href="#cb23-747" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"New York Times"</span>, <span class="st">"Wall Street Journal"</span>, <span class="st">"ABC News"</span>, <span class="st">"NBC News"</span>, </span>
<span id="cb23-748"><a href="#cb23-748" aria-hidden="true" tabindex="-1"></a>     <span class="st">"CNN"</span>, <span class="st">"New York Post"</span>, <span class="st">"Washington Post"</span>, <span class="st">"Fox News"</span>, <span class="st">"BBC"</span>], </span>
<span id="cb23-749"><a href="#cb23-749" aria-hidden="true" tabindex="-1"></a>    ordered<span class="op">=</span><span class="va">False</span></span>
<span id="cb23-750"><a href="#cb23-750" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-751"><a href="#cb23-751" aria-hidden="true" tabindex="-1"></a>polarity_devs[<span class="st">'source'</span>] <span class="op">=</span> polarity_devs[<span class="st">'source'</span>].astype(source_order)</span>
<span id="cb23-752"><a href="#cb23-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-753"><a href="#cb23-753" aria-hidden="true" tabindex="-1"></a>polarity_devs</span>
<span id="cb23-754"><a href="#cb23-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-755"><a href="#cb23-755" aria-hidden="true" tabindex="-1"></a><span class="co"># create bar chart representing how much, on average, a source's article</span></span>
<span id="cb23-756"><a href="#cb23-756" aria-hidden="true" tabindex="-1"></a><span class="co"># deviates away from that topic's mean polarity score</span></span>
<span id="cb23-757"><a href="#cb23-757" aria-hidden="true" tabindex="-1"></a><span class="co"># intended to show if a source tends to be more negative/positive than average, </span></span>
<span id="cb23-758"><a href="#cb23-758" aria-hidden="true" tabindex="-1"></a><span class="co"># even accounting for the nature of the topic</span></span>
<span id="cb23-759"><a href="#cb23-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-760"><a href="#cb23-760" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {<span class="st">"pos"</span>:<span class="st">'#ffbf00'</span>, <span class="st">"neg"</span>:<span class="st">'#b7141c'</span>}  </span>
<span id="cb23-761"><a href="#cb23-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-762"><a href="#cb23-762" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb23-763"><a href="#cb23-763" aria-hidden="true" tabindex="-1"></a>ggplot(polarity_devs, aes(x<span class="op">=</span><span class="st">"source"</span>, y<span class="op">=</span><span class="st">"polarity_dev"</span>, fill<span class="op">=</span><span class="st">"sign"</span>))</span>
<span id="cb23-764"><a href="#cb23-764" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_col(stat<span class="op">=</span><span class="st">"identity"</span>)</span>
<span id="cb23-765"><a href="#cb23-765" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">"Source"</span>, y<span class="op">=</span><span class="st">'Average Article Polarity Score Deviation'</span>,</span>
<span id="cb23-766"><a href="#cb23-766" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">'Average Source Polarity Score Deviations from Mean'</span>)</span>
<span id="cb23-767"><a href="#cb23-767" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> theme(figure_size <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">4</span>), axis_text_x<span class="op">=</span>element_text(size<span class="op">=</span><span class="dv">7</span>, face<span class="op">=</span><span class="st">'bold'</span>), </span>
<span id="cb23-768"><a href="#cb23-768" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> element_text(size<span class="op">=</span><span class="dv">13</span>),legend_position <span class="op">=</span> <span class="st">"none"</span>)</span>
<span id="cb23-769"><a href="#cb23-769" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> scale_fill_manual(values <span class="op">=</span> colors)</span>
<span id="cb23-770"><a href="#cb23-770" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-771"><a href="#cb23-771" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-772"><a href="#cb23-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-773"><a href="#cb23-773" aria-hidden="true" tabindex="-1"></a>The visualization above tells us that The New York Times articles tend to be the most negative on average. They have the largest deviations from the topic's average polarity scores, which may indicate stronger wording. Each individual may infer what they wish with these results, but definitive conclusions should not be drawn from them, which will be explained a bit later. ABC News tends to write their articles more positive than average for the topics' mean polarity scores. BBC's articles are the closest to average polarity scores for each topic. The polarity scores may be viewed individually for the set of articles of any topic. Below, we see the polarity scores for the articles about the ruling against affirmative action. The New York Times was the only source with a negative polarity score for this topic, which could already be suggestive of its political views. </span>
<span id="cb23-774"><a href="#cb23-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-777"><a href="#cb23-777" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-778"><a href="#cb23-778" aria-hidden="true" tabindex="-1"></a><span class="co"># create bar chart of polarity scores of articles about a specific topic</span></span>
<span id="cb23-779"><a href="#cb23-779" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb23-780"><a href="#cb23-780" aria-hidden="true" tabindex="-1"></a>ggplot(scores_df[scores_df[<span class="st">'topic'</span>]<span class="op">==</span><span class="st">"Supreme Court Ruling on Affirmative Action"</span>],</span>
<span id="cb23-781"><a href="#cb23-781" aria-hidden="true" tabindex="-1"></a>       aes(x<span class="op">=</span><span class="st">"reorder(source, -polarity_score)"</span>, y<span class="op">=</span><span class="st">"polarity_score"</span>, fill<span class="op">=</span><span class="st">"polarity_sign"</span>))</span>
<span id="cb23-782"><a href="#cb23-782" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_col(stat<span class="op">=</span><span class="st">"identity"</span>)</span>
<span id="cb23-783"><a href="#cb23-783" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">'Source'</span>, y<span class="op">=</span><span class="st">'Polarity Score'</span>, title<span class="op">=</span><span class="st">'Polarity Scores for Ban on Affirmative Action Articles'</span>)</span>
<span id="cb23-784"><a href="#cb23-784" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> theme(figure_size <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">4</span>), axis_text_x<span class="op">=</span>element_text(size<span class="op">=</span><span class="fl">8.5</span>, face<span class="op">=</span><span class="st">'bold'</span>), </span>
<span id="cb23-785"><a href="#cb23-785" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> element_text(size<span class="op">=</span><span class="dv">13</span>), legend_position <span class="op">=</span> <span class="st">"none"</span>)</span>
<span id="cb23-786"><a href="#cb23-786" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> scale_fill_manual(values <span class="op">=</span> colors)</span>
<span id="cb23-787"><a href="#cb23-787" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-788"><a href="#cb23-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-789"><a href="#cb23-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-790"><a href="#cb23-790" aria-hidden="true" tabindex="-1"></a>Now let's look at a comparison of each source's average subjectivity scores. These do not need to be broken up by topic, since the scale for the score is <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>.</span>
<span id="cb23-791"><a href="#cb23-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-794"><a href="#cb23-794" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-795"><a href="#cb23-795" aria-hidden="true" tabindex="-1"></a><span class="co"># create a df with average subjectivity scores per source</span></span>
<span id="cb23-796"><a href="#cb23-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-797"><a href="#cb23-797" aria-hidden="true" tabindex="-1"></a>source_avg_subj <span class="op">=</span> pd.DataFrame({<span class="st">'source'</span>:scores_df[<span class="st">'source'</span>].value_counts().index, </span>
<span id="cb23-798"><a href="#cb23-798" aria-hidden="true" tabindex="-1"></a>                              <span class="st">'avg_subj'</span>:scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> <span class="st">'Chinese Surveillance Balloon'</span>]<span class="op">\</span></span>
<span id="cb23-799"><a href="#cb23-799" aria-hidden="true" tabindex="-1"></a>                                [<span class="st">'average_subjectivity_for_source'</span>]})</span>
<span id="cb23-800"><a href="#cb23-800" aria-hidden="true" tabindex="-1"></a>source_avg_subj.sort_values(by<span class="op">=</span><span class="st">'avg_subj'</span>, ascending<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-801"><a href="#cb23-801" aria-hidden="true" tabindex="-1"></a>source_avg_subj.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-802"><a href="#cb23-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-803"><a href="#cb23-803" aria-hidden="true" tabindex="-1"></a><span class="co"># order in ascending order of subjectivity</span></span>
<span id="cb23-804"><a href="#cb23-804" aria-hidden="true" tabindex="-1"></a>source_order <span class="op">=</span> CategoricalDtype(</span>
<span id="cb23-805"><a href="#cb23-805" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Fox News"</span>, <span class="st">"Wall Street Journal"</span>, <span class="st">"CNN"</span>, </span>
<span id="cb23-806"><a href="#cb23-806" aria-hidden="true" tabindex="-1"></a>     <span class="st">"Washington Post"</span>, <span class="st">"BBC"</span>, <span class="st">"New York Post"</span>, <span class="st">"ABC News"</span>, <span class="st">"NBC News"</span>, <span class="st">"New York Times"</span>], </span>
<span id="cb23-807"><a href="#cb23-807" aria-hidden="true" tabindex="-1"></a>    ordered<span class="op">=</span><span class="va">False</span></span>
<span id="cb23-808"><a href="#cb23-808" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-809"><a href="#cb23-809" aria-hidden="true" tabindex="-1"></a>source_avg_subj[<span class="st">'source'</span>] <span class="op">=</span> source_avg_subj[<span class="st">'source'</span>].astype(source_order)</span>
<span id="cb23-810"><a href="#cb23-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-811"><a href="#cb23-811" aria-hidden="true" tabindex="-1"></a>source_avg_subj</span>
<span id="cb23-812"><a href="#cb23-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-813"><a href="#cb23-813" aria-hidden="true" tabindex="-1"></a><span class="co"># bar chart showing avg subjectivity per source</span></span>
<span id="cb23-814"><a href="#cb23-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-815"><a href="#cb23-815" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb23-816"><a href="#cb23-816" aria-hidden="true" tabindex="-1"></a>ggplot(source_avg_subj, aes(x<span class="op">=</span><span class="st">"source"</span>, y<span class="op">=</span><span class="st">"avg_subj"</span>))</span>
<span id="cb23-817"><a href="#cb23-817" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> geom_col(stat<span class="op">=</span><span class="st">"identity"</span>, fill<span class="op">=</span><span class="st">"#ffa140"</span>)</span>
<span id="cb23-818"><a href="#cb23-818" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> labs(x<span class="op">=</span><span class="st">'Source'</span>, y<span class="op">=</span><span class="st">'Average Subjectivity Score'</span>, title<span class="op">=</span><span class="st">'Average Subjectivity Score by Source'</span>)</span>
<span id="cb23-819"><a href="#cb23-819" aria-hidden="true" tabindex="-1"></a><span class="op">+</span> theme(figure_size <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">4</span>), axis_text_x<span class="op">=</span>element_text(size<span class="op">=</span><span class="fl">8.5</span>, face<span class="op">=</span><span class="st">'bold'</span>), </span>
<span id="cb23-820"><a href="#cb23-820" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> element_text(size<span class="op">=</span><span class="dv">13</span>), legend_position <span class="op">=</span> <span class="st">"none"</span>)</span>
<span id="cb23-821"><a href="#cb23-821" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-822"><a href="#cb23-822" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-823"><a href="#cb23-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-824"><a href="#cb23-824" aria-hidden="true" tabindex="-1"></a>The bar chart above shows that Fox News has the lowest subjectivity scores on average, while the New York Times has the highest subjectivity score on average. Surprised? Do these results align with your opinions or what you commonly hear about these sources? Regardless, it is important to remember what these scores mean exactly and what we are able to reasonably infer from these results. The package that computes these scores has limited documentation, so the exact formulas are not known for the computation of a polarity or subjectivity score of a text. Each individual word has a scoring of its own and is marked as positive or negative, and the overall polarity score is some sort of aggregation of these scores. This does limit us in the interpretation of the scores. Moreover, this is only a small sample of articles collected from each source and their insights should not represent a source as a whole. It is entirely possible that a larger sample of articles from a wider variety of topics could yield different results.</span>
<span id="cb23-825"><a href="#cb23-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-826"><a href="#cb23-826" aria-hidden="true" tabindex="-1"></a>To further analyze the polarity scores, we may want to know which ones are outliers. So, for each topic, the interquartile range formula is used to find any articles with an outlier score. The articles and their polarity scores that were identified as outliers are shown in the table below. </span>
<span id="cb23-827"><a href="#cb23-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-830"><a href="#cb23-830" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-831"><a href="#cb23-831" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of tuples that contain articles that have an outlier polarity score,</span></span>
<span id="cb23-832"><a href="#cb23-832" aria-hidden="true" tabindex="-1"></a><span class="co"># compared to other articles about the same topic</span></span>
<span id="cb23-833"><a href="#cb23-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-834"><a href="#cb23-834" aria-hidden="true" tabindex="-1"></a>articles <span class="op">=</span>[]</span>
<span id="cb23-835"><a href="#cb23-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-836"><a href="#cb23-836" aria-hidden="true" tabindex="-1"></a><span class="co"># check for outliers topic by topic</span></span>
<span id="cb23-837"><a href="#cb23-837" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> scores_df[<span class="st">'topic'</span>].value_counts().index:</span>
<span id="cb23-838"><a href="#cb23-838" aria-hidden="true" tabindex="-1"></a>    topic_df <span class="op">=</span> scores_df[scores_df[<span class="st">'topic'</span>] <span class="op">==</span> topic]</span>
<span id="cb23-839"><a href="#cb23-839" aria-hidden="true" tabindex="-1"></a>    average_polarity <span class="op">=</span> topic_df[<span class="st">'average_polarity_for_topic'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb23-840"><a href="#cb23-840" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use IQR formula to detect outliers</span></span>
<span id="cb23-841"><a href="#cb23-841" aria-hidden="true" tabindex="-1"></a>    Q1 <span class="op">=</span> topic_df[<span class="st">'polarity_score'</span>].describe()[<span class="dv">4</span>]</span>
<span id="cb23-842"><a href="#cb23-842" aria-hidden="true" tabindex="-1"></a>    Q3 <span class="op">=</span> topic_df[<span class="st">'polarity_score'</span>].describe()[<span class="dv">6</span>]</span>
<span id="cb23-843"><a href="#cb23-843" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> Q3<span class="op">-</span>Q1</span>
<span id="cb23-844"><a href="#cb23-844" aria-hidden="true" tabindex="-1"></a>    lower <span class="op">=</span> Q1<span class="op">-</span>(<span class="fl">1.5</span><span class="op">*</span>IQR)</span>
<span id="cb23-845"><a href="#cb23-845" aria-hidden="true" tabindex="-1"></a>    upper <span class="op">=</span> Q3<span class="op">+</span>(<span class="fl">1.5</span><span class="op">*</span>IQR)</span>
<span id="cb23-846"><a href="#cb23-846" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the line below would print the lower and upper bounds for non-outliers</span></span>
<span id="cb23-847"><a href="#cb23-847" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   for each topic</span></span>
<span id="cb23-848"><a href="#cb23-848" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(topic, "lower bound:", round(lower, 2), "upper bound:", round(upper,2))</span></span>
<span id="cb23-849"><a href="#cb23-849" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check if an article has an outlier polarity score (beyond lower/upper limits)</span></span>
<span id="cb23-850"><a href="#cb23-850" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> topic_df.itertuples():</span>
<span id="cb23-851"><a href="#cb23-851" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> row.polarity_score <span class="op">&lt;</span> lower <span class="kw">or</span> row.polarity_score <span class="op">&gt;</span> upper:</span>
<span id="cb23-852"><a href="#cb23-852" aria-hidden="true" tabindex="-1"></a>            articles.append((row.source, row.topic, row.polarity_score, average_polarity))</span>
<span id="cb23-853"><a href="#cb23-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-854"><a href="#cb23-854" aria-hidden="true" tabindex="-1"></a><span class="co"># create lists about outlier article information to create dataframe for them</span></span>
<span id="cb23-855"><a href="#cb23-855" aria-hidden="true" tabindex="-1"></a>outlier_source_list <span class="op">=</span> []</span>
<span id="cb23-856"><a href="#cb23-856" aria-hidden="true" tabindex="-1"></a>outlier_topic_list <span class="op">=</span> []</span>
<span id="cb23-857"><a href="#cb23-857" aria-hidden="true" tabindex="-1"></a>outlier_polarity_list <span class="op">=</span> []</span>
<span id="cb23-858"><a href="#cb23-858" aria-hidden="true" tabindex="-1"></a>average_polarity_list <span class="op">=</span> []</span>
<span id="cb23-859"><a href="#cb23-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-860"><a href="#cb23-860" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> article <span class="kw">in</span> articles:</span>
<span id="cb23-861"><a href="#cb23-861" aria-hidden="true" tabindex="-1"></a>  outlier_source_list.append(article[<span class="dv">0</span>])</span>
<span id="cb23-862"><a href="#cb23-862" aria-hidden="true" tabindex="-1"></a>  outlier_topic_list.append(article[<span class="dv">1</span>])</span>
<span id="cb23-863"><a href="#cb23-863" aria-hidden="true" tabindex="-1"></a>  outlier_polarity_list.append(article[<span class="dv">2</span>])</span>
<span id="cb23-864"><a href="#cb23-864" aria-hidden="true" tabindex="-1"></a>  average_polarity_list.append(article[<span class="dv">3</span>])</span>
<span id="cb23-865"><a href="#cb23-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-866"><a href="#cb23-866" aria-hidden="true" tabindex="-1"></a><span class="co"># display in dataframe the articles with an outlier polarity score</span></span>
<span id="cb23-867"><a href="#cb23-867" aria-hidden="true" tabindex="-1"></a>outlier_df <span class="op">=</span> pd.DataFrame({<span class="st">'Source'</span>:outlier_source_list,</span>
<span id="cb23-868"><a href="#cb23-868" aria-hidden="true" tabindex="-1"></a>                           <span class="st">'Topic'</span>:outlier_topic_list,</span>
<span id="cb23-869"><a href="#cb23-869" aria-hidden="true" tabindex="-1"></a>                           <span class="st">'Polarity Score'</span>:outlier_polarity_list,</span>
<span id="cb23-870"><a href="#cb23-870" aria-hidden="true" tabindex="-1"></a>                           <span class="st">'Average Polarity for Topic'</span>:average_polarity_list})</span>
<span id="cb23-871"><a href="#cb23-871" aria-hidden="true" tabindex="-1"></a>display(outlier_df)</span>
<span id="cb23-872"><a href="#cb23-872" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-873"><a href="#cb23-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-874"><a href="#cb23-874" aria-hidden="true" tabindex="-1"></a>From looking at outlier polarity scores, the articles about the Supreme Court ruling against affirmative action have several outlier scores, both positive and negative. This could be telling of the major difference in opinions regarding this topic. As for sources, The Wall Street Journal had the most amount of articles with an outlier polarity score.</span>
<span id="cb23-875"><a href="#cb23-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-876"><a href="#cb23-876" aria-hidden="true" tabindex="-1"></a><span class="fu"># Predictive Modeling</span></span>
<span id="cb23-877"><a href="#cb23-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-878"><a href="#cb23-878" aria-hidden="true" tabindex="-1"></a><span class="fu">## Naive Bayes</span></span>
<span id="cb23-879"><a href="#cb23-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-880"><a href="#cb23-880" aria-hidden="true" tabindex="-1"></a>Now, for the main part of this project, a few different classifiers will be used to attempt to predict the source of a given article. If this can be done with high accuracy, it will show that the wording alone of the studied sources is distinct enough to be an identifying factor. First, Naive Bayes classifiers will be trained and tested on the articles included. Six topics will be used for training, and two for testing. As many classifiers will be created as needed (28) in order to use every possible split of topics for the training and testing sets. In addition, this process will be repeated for each of the three dataframes created (using binary values, frequencies, and TF-IDF scores of tokens) for a total of 84 Naive Bayes classifiers. This means that the models will be trained on the tokens (words) that appear in each model and their values (one of the three possible mentioned). The average accuracy scores of Naive Bayes models trained on any of the three value types are displayed in the table below. <span class="co">[</span><span class="ot">Data Camp</span><span class="co">](https://www.datacamp.com/tutorial/naive-bayes-scikit-learn)</span> was used as a source to code the Naive Bayes model, including fitting the model and making predictions (see @nb_confusion).</span>
<span id="cb23-881"><a href="#cb23-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-884"><a href="#cb23-884" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-885"><a href="#cb23-885" aria-hidden="true" tabindex="-1"></a><span class="co"># import libraries</span></span>
<span id="cb23-886"><a href="#cb23-886" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb23-887"><a href="#cb23-887" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics  </span>
<span id="cb23-888"><a href="#cb23-888" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score</span>
<span id="cb23-889"><a href="#cb23-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-890"><a href="#cb23-890" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a dictionary that will show the best accuracy for each kind of classifier used</span></span>
<span id="cb23-891"><a href="#cb23-891" aria-hidden="true" tabindex="-1"></a>best_accuracies_by_cf <span class="op">=</span> {}</span>
<span id="cb23-892"><a href="#cb23-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-893"><a href="#cb23-893" aria-hidden="true" tabindex="-1"></a><span class="co"># define list of topics to later choose training/testing sets with</span></span>
<span id="cb23-894"><a href="#cb23-894" aria-hidden="true" tabindex="-1"></a>topic_list <span class="op">=</span> topic_choices  <span class="op">=</span> [</span>
<span id="cb23-895"><a href="#cb23-895" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Supreme Court Ruling on Affirmative Action"</span></span>
<span id="cb23-896"><a href="#cb23-896" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Chinese Surveillance Balloon"</span></span>
<span id="cb23-897"><a href="#cb23-897" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Biden's Low Approval Rates in Polls"</span></span>
<span id="cb23-898"><a href="#cb23-898" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"The Deadliest Attack by Hamas"</span></span>
<span id="cb23-899"><a href="#cb23-899" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Pentagon Documents Leak"</span></span>
<span id="cb23-900"><a href="#cb23-900" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"George Santos' Expulsion from Congress"</span></span>
<span id="cb23-901"><a href="#cb23-901" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"U.S. and Germany Send Tanks to Ukraine"</span></span>
<span id="cb23-902"><a href="#cb23-902" aria-hidden="true" tabindex="-1"></a>    , <span class="st">"Trump's Indictment"</span></span>
<span id="cb23-903"><a href="#cb23-903" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-904"><a href="#cb23-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-905"><a href="#cb23-905" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize lists that will later be used to create a df showing the</span></span>
<span id="cb23-906"><a href="#cb23-906" aria-hidden="true" tabindex="-1"></a><span class="co"># average model accuracy for each type of value the models were trained on</span></span>
<span id="cb23-907"><a href="#cb23-907" aria-hidden="true" tabindex="-1"></a>average_accuracies <span class="op">=</span> []</span>
<span id="cb23-908"><a href="#cb23-908" aria-hidden="true" tabindex="-1"></a>value_types <span class="op">=</span> []</span>
<span id="cb23-909"><a href="#cb23-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-910"><a href="#cb23-910" aria-hidden="true" tabindex="-1"></a><span class="co"># function to apply Naive Bayes model using different holdout article topics each time</span></span>
<span id="cb23-911"><a href="#cb23-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-912"><a href="#cb23-912" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> naive_bayes_cf(df, value_type):</span>
<span id="cb23-913"><a href="#cb23-913" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Takes in a dataframe of tokens and the desired values </span></span>
<span id="cb23-914"><a href="#cb23-914" aria-hidden="true" tabindex="-1"></a><span class="co">    to train the model on (i.e. tfidf scores, frequencies)</span></span>
<span id="cb23-915"><a href="#cb23-915" aria-hidden="true" tabindex="-1"></a><span class="co">    and a string representing what the values are.</span></span>
<span id="cb23-916"><a href="#cb23-916" aria-hidden="true" tabindex="-1"></a><span class="co">    Trains, tests, and outputs performance metrics of a series</span></span>
<span id="cb23-917"><a href="#cb23-917" aria-hidden="true" tabindex="-1"></a><span class="co">    of Multinomial Naive Bayes classifiers, one for each</span></span>
<span id="cb23-918"><a href="#cb23-918" aria-hidden="true" tabindex="-1"></a><span class="co">    possible holdout set of two article topics.'''</span></span>
<span id="cb23-919"><a href="#cb23-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-920"><a href="#cb23-920" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize lists where each model's accuracy/f1 score will be appended</span></span>
<span id="cb23-921"><a href="#cb23-921" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to later be able to determine the min/max of each</span></span>
<span id="cb23-922"><a href="#cb23-922" aria-hidden="true" tabindex="-1"></a>    accuracy_list <span class="op">=</span> []</span>
<span id="cb23-923"><a href="#cb23-923" aria-hidden="true" tabindex="-1"></a>    f1_score_list <span class="op">=</span> []</span>
<span id="cb23-924"><a href="#cb23-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-925"><a href="#cb23-925" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loops to create a model with each combination of a testing set</span></span>
<span id="cb23-926"><a href="#cb23-926" aria-hidden="true" tabindex="-1"></a>    <span class="co"># consisting of two topics</span></span>
<span id="cb23-927"><a href="#cb23-927" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(topic_list)):</span>
<span id="cb23-928"><a href="#cb23-928" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="bu">len</span>(topic_list)):</span>
<span id="cb23-929"><a href="#cb23-929" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below to print the holdout topics for the model</span></span>
<span id="cb23-930"><a href="#cb23-930" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("Holdout article topics: ")</span></span>
<span id="cb23-931"><a href="#cb23-931" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(topic_list[i])</span></span>
<span id="cb23-932"><a href="#cb23-932" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(topic_list[j])</span></span>
<span id="cb23-933"><a href="#cb23-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-934"><a href="#cb23-934" aria-hidden="true" tabindex="-1"></a>            <span class="co"># split into training/testing df's for predictor and target variables</span></span>
<span id="cb23-935"><a href="#cb23-935" aria-hidden="true" tabindex="-1"></a>            X_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb23-936"><a href="#cb23-936" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb23-937"><a href="#cb23-937" aria-hidden="true" tabindex="-1"></a>            X_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb23-938"><a href="#cb23-938" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb23-939"><a href="#cb23-939" aria-hidden="true" tabindex="-1"></a>            y_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb23-940"><a href="#cb23-940" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb23-941"><a href="#cb23-941" aria-hidden="true" tabindex="-1"></a>            y_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb23-942"><a href="#cb23-942" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb23-943"><a href="#cb23-943" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-944"><a href="#cb23-944" aria-hidden="true" tabindex="-1"></a>            <span class="co"># transform the training/testing df's to arrays</span></span>
<span id="cb23-945"><a href="#cb23-945" aria-hidden="true" tabindex="-1"></a>            X_train <span class="op">=</span> X_train_df.to_numpy()</span>
<span id="cb23-946"><a href="#cb23-946" aria-hidden="true" tabindex="-1"></a>            X_test <span class="op">=</span> X_test_df.to_numpy()</span>
<span id="cb23-947"><a href="#cb23-947" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> y_train_df.to_numpy()</span>
<span id="cb23-948"><a href="#cb23-948" aria-hidden="true" tabindex="-1"></a>            y_test <span class="op">=</span> y_test_df.to_numpy()</span>
<span id="cb23-949"><a href="#cb23-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-950"><a href="#cb23-950" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create classifier object and train it on our training data</span></span>
<span id="cb23-951"><a href="#cb23-951" aria-hidden="true" tabindex="-1"></a>            nb_model <span class="op">=</span> MultinomialNB()</span>
<span id="cb23-952"><a href="#cb23-952" aria-hidden="true" tabindex="-1"></a>            nb_model.fit(X_train, y_train)</span>
<span id="cb23-953"><a href="#cb23-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-954"><a href="#cb23-954" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make predictions on our testing set, output as an array</span></span>
<span id="cb23-955"><a href="#cb23-955" aria-hidden="true" tabindex="-1"></a>            predictions_array <span class="op">=</span> nb_model.predict(X_test)</span>
<span id="cb23-956"><a href="#cb23-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-957"><a href="#cb23-957" aria-hidden="true" tabindex="-1"></a>            <span class="co"># turn the array with predicted sources into a list</span></span>
<span id="cb23-958"><a href="#cb23-958" aria-hidden="true" tabindex="-1"></a>            prediction_list <span class="op">=</span> []</span>
<span id="cb23-959"><a href="#cb23-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-960"><a href="#cb23-960" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> pred <span class="kw">in</span> predictions_array:</span>
<span id="cb23-961"><a href="#cb23-961" aria-hidden="true" tabindex="-1"></a>                prediction_list.append(pred)</span>
<span id="cb23-962"><a href="#cb23-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-963"><a href="#cb23-963" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a list of the true sources</span></span>
<span id="cb23-964"><a href="#cb23-964" aria-hidden="true" tabindex="-1"></a>            actuals_list <span class="op">=</span> []</span>
<span id="cb23-965"><a href="#cb23-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-966"><a href="#cb23-966" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> source <span class="kw">in</span> y_test:</span>
<span id="cb23-967"><a href="#cb23-967" aria-hidden="true" tabindex="-1"></a>                actuals_list.append(source)</span>
<span id="cb23-968"><a href="#cb23-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-969"><a href="#cb23-969" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a dataframe from the two lists created above to show the actual vs predicted sources</span></span>
<span id="cb23-970"><a href="#cb23-970" aria-hidden="true" tabindex="-1"></a>            preds_actuals_df <span class="op">=</span> pd.DataFrame({<span class="st">'Actual'</span>:actuals_list, <span class="st">'Predicted'</span>:prediction_list})</span>
<span id="cb23-971"><a href="#cb23-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-972"><a href="#cb23-972" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment line below if want to see the dataframe of actual vs predicted sources for each model</span></span>
<span id="cb23-973"><a href="#cb23-973" aria-hidden="true" tabindex="-1"></a>            <span class="co"># display(preds_actuals_df)</span></span>
<span id="cb23-974"><a href="#cb23-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-975"><a href="#cb23-975" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate performance metrics</span></span>
<span id="cb23-976"><a href="#cb23-976" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> <span class="bu">round</span>(accuracy_score(predictions_array, y_test), <span class="dv">2</span>)</span>
<span id="cb23-977"><a href="#cb23-977" aria-hidden="true" tabindex="-1"></a>            f1 <span class="op">=</span> <span class="bu">round</span>(f1_score(predictions_array, y_test, average<span class="op">=</span><span class="st">"weighted"</span>), <span class="dv">2</span>)</span>
<span id="cb23-978"><a href="#cb23-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-979"><a href="#cb23-979" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the model's accuracy/f1 score to the list of accuracies/f1 scores</span></span>
<span id="cb23-980"><a href="#cb23-980" aria-hidden="true" tabindex="-1"></a>            accuracy_list.append(accuracy)</span>
<span id="cb23-981"><a href="#cb23-981" aria-hidden="true" tabindex="-1"></a>            f1_score_list.append(f1)</span>
<span id="cb23-982"><a href="#cb23-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-983"><a href="#cb23-983" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if this model's accuracy is better than the current best accuracy</span></span>
<span id="cb23-984"><a href="#cb23-984" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for Naive Bayes, and if it is, then set its accuracy to be the</span></span>
<span id="cb23-985"><a href="#cb23-985" aria-hidden="true" tabindex="-1"></a>            <span class="co"># value for Naive Bayes in the dictionary initialized earlier</span></span>
<span id="cb23-986"><a href="#cb23-986" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb23-987"><a href="#cb23-987" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> accuracy <span class="op">&gt;</span> best_accuracies_by_cf[<span class="st">'Naive Bayes'</span>]:</span>
<span id="cb23-988"><a href="#cb23-988" aria-hidden="true" tabindex="-1"></a>                    best_accuracies_by_cf[<span class="st">'Naive Bayes'</span>] <span class="op">=</span> accuracy</span>
<span id="cb23-989"><a href="#cb23-989" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span>:</span>
<span id="cb23-990"><a href="#cb23-990" aria-hidden="true" tabindex="-1"></a>                best_accuracies_by_cf[<span class="st">'Naive Bayes'</span>] <span class="op">=</span> accuracy</span>
<span id="cb23-991"><a href="#cb23-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-992"><a href="#cb23-992" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below to display results of every model</span></span>
<span id="cb23-993"><a href="#cb23-993" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("Accuracy of Multinomial Naive Bayes Model:", accuracy)</span></span>
<span id="cb23-994"><a href="#cb23-994" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("F1 score of Multinomial Naive Bayes Model:", f1)</span></span>
<span id="cb23-995"><a href="#cb23-995" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print("\n\n")</span></span>
<span id="cb23-996"><a href="#cb23-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-997"><a href="#cb23-997" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize a cumulative sum of accuracies for models trained</span></span>
<span id="cb23-998"><a href="#cb23-998" aria-hidden="true" tabindex="-1"></a>    <span class="co"># on the dataframe/value types defined in the function</span></span>
<span id="cb23-999"><a href="#cb23-999" aria-hidden="true" tabindex="-1"></a>    cum_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-1000"><a href="#cb23-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1001"><a href="#cb23-1001" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add each model's accuracy to the cumulative sum</span></span>
<span id="cb23-1002"><a href="#cb23-1002" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> acc <span class="kw">in</span> accuracy_list:</span>
<span id="cb23-1003"><a href="#cb23-1003" aria-hidden="true" tabindex="-1"></a>        cum_acc <span class="op">+=</span> acc</span>
<span id="cb23-1004"><a href="#cb23-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1005"><a href="#cb23-1005" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the average accuracy for a model trained on this</span></span>
<span id="cb23-1006"><a href="#cb23-1006" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dataframe/value type and add it to the list of average accuracies</span></span>
<span id="cb23-1007"><a href="#cb23-1007" aria-hidden="true" tabindex="-1"></a>    average_accuracies.append(<span class="bu">round</span>(cum_acc<span class="op">/</span><span class="bu">len</span>(accuracy_list), <span class="dv">2</span>))</span>
<span id="cb23-1008"><a href="#cb23-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1009"><a href="#cb23-1009" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add the value type used to train these models to the list of value</span></span>
<span id="cb23-1010"><a href="#cb23-1010" aria-hidden="true" tabindex="-1"></a>    <span class="co"># types, which will be used to create a df of average accuracies by value type</span></span>
<span id="cb23-1011"><a href="#cb23-1011" aria-hidden="true" tabindex="-1"></a>    value_types.append(value_type)</span>
<span id="cb23-1012"><a href="#cb23-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1013"><a href="#cb23-1013" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uncomment lines below if interested in displaying min/max</span></span>
<span id="cb23-1014"><a href="#cb23-1014" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   accuracy scores for the models trained within the function</span></span>
<span id="cb23-1015"><a href="#cb23-1015" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"Minimum Accuracy Score of any Multinomial Naive Bayes Model using {value_type}:",</span></span>
<span id="cb23-1016"><a href="#cb23-1016" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    min(accuracy_list))</span></span>
<span id="cb23-1017"><a href="#cb23-1017" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"Maximum Accuracy Score of any Multinomial Naive Bayes Model using {value_type}:",</span></span>
<span id="cb23-1018"><a href="#cb23-1018" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    max(accuracy_list))</span></span>
<span id="cb23-1019"><a href="#cb23-1019" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(f"Maximum F1 Score of any Multinomial Naive Bayes Model using {value_type}:",</span></span>
<span id="cb23-1020"><a href="#cb23-1020" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    max(f1_score_list))</span></span>
<span id="cb23-1021"><a href="#cb23-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1022"><a href="#cb23-1022" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of all the dataframes/value types we want to train our classifiers on</span></span>
<span id="cb23-1023"><a href="#cb23-1023" aria-hidden="true" tabindex="-1"></a>dfs <span class="op">=</span> [(tfidf_df, <span class="st">"TFIDF Score"</span>), (binary_df, <span class="st">"Binary Values"</span>), (freq_df, <span class="st">"Frequencies"</span>)]</span>
<span id="cb23-1024"><a href="#cb23-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1025"><a href="#cb23-1025" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the list above to create many models</span></span>
<span id="cb23-1026"><a href="#cb23-1026" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df <span class="kw">in</span> dfs:</span>
<span id="cb23-1027"><a href="#cb23-1027" aria-hidden="true" tabindex="-1"></a>    naive_bayes_cf(df[<span class="dv">0</span>], df[<span class="dv">1</span>])</span>
<span id="cb23-1028"><a href="#cb23-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1029"><a href="#cb23-1029" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe showing each value type Naive Bayes models were trained on and the average accuracies for each</span></span>
<span id="cb23-1030"><a href="#cb23-1030" aria-hidden="true" tabindex="-1"></a>nb_results_df <span class="op">=</span> pd.DataFrame({<span class="st">'Values Trained On'</span>:value_types, <span class="st">'Average Accuracy'</span>:average_accuracies})</span>
<span id="cb23-1031"><a href="#cb23-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1032"><a href="#cb23-1032" aria-hidden="true" tabindex="-1"></a>display(nb_results_df)</span>
<span id="cb23-1033"><a href="#cb23-1033" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1034"><a href="#cb23-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1035"><a href="#cb23-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1036"><a href="#cb23-1036" aria-hidden="true" tabindex="-1"></a>For the series of Naive Bayes models created, the maximum accuracy score reached for any model is 44%. This was for a model trained on TF-IDF scores. The average accuracy scores for Naive Bayes models trained on the different value types of tokens range between 22-29%. These accuracy scores do not seem very high. Even the best of these models' predictions are wrong more often than they are right. However, since there are nine possible predictions that could be made (nine different sources), a random guess would expect to result in a 11% accuracy rate on average. This happens to also be the minimum accuracy score of any of these models. Therefore, all of these classifiers are still just as good or better than guessing which source an article comes from, simply given its text. </span>
<span id="cb23-1037"><a href="#cb23-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1038"><a href="#cb23-1038" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forest</span></span>
<span id="cb23-1039"><a href="#cb23-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1040"><a href="#cb23-1040" aria-hidden="true" tabindex="-1"></a>The same will be repeated with Random Forest models. In addition to using various holdout topic sets and dataframes to train and test these models on, a couple parameters will also be tested to see which ones result in the best performing model. Again, a table below is displayed with the average accuracy scores of models trained on a particular dataframe (value types) and set of parameters.</span>
<span id="cb23-1041"><a href="#cb23-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1044"><a href="#cb23-1044" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-1045"><a href="#cb23-1045" aria-hidden="true" tabindex="-1"></a><span class="co"># import library</span></span>
<span id="cb23-1046"><a href="#cb23-1046" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb23-1047"><a href="#cb23-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1048"><a href="#cb23-1048" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a dictionary that will hold the parameters that lead to best</span></span>
<span id="cb23-1049"><a href="#cb23-1049" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracies of a Random Forest Model</span></span>
<span id="cb23-1050"><a href="#cb23-1050" aria-hidden="true" tabindex="-1"></a>best_rf_params <span class="op">=</span> {}</span>
<span id="cb23-1051"><a href="#cb23-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1052"><a href="#cb23-1052" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize lists that will be used to create a dataframe showing the</span></span>
<span id="cb23-1053"><a href="#cb23-1053" aria-hidden="true" tabindex="-1"></a><span class="co"># average accuracies of Random Forest models trained on a particular</span></span>
<span id="cb23-1054"><a href="#cb23-1054" aria-hidden="true" tabindex="-1"></a><span class="co"># dataframe and with a particular set of parameters</span></span>
<span id="cb23-1055"><a href="#cb23-1055" aria-hidden="true" tabindex="-1"></a>average_accuracies <span class="op">=</span> []</span>
<span id="cb23-1056"><a href="#cb23-1056" aria-hidden="true" tabindex="-1"></a>value_types <span class="op">=</span> []</span>
<span id="cb23-1057"><a href="#cb23-1057" aria-hidden="true" tabindex="-1"></a>n_estimators <span class="op">=</span> []</span>
<span id="cb23-1058"><a href="#cb23-1058" aria-hidden="true" tabindex="-1"></a>criterions <span class="op">=</span> []</span>
<span id="cb23-1059"><a href="#cb23-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1060"><a href="#cb23-1060" aria-hidden="true" tabindex="-1"></a>best_accuracies_by_cf[<span class="st">'Random Forest'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-1061"><a href="#cb23-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1062"><a href="#cb23-1062" aria-hidden="true" tabindex="-1"></a><span class="co"># create a function to apply a Random Forest model using different</span></span>
<span id="cb23-1063"><a href="#cb23-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># holdout sets and given parameters and value types to train on</span></span>
<span id="cb23-1064"><a href="#cb23-1064" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_forest_cf(df, value_type, n, crit):</span>
<span id="cb23-1065"><a href="#cb23-1065" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Takes in a dataframe of tokens and the desired values </span></span>
<span id="cb23-1066"><a href="#cb23-1066" aria-hidden="true" tabindex="-1"></a><span class="co">    to train the model on (i.e. tfidf scores, frequencies)</span></span>
<span id="cb23-1067"><a href="#cb23-1067" aria-hidden="true" tabindex="-1"></a><span class="co">    and a string representing what the values are. </span></span>
<span id="cb23-1068"><a href="#cb23-1068" aria-hidden="true" tabindex="-1"></a><span class="co">    Also takes in specifications for n_estimators and criterion</span></span>
<span id="cb23-1069"><a href="#cb23-1069" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters of the Random Forest classifier. Trains, tests,</span></span>
<span id="cb23-1070"><a href="#cb23-1070" aria-hidden="true" tabindex="-1"></a><span class="co">    and outputs performance metrics of a series of RF classifiers, </span></span>
<span id="cb23-1071"><a href="#cb23-1071" aria-hidden="true" tabindex="-1"></a><span class="co">    one for each possible holdout set of two article topics.'''</span></span>
<span id="cb23-1072"><a href="#cb23-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1073"><a href="#cb23-1073" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initalize lists for accuracy/f1 scores to later be able to determine the min/max of each </span></span>
<span id="cb23-1074"><a href="#cb23-1074" aria-hidden="true" tabindex="-1"></a>    accuracy_list <span class="op">=</span> []</span>
<span id="cb23-1075"><a href="#cb23-1075" aria-hidden="true" tabindex="-1"></a>    f1_score_list <span class="op">=</span> []</span>
<span id="cb23-1076"><a href="#cb23-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1077"><a href="#cb23-1077" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loops to create a model with each combination of a testing set</span></span>
<span id="cb23-1078"><a href="#cb23-1078" aria-hidden="true" tabindex="-1"></a>    <span class="co"># consisting of two topics </span></span>
<span id="cb23-1079"><a href="#cb23-1079" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(topic_list)):</span>
<span id="cb23-1080"><a href="#cb23-1080" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="bu">len</span>(topic_list)):</span>
<span id="cb23-1081"><a href="#cb23-1081" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below to print the holdout topics</span></span>
<span id="cb23-1082"><a href="#cb23-1082" aria-hidden="true" tabindex="-1"></a>            <span class="co">#   and parameters used for the model</span></span>
<span id="cb23-1083"><a href="#cb23-1083" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("Holdout article topics: ")</span></span>
<span id="cb23-1084"><a href="#cb23-1084" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(topic_list[i])</span></span>
<span id="cb23-1085"><a href="#cb23-1085" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(topic_list[j])</span></span>
<span id="cb23-1086"><a href="#cb23-1086" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("n_estimators and criterion:")</span></span>
<span id="cb23-1087"><a href="#cb23-1087" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(n, crit)</span></span>
<span id="cb23-1088"><a href="#cb23-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1089"><a href="#cb23-1089" aria-hidden="true" tabindex="-1"></a>            <span class="co"># split into training/testing df's for predictor and target variables</span></span>
<span id="cb23-1090"><a href="#cb23-1090" aria-hidden="true" tabindex="-1"></a>            X_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb23-1091"><a href="#cb23-1091" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb23-1092"><a href="#cb23-1092" aria-hidden="true" tabindex="-1"></a>            X_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb23-1093"><a href="#cb23-1093" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb23-1094"><a href="#cb23-1094" aria-hidden="true" tabindex="-1"></a>            y_train_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[i]) <span class="op">&amp;</span> </span>
<span id="cb23-1095"><a href="#cb23-1095" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">!=</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb23-1096"><a href="#cb23-1096" aria-hidden="true" tabindex="-1"></a>            y_test_df <span class="op">=</span> df[(df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[i]) <span class="op">|</span> </span>
<span id="cb23-1097"><a href="#cb23-1097" aria-hidden="true" tabindex="-1"></a>                    (df[<span class="st">'article_topic'</span>] <span class="op">==</span> topic_list[j])][<span class="st">'article_source'</span>]</span>
<span id="cb23-1098"><a href="#cb23-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1099"><a href="#cb23-1099" aria-hidden="true" tabindex="-1"></a>            <span class="co"># transform the training/testing df's to arrays</span></span>
<span id="cb23-1100"><a href="#cb23-1100" aria-hidden="true" tabindex="-1"></a>            X_train <span class="op">=</span> X_train_df.to_numpy()</span>
<span id="cb23-1101"><a href="#cb23-1101" aria-hidden="true" tabindex="-1"></a>            X_test <span class="op">=</span> X_test_df.to_numpy()</span>
<span id="cb23-1102"><a href="#cb23-1102" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> y_train_df.to_numpy()</span>
<span id="cb23-1103"><a href="#cb23-1103" aria-hidden="true" tabindex="-1"></a>            y_test <span class="op">=</span> y_test_df.to_numpy()</span>
<span id="cb23-1104"><a href="#cb23-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1105"><a href="#cb23-1105" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a classifier object and train it on the training data</span></span>
<span id="cb23-1106"><a href="#cb23-1106" aria-hidden="true" tabindex="-1"></a>            rf_model <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> n, criterion<span class="op">=</span>crit, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb23-1107"><a href="#cb23-1107" aria-hidden="true" tabindex="-1"></a>            rf_model.fit(X_train, y_train)</span>
<span id="cb23-1108"><a href="#cb23-1108" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-1109"><a href="#cb23-1109" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make predictions on our testing set, output as an array</span></span>
<span id="cb23-1110"><a href="#cb23-1110" aria-hidden="true" tabindex="-1"></a>            predictions_array <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb23-1111"><a href="#cb23-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1112"><a href="#cb23-1112" aria-hidden="true" tabindex="-1"></a>            <span class="co"># turn the array with predicted sources into a list</span></span>
<span id="cb23-1113"><a href="#cb23-1113" aria-hidden="true" tabindex="-1"></a>            prediction_list <span class="op">=</span> []</span>
<span id="cb23-1114"><a href="#cb23-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1115"><a href="#cb23-1115" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> pred <span class="kw">in</span> predictions_array:</span>
<span id="cb23-1116"><a href="#cb23-1116" aria-hidden="true" tabindex="-1"></a>                prediction_list.append(pred)</span>
<span id="cb23-1117"><a href="#cb23-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1118"><a href="#cb23-1118" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a list of the true sources</span></span>
<span id="cb23-1119"><a href="#cb23-1119" aria-hidden="true" tabindex="-1"></a>            actuals_list <span class="op">=</span> []</span>
<span id="cb23-1120"><a href="#cb23-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1121"><a href="#cb23-1121" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> source <span class="kw">in</span> y_test:</span>
<span id="cb23-1122"><a href="#cb23-1122" aria-hidden="true" tabindex="-1"></a>                actuals_list.append(source)</span>
<span id="cb23-1123"><a href="#cb23-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1124"><a href="#cb23-1124" aria-hidden="true" tabindex="-1"></a>            <span class="co"># create a dataframe from the two lists above to show the actual</span></span>
<span id="cb23-1125"><a href="#cb23-1125" aria-hidden="true" tabindex="-1"></a>            <span class="co"># vs predicted sources for this model</span></span>
<span id="cb23-1126"><a href="#cb23-1126" aria-hidden="true" tabindex="-1"></a>            preds_actuals_df <span class="op">=</span> pd.DataFrame({<span class="st">'Actual'</span>:actuals_list, <span class="st">'Predicted'</span>:prediction_list})</span>
<span id="cb23-1127"><a href="#cb23-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1128"><a href="#cb23-1128" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment line below if want to see the dataframe of actual</span></span>
<span id="cb23-1129"><a href="#cb23-1129" aria-hidden="true" tabindex="-1"></a>            <span class="co">#   vs predicted sources for each model</span></span>
<span id="cb23-1130"><a href="#cb23-1130" aria-hidden="true" tabindex="-1"></a>            <span class="co"># display(preds_actuals_df)</span></span>
<span id="cb23-1131"><a href="#cb23-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1132"><a href="#cb23-1132" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate performance metrics</span></span>
<span id="cb23-1133"><a href="#cb23-1133" aria-hidden="true" tabindex="-1"></a>            accuracy <span class="op">=</span> <span class="bu">round</span>(accuracy_score(predictions_array, y_test), <span class="dv">2</span>)</span>
<span id="cb23-1134"><a href="#cb23-1134" aria-hidden="true" tabindex="-1"></a>            f1 <span class="op">=</span> <span class="bu">round</span>(f1_score(predictions_array, y_test, average<span class="op">=</span><span class="st">"weighted"</span>), <span class="dv">2</span>)</span>
<span id="cb23-1135"><a href="#cb23-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1136"><a href="#cb23-1136" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the accuracy/f1 scores for this model to the lists of accuracy/f1 scores </span></span>
<span id="cb23-1137"><a href="#cb23-1137" aria-hidden="true" tabindex="-1"></a>            accuracy_list.append(accuracy)</span>
<span id="cb23-1138"><a href="#cb23-1138" aria-hidden="true" tabindex="-1"></a>            f1_score_list.append(f1)</span>
<span id="cb23-1139"><a href="#cb23-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1140"><a href="#cb23-1140" aria-hidden="true" tabindex="-1"></a>            <span class="co"># check if this model's accuracy is better than the current best </span></span>
<span id="cb23-1141"><a href="#cb23-1141" aria-hidden="true" tabindex="-1"></a>            <span class="co"># accuracy for any Random Forest model, and if it is, then set its </span></span>
<span id="cb23-1142"><a href="#cb23-1142" aria-hidden="true" tabindex="-1"></a>            <span class="co"># accuracy to be the value for Random Forest in the dictionary we </span></span>
<span id="cb23-1143"><a href="#cb23-1143" aria-hidden="true" tabindex="-1"></a>            <span class="co"># initialized earlier and update the best parameters dictionary</span></span>
<span id="cb23-1144"><a href="#cb23-1144" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb23-1145"><a href="#cb23-1145" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> accuracy <span class="op">&gt;</span> best_accuracies_by_cf[<span class="st">'Random Forest'</span>]:</span>
<span id="cb23-1146"><a href="#cb23-1146" aria-hidden="true" tabindex="-1"></a>                    best_accuracies_by_cf[<span class="st">'Random Forest'</span>] <span class="op">=</span> accuracy</span>
<span id="cb23-1147"><a href="#cb23-1147" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'n_estimators'</span>] <span class="op">=</span> n</span>
<span id="cb23-1148"><a href="#cb23-1148" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'criterion'</span>] <span class="op">=</span> crit </span>
<span id="cb23-1149"><a href="#cb23-1149" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'value'</span>] <span class="op">=</span> value_type</span>
<span id="cb23-1150"><a href="#cb23-1150" aria-hidden="true" tabindex="-1"></a>                    best_rf_params[<span class="st">'holdout_topics'</span>] <span class="op">=</span> (topic_list[i], topic_list[j])</span>
<span id="cb23-1151"><a href="#cb23-1151" aria-hidden="true" tabindex="-1"></a>                    best_model_pred_vs_actuals <span class="op">=</span> preds_actuals_df.copy()</span>
<span id="cb23-1152"><a href="#cb23-1152" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span>:</span>
<span id="cb23-1153"><a href="#cb23-1153" aria-hidden="true" tabindex="-1"></a>                best_accuracies_by_cf[<span class="st">'Random Forest'</span>] <span class="op">=</span> accuracy</span>
<span id="cb23-1154"><a href="#cb23-1154" aria-hidden="true" tabindex="-1"></a>                best_rf_params[<span class="st">'n_estimators'</span>] <span class="op">=</span> n</span>
<span id="cb23-1155"><a href="#cb23-1155" aria-hidden="true" tabindex="-1"></a>                best_rf_params[<span class="st">'criterion'</span>] <span class="op">=</span> crit </span>
<span id="cb23-1156"><a href="#cb23-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1157"><a href="#cb23-1157" aria-hidden="true" tabindex="-1"></a>            <span class="co"># uncomment lines below if want to see the performance metrics</span></span>
<span id="cb23-1158"><a href="#cb23-1158" aria-hidden="true" tabindex="-1"></a>            <span class="co">#   for the model</span></span>
<span id="cb23-1159"><a href="#cb23-1159" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("Accuracy of Random Forest Model:", accuracy)</span></span>
<span id="cb23-1160"><a href="#cb23-1160" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("F1 score of Random Forest Model:", f1)</span></span>
<span id="cb23-1161"><a href="#cb23-1161" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("\n\n")</span></span>
<span id="cb23-1162"><a href="#cb23-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1163"><a href="#cb23-1163" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize a cumulative sum of accuracies for models trained on the</span></span>
<span id="cb23-1164"><a href="#cb23-1164" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dataframe/value types defined in the function</span></span>
<span id="cb23-1165"><a href="#cb23-1165" aria-hidden="true" tabindex="-1"></a>    cum_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-1166"><a href="#cb23-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1167"><a href="#cb23-1167" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add each model's accuracy to the cumulative sum</span></span>
<span id="cb23-1168"><a href="#cb23-1168" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> acc <span class="kw">in</span> accuracy_list:</span>
<span id="cb23-1169"><a href="#cb23-1169" aria-hidden="true" tabindex="-1"></a>        cum_acc <span class="op">+=</span> acc</span>
<span id="cb23-1170"><a href="#cb23-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1171"><a href="#cb23-1171" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the average accuracy of a model trained on this dataframe/value</span></span>
<span id="cb23-1172"><a href="#cb23-1172" aria-hidden="true" tabindex="-1"></a>    <span class="co"># type and with these parameters and add it to the list of average accuracies</span></span>
<span id="cb23-1173"><a href="#cb23-1173" aria-hidden="true" tabindex="-1"></a>    average_accuracies.append(<span class="bu">round</span>(cum_acc<span class="op">/</span><span class="bu">len</span>(accuracy_list), <span class="dv">2</span>))</span>
<span id="cb23-1174"><a href="#cb23-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1175"><a href="#cb23-1175" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add the value type used for training this model and its parameters to</span></span>
<span id="cb23-1176"><a href="#cb23-1176" aria-hidden="true" tabindex="-1"></a>    <span class="co"># their lists, which will be used to create a dataframe of average</span></span>
<span id="cb23-1177"><a href="#cb23-1177" aria-hidden="true" tabindex="-1"></a>    <span class="co"># accuracies for each of these options</span></span>
<span id="cb23-1178"><a href="#cb23-1178" aria-hidden="true" tabindex="-1"></a>    value_types.append(value_type)</span>
<span id="cb23-1179"><a href="#cb23-1179" aria-hidden="true" tabindex="-1"></a>    n_estimators.append(n)</span>
<span id="cb23-1180"><a href="#cb23-1180" aria-hidden="true" tabindex="-1"></a>    criterions.append(crit)</span>
<span id="cb23-1181"><a href="#cb23-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1182"><a href="#cb23-1182" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uncomment lines below if interested in displaying min/max accuracies of</span></span>
<span id="cb23-1183"><a href="#cb23-1183" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   models trained within this function.</span></span>
<span id="cb23-1184"><a href="#cb23-1184" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Minimum Accuracy Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",</span></span>
<span id="cb23-1185"><a href="#cb23-1185" aria-hidden="true" tabindex="-1"></a>    <span class="co">#      min(accuracy_list))</span></span>
<span id="cb23-1186"><a href="#cb23-1186" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Maximum Accuracy Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:", max(accuracy_list))</span></span>
<span id="cb23-1187"><a href="#cb23-1187" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"Maximum F1 Score of any Random Forest Model with {n} n_estimators, {crit} criterion, using {value_type}:",</span></span>
<span id="cb23-1188"><a href="#cb23-1188" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       max(f1_score_list))</span></span>
<span id="cb23-1189"><a href="#cb23-1189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("\n")</span></span>
<span id="cb23-1190"><a href="#cb23-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1191"><a href="#cb23-1191" aria-hidden="true" tabindex="-1"></a><span class="co"># specify all parameters that we want to test models using</span></span>
<span id="cb23-1192"><a href="#cb23-1192" aria-hidden="true" tabindex="-1"></a>crits <span class="op">=</span> [<span class="st">'gini'</span>, <span class="st">'entropy'</span>]</span>
<span id="cb23-1193"><a href="#cb23-1193" aria-hidden="true" tabindex="-1"></a>n_ests <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">200</span>]</span>
<span id="cb23-1194"><a href="#cb23-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1195"><a href="#cb23-1195" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the desired dataframes and parameters to create models with and</span></span>
<span id="cb23-1196"><a href="#cb23-1196" aria-hidden="true" tabindex="-1"></a><span class="co"># create multiple models with various holdout sets using the function defined above</span></span>
<span id="cb23-1197"><a href="#cb23-1197" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df <span class="kw">in</span> dfs:</span>
<span id="cb23-1198"><a href="#cb23-1198" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> crit <span class="kw">in</span> crits:</span>
<span id="cb23-1199"><a href="#cb23-1199" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> n_ests:</span>
<span id="cb23-1200"><a href="#cb23-1200" aria-hidden="true" tabindex="-1"></a>            random_forest_cf(df[<span class="dv">0</span>], df[<span class="dv">1</span>], n, crit)</span>
<span id="cb23-1201"><a href="#cb23-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1202"><a href="#cb23-1202" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe showing each value type that Random Forest models were trained on,</span></span>
<span id="cb23-1203"><a href="#cb23-1203" aria-hidden="true" tabindex="-1"></a><span class="co"># each parameter used, and the average accuracies for each of these combinations</span></span>
<span id="cb23-1204"><a href="#cb23-1204" aria-hidden="true" tabindex="-1"></a>rf_results_df <span class="op">=</span> pd.DataFrame({<span class="st">'Values Trained On'</span>:value_types, <span class="st">'n_estimators'</span>:n_estimators, <span class="st">'criterion'</span>:criterions, <span class="st">'Average Accuracy'</span>: average_accuracies}).sort_values(<span class="st">'Average Accuracy'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-1205"><a href="#cb23-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1206"><a href="#cb23-1206" aria-hidden="true" tabindex="-1"></a>rf_results_df <span class="op">=</span> rf_results_df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-1207"><a href="#cb23-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1208"><a href="#cb23-1208" aria-hidden="true" tabindex="-1"></a>display(rf_results_df)</span>
<span id="cb23-1209"><a href="#cb23-1209" aria-hidden="true" tabindex="-1"></a><span class="co"># if interested in displaying best parameters and holdout set for random forest models,</span></span>
<span id="cb23-1210"><a href="#cb23-1210" aria-hidden="true" tabindex="-1"></a><span class="co"># uncomment line below. there could have also been other parameters</span></span>
<span id="cb23-1211"><a href="#cb23-1211" aria-hidden="true" tabindex="-1"></a><span class="co"># and holdout sets that led to the same accuracy score.</span></span>
<span id="cb23-1212"><a href="#cb23-1212" aria-hidden="true" tabindex="-1"></a><span class="co"># best_rf_params</span></span>
<span id="cb23-1213"><a href="#cb23-1213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1214"><a href="#cb23-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1215"><a href="#cb23-1215" aria-hidden="true" tabindex="-1"></a>The best random forest classifier had an accuracy score of 61%. A few models with various parameters and holdout sets resulted in this accuracy score. The same parameters that have resulted in a model with a 61% accuracy have also resulted in models with an accuracy score as low as 11% (with a different holdout set). A few of the models that performed the worst (with worse parameters) had an accuracy score of 0%. As seen, using even the best parameters in the random forest models, the accuracy can wildly differ (11-61%) depending on which topics were used for training and testing. This is why the average accuracies are also displayed above for each combination of dataframe used and n_estimators and criterion parameters. It is reasonable to assume that this highly varied accuracy score that depends on the training/testing split could be remedied with a much larger sample of articles in the future, which could provide a rich set for both training and testing data. Still, a lot of the models' results can be considered good for our purposes! Using the right parameters and training/testing sets, we could correctly predict the source of an article about three out of every five times. It is worth pointing out that our goal here wasn't necessarily to determine with certainty which source a given article text comes from. Instead, the point of looking at the accuracies of these models is to be able to conclude whether the information about the words in an article is useful in determining the source of the article. I think we can say that this has been achieved. The vast majority of the time, the models had an accuracy score that was above the expected accuracy if one were to guess the source of an article (1/9, or 11%). This tells us that the wording between sources differs significantly enough to help us identify the source of an article. </span>
<span id="cb23-1216"><a href="#cb23-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1219"><a href="#cb23-1219" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-1220"><a href="#cb23-1220" aria-hidden="true" tabindex="-1"></a><span class="co"># create confusion matrix of model with best parameters</span></span>
<span id="cb23-1221"><a href="#cb23-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1222"><a href="#cb23-1222" aria-hidden="true" tabindex="-1"></a><span class="co"># set the training/testing datasets</span></span>
<span id="cb23-1223"><a href="#cb23-1223" aria-hidden="true" tabindex="-1"></a>X_train_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">&amp;</span> </span>
<span id="cb23-1224"><a href="#cb23-1224" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'Pentagon Documents Leak'</span>)].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb23-1225"><a href="#cb23-1225" aria-hidden="true" tabindex="-1"></a>X_test_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">|</span> </span>
<span id="cb23-1226"><a href="#cb23-1226" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'Pentagon Documents Leak'</span>)].iloc[:, :<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb23-1227"><a href="#cb23-1227" aria-hidden="true" tabindex="-1"></a>y_train_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">&amp;</span> </span>
<span id="cb23-1228"><a href="#cb23-1228" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">!=</span> <span class="st">'Pentagon Documents Leak'</span>)][<span class="st">'article_source'</span>]</span>
<span id="cb23-1229"><a href="#cb23-1229" aria-hidden="true" tabindex="-1"></a>y_test_df <span class="op">=</span> tfidf_df[(tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'U.S. and Germany Send Tanks to Ukraine'</span>) <span class="op">|</span> </span>
<span id="cb23-1230"><a href="#cb23-1230" aria-hidden="true" tabindex="-1"></a>        (tfidf_df[<span class="st">'article_topic'</span>] <span class="op">==</span> <span class="st">'Pentagon Documents Leak'</span>)][<span class="st">'article_source'</span>]</span>
<span id="cb23-1231"><a href="#cb23-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1232"><a href="#cb23-1232" aria-hidden="true" tabindex="-1"></a><span class="co"># convert training/testing datasets to arrays </span></span>
<span id="cb23-1233"><a href="#cb23-1233" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train_df.to_numpy()</span>
<span id="cb23-1234"><a href="#cb23-1234" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test_df.to_numpy()</span>
<span id="cb23-1235"><a href="#cb23-1235" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train_df.to_numpy()</span>
<span id="cb23-1236"><a href="#cb23-1236" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test_df.to_numpy()</span>
<span id="cb23-1237"><a href="#cb23-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1238"><a href="#cb23-1238" aria-hidden="true" tabindex="-1"></a><span class="co"># train the Random Forest model with the best parameters</span></span>
<span id="cb23-1239"><a href="#cb23-1239" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> <span class="dv">200</span>, criterion<span class="op">=</span><span class="st">"gini"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb23-1240"><a href="#cb23-1240" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb23-1241"><a href="#cb23-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1242"><a href="#cb23-1242" aria-hidden="true" tabindex="-1"></a><span class="co"># make source predictions</span></span>
<span id="cb23-1243"><a href="#cb23-1243" aria-hidden="true" tabindex="-1"></a>predictions_array <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb23-1244"><a href="#cb23-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1245"><a href="#cb23-1245" aria-hidden="true" tabindex="-1"></a><span class="co"># specify the labels to make the confusion matrix</span></span>
<span id="cb23-1246"><a href="#cb23-1246" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">"ABC News"</span>, <span class="st">"BBC"</span>, <span class="st">"CNN"</span>, <span class="st">"Fox News"</span>, <span class="st">"NBC News"</span>, <span class="st">"New York Post"</span>,</span>
<span id="cb23-1247"><a href="#cb23-1247" aria-hidden="true" tabindex="-1"></a>          <span class="st">"The New York Times"</span>, <span class="st">"The Washington Post"</span>, <span class="st">"The Wall Street Journal"</span>]</span>
<span id="cb23-1248"><a href="#cb23-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1249"><a href="#cb23-1249" aria-hidden="true" tabindex="-1"></a><span class="co"># shorten the labels so that the source names can easily fit on the confusion matrix</span></span>
<span id="cb23-1250"><a href="#cb23-1250" aria-hidden="true" tabindex="-1"></a>labels_short <span class="op">=</span> [<span class="st">"ABC"</span>, <span class="st">"BBC"</span>, <span class="st">"CNN"</span>, <span class="st">"Fox"</span>, <span class="st">"NBC"</span>, <span class="st">"NYP"</span>,</span>
<span id="cb23-1251"><a href="#cb23-1251" aria-hidden="true" tabindex="-1"></a>          <span class="st">"NYT"</span>, <span class="st">"WP"</span>, <span class="st">"WSJ"</span>]</span>
<span id="cb23-1252"><a href="#cb23-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1253"><a href="#cb23-1253" aria-hidden="true" tabindex="-1"></a><span class="co"># create the plot of the confusion matrix </span></span>
<span id="cb23-1254"><a href="#cb23-1254" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, predictions_array, labels<span class="op">=</span>labels)</span>
<span id="cb23-1255"><a href="#cb23-1255" aria-hidden="true" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm, display_labels<span class="op">=</span>labels_short)</span>
<span id="cb23-1256"><a href="#cb23-1256" aria-hidden="true" tabindex="-1"></a>disp.plot()</span>
<span id="cb23-1257"><a href="#cb23-1257" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> disp.ax_.get_figure() </span>
<span id="cb23-1258"><a href="#cb23-1258" aria-hidden="true" tabindex="-1"></a>fig.set_figwidth(<span class="dv">5</span>)</span>
<span id="cb23-1259"><a href="#cb23-1259" aria-hidden="true" tabindex="-1"></a>fig.set_figheight(<span class="dv">5</span>)</span>
<span id="cb23-1260"><a href="#cb23-1260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-1261"><a href="#cb23-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1262"><a href="#cb23-1262" aria-hidden="true" tabindex="-1"></a>We may also look at the confusion matrix above that was generated from the results of a Random Forest model that uses one of the parameters that led to the highest accuracy score of 61%. <span class="co">[</span><span class="ot">Data Camp</span><span class="co">](https://www.datacamp.com/tutorial/naive-bayes-scikit-learn)</span> was used as a source to code the confusion matrix (see @nb_confusion). It shows us which sources the classifier predicted for an article, versus what the correct source actually was. For more than half of the articles used in the holdout set (11/18), the predicted and actual source were the same (as seen on the diagonal of the confusion matrix). We can see, for example, that it twice incorrectly predicted Fox News to be the source, when the true source was New York Post. Neither of the New York Post or New York Times articles were correctly classified as such. Again, these are the predictions for just one of the many Random Forest models created. Unfortunately, it is difficult to infer the reasoning for these predictions, especially with a black box model like Random Forest which does not provide interpretable reasoning behind prediction decisions. </span>
<span id="cb23-1263"><a href="#cb23-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1264"><a href="#cb23-1264" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusions</span></span>
<span id="cb23-1265"><a href="#cb23-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1266"><a href="#cb23-1266" aria-hidden="true" tabindex="-1"></a>To summarize what was revealed in this project, here are a few takeaways:</span>
<span id="cb23-1267"><a href="#cb23-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1268"><a href="#cb23-1268" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Fox News uniquely mentions Trump and Biden much more frequently than other news sources, given its article lengths.</span>
<span id="cb23-1269"><a href="#cb23-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1270"><a href="#cb23-1270" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The New York Times articles in the sample have the strongest (negative) tone.</span>
<span id="cb23-1271"><a href="#cb23-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1272"><a href="#cb23-1272" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>With a Random Forest model, we were able to correctly predict the source of an article up to 61% of the time, leading us to conclude that the wording between news sources is, in fact, distinct. </span>
<span id="cb23-1273"><a href="#cb23-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1274"><a href="#cb23-1274" aria-hidden="true" tabindex="-1"></a><span class="fu"># Future Work</span></span>
<span id="cb23-1275"><a href="#cb23-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1276"><a href="#cb23-1276" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Other kinds of classifiers could also be tested, such as Stochastic Gradient Descent (SGD). </span>
<span id="cb23-1277"><a href="#cb23-1277" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using a much larger sample of articles could lead to better results, as they would better represent each source.</span>
<span id="cb23-1278"><a href="#cb23-1278" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Create a definition of bias in terms of word usage. Can we classify an article or source as biased? </span>
<span id="cb23-1279"><a href="#cb23-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-1280"><a href="#cb23-1280" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb23-1281"><a href="#cb23-1281" aria-hidden="true" tabindex="-1"></a><span class="fu">## Articles used in project</span></span>
<span id="cb23-1282"><a href="#cb23-1282" aria-hidden="true" tabindex="-1"></a>@abcaffirmative</span>
<span id="cb23-1283"><a href="#cb23-1283" aria-hidden="true" tabindex="-1"></a>@abcballoon</span>
<span id="cb23-1284"><a href="#cb23-1284" aria-hidden="true" tabindex="-1"></a>@abcbiden</span>
<span id="cb23-1285"><a href="#cb23-1285" aria-hidden="true" tabindex="-1"></a>@abchamas</span>
<span id="cb23-1286"><a href="#cb23-1286" aria-hidden="true" tabindex="-1"></a>@abcpentagon</span>
<span id="cb23-1287"><a href="#cb23-1287" aria-hidden="true" tabindex="-1"></a>@abcsantos</span>
<span id="cb23-1288"><a href="#cb23-1288" aria-hidden="true" tabindex="-1"></a>@abctanks</span>
<span id="cb23-1289"><a href="#cb23-1289" aria-hidden="true" tabindex="-1"></a>@abctrump</span>
<span id="cb23-1290"><a href="#cb23-1290" aria-hidden="true" tabindex="-1"></a>@bbcaffirmative</span>
<span id="cb23-1291"><a href="#cb23-1291" aria-hidden="true" tabindex="-1"></a>@bbcballoon</span>
<span id="cb23-1292"><a href="#cb23-1292" aria-hidden="true" tabindex="-1"></a>@bbcbiden</span>
<span id="cb23-1293"><a href="#cb23-1293" aria-hidden="true" tabindex="-1"></a>@bbchamas</span>
<span id="cb23-1294"><a href="#cb23-1294" aria-hidden="true" tabindex="-1"></a>@bbcpentagon</span>
<span id="cb23-1295"><a href="#cb23-1295" aria-hidden="true" tabindex="-1"></a>@bbcsantos</span>
<span id="cb23-1296"><a href="#cb23-1296" aria-hidden="true" tabindex="-1"></a>@bbctanks</span>
<span id="cb23-1297"><a href="#cb23-1297" aria-hidden="true" tabindex="-1"></a>@bbctrump</span>
<span id="cb23-1298"><a href="#cb23-1298" aria-hidden="true" tabindex="-1"></a>@cnnaffirmative</span>
<span id="cb23-1299"><a href="#cb23-1299" aria-hidden="true" tabindex="-1"></a>@cnnballoon</span>
<span id="cb23-1300"><a href="#cb23-1300" aria-hidden="true" tabindex="-1"></a>@cnnbiden</span>
<span id="cb23-1301"><a href="#cb23-1301" aria-hidden="true" tabindex="-1"></a>@cnnhamas</span>
<span id="cb23-1302"><a href="#cb23-1302" aria-hidden="true" tabindex="-1"></a>@cnnpentagon</span>
<span id="cb23-1303"><a href="#cb23-1303" aria-hidden="true" tabindex="-1"></a>@cnnsantos</span>
<span id="cb23-1304"><a href="#cb23-1304" aria-hidden="true" tabindex="-1"></a>@cnntanks</span>
<span id="cb23-1305"><a href="#cb23-1305" aria-hidden="true" tabindex="-1"></a>@cnntrump</span>
<span id="cb23-1306"><a href="#cb23-1306" aria-hidden="true" tabindex="-1"></a>@foxaffirmative</span>
<span id="cb23-1307"><a href="#cb23-1307" aria-hidden="true" tabindex="-1"></a>@foxballoon</span>
<span id="cb23-1308"><a href="#cb23-1308" aria-hidden="true" tabindex="-1"></a>@foxbiden</span>
<span id="cb23-1309"><a href="#cb23-1309" aria-hidden="true" tabindex="-1"></a>@foxhamas</span>
<span id="cb23-1310"><a href="#cb23-1310" aria-hidden="true" tabindex="-1"></a>@foxpentagon</span>
<span id="cb23-1311"><a href="#cb23-1311" aria-hidden="true" tabindex="-1"></a>@foxsantos</span>
<span id="cb23-1312"><a href="#cb23-1312" aria-hidden="true" tabindex="-1"></a>@foxtanks</span>
<span id="cb23-1313"><a href="#cb23-1313" aria-hidden="true" tabindex="-1"></a>@foxtrump</span>
<span id="cb23-1314"><a href="#cb23-1314" aria-hidden="true" tabindex="-1"></a>@nbcaffirmative</span>
<span id="cb23-1315"><a href="#cb23-1315" aria-hidden="true" tabindex="-1"></a>@nbcballoon</span>
<span id="cb23-1316"><a href="#cb23-1316" aria-hidden="true" tabindex="-1"></a>@nbcbiden</span>
<span id="cb23-1317"><a href="#cb23-1317" aria-hidden="true" tabindex="-1"></a>@nbchamas</span>
<span id="cb23-1318"><a href="#cb23-1318" aria-hidden="true" tabindex="-1"></a>@nbcpentagon</span>
<span id="cb23-1319"><a href="#cb23-1319" aria-hidden="true" tabindex="-1"></a>@nbcsantos</span>
<span id="cb23-1320"><a href="#cb23-1320" aria-hidden="true" tabindex="-1"></a>@nbctanks</span>
<span id="cb23-1321"><a href="#cb23-1321" aria-hidden="true" tabindex="-1"></a>@nbctrump</span>
<span id="cb23-1322"><a href="#cb23-1322" aria-hidden="true" tabindex="-1"></a>@nytaffirmative</span>
<span id="cb23-1323"><a href="#cb23-1323" aria-hidden="true" tabindex="-1"></a>@nytballoon</span>
<span id="cb23-1324"><a href="#cb23-1324" aria-hidden="true" tabindex="-1"></a>@nytbiden</span>
<span id="cb23-1325"><a href="#cb23-1325" aria-hidden="true" tabindex="-1"></a>@nythamas</span>
<span id="cb23-1326"><a href="#cb23-1326" aria-hidden="true" tabindex="-1"></a>@nytpentagon</span>
<span id="cb23-1327"><a href="#cb23-1327" aria-hidden="true" tabindex="-1"></a>@nytsantos</span>
<span id="cb23-1328"><a href="#cb23-1328" aria-hidden="true" tabindex="-1"></a>@nyttanks</span>
<span id="cb23-1329"><a href="#cb23-1329" aria-hidden="true" tabindex="-1"></a>@nyttrump</span>
<span id="cb23-1330"><a href="#cb23-1330" aria-hidden="true" tabindex="-1"></a>@nypaffirmative</span>
<span id="cb23-1331"><a href="#cb23-1331" aria-hidden="true" tabindex="-1"></a>@nypballoon</span>
<span id="cb23-1332"><a href="#cb23-1332" aria-hidden="true" tabindex="-1"></a>@nypbiden</span>
<span id="cb23-1333"><a href="#cb23-1333" aria-hidden="true" tabindex="-1"></a>@nyphamas</span>
<span id="cb23-1334"><a href="#cb23-1334" aria-hidden="true" tabindex="-1"></a>@nyppentagon</span>
<span id="cb23-1335"><a href="#cb23-1335" aria-hidden="true" tabindex="-1"></a>@nypsantos</span>
<span id="cb23-1336"><a href="#cb23-1336" aria-hidden="true" tabindex="-1"></a>@nyptanks</span>
<span id="cb23-1337"><a href="#cb23-1337" aria-hidden="true" tabindex="-1"></a>@nyptrump</span>
<span id="cb23-1338"><a href="#cb23-1338" aria-hidden="true" tabindex="-1"></a>@wsjaffirmative</span>
<span id="cb23-1339"><a href="#cb23-1339" aria-hidden="true" tabindex="-1"></a>@wsjballoon</span>
<span id="cb23-1340"><a href="#cb23-1340" aria-hidden="true" tabindex="-1"></a>@wsjbiden</span>
<span id="cb23-1341"><a href="#cb23-1341" aria-hidden="true" tabindex="-1"></a>@wsjhamas</span>
<span id="cb23-1342"><a href="#cb23-1342" aria-hidden="true" tabindex="-1"></a>@wsjpentagon</span>
<span id="cb23-1343"><a href="#cb23-1343" aria-hidden="true" tabindex="-1"></a>@wsjsantos</span>
<span id="cb23-1344"><a href="#cb23-1344" aria-hidden="true" tabindex="-1"></a>@wsjtanks</span>
<span id="cb23-1345"><a href="#cb23-1345" aria-hidden="true" tabindex="-1"></a>@wsjtrump</span>
<span id="cb23-1346"><a href="#cb23-1346" aria-hidden="true" tabindex="-1"></a>@wpaffirmative</span>
<span id="cb23-1347"><a href="#cb23-1347" aria-hidden="true" tabindex="-1"></a>@wpballoon</span>
<span id="cb23-1348"><a href="#cb23-1348" aria-hidden="true" tabindex="-1"></a>@wpbiden</span>
<span id="cb23-1349"><a href="#cb23-1349" aria-hidden="true" tabindex="-1"></a>@wphamas</span>
<span id="cb23-1350"><a href="#cb23-1350" aria-hidden="true" tabindex="-1"></a>@wppentagon</span>
<span id="cb23-1351"><a href="#cb23-1351" aria-hidden="true" tabindex="-1"></a>@wpsantos</span>
<span id="cb23-1352"><a href="#cb23-1352" aria-hidden="true" tabindex="-1"></a>@wptanks</span>
<span id="cb23-1353"><a href="#cb23-1353" aria-hidden="true" tabindex="-1"></a>@wptrump</span>
<span id="cb23-1354"><a href="#cb23-1354" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>